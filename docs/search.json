[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lecture notes for MA52112 (Statistics for Data Science)",
    "section": "",
    "text": "Overview of Statistics for Data Science",
    "crumbs": [
      "Overview of Statistics for Data Science"
    ]
  },
  {
    "objectID": "index.html#synopsis",
    "href": "index.html#synopsis",
    "title": "Lecture notes for MA52112 (Statistics for Data Science)",
    "section": "Synopsis",
    "text": "Synopsis\nIn this unit you will develop your understanding of the basic theory of probability and statistics and recognise when this theory can be applied in practice.",
    "crumbs": [
      "Overview of Statistics for Data Science"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Lecture notes for MA52112 (Statistics for Data Science)",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of the unit you will be able to:\n\nperform elementary mathematical operations in probability and statistics\ntranslate real-world problems into a probabilistic or statistical framework\nsolve statistical problems in abstract form\ncritically interpret the outcomes of statistical analysis in a real-world context\nrelate underlying theory to requirements in practical data science",
    "crumbs": [
      "Overview of Statistics for Data Science"
    ]
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Lecture notes for MA52112 (Statistics for Data Science)",
    "section": "Content",
    "text": "Content\nThe laws of probability. Discrete and continuous random variables. Expectation, variance and correlation. Conditional and marginal distributions. Common distributions including the normal, binomial and Poisson. Statistical estimation including maximum likelihood. Hypothesis testing and confidence intervals.",
    "crumbs": [
      "Overview of Statistics for Data Science"
    ]
  },
  {
    "objectID": "index.html#summative-assessment",
    "href": "index.html#summative-assessment",
    "title": "Lecture notes for MA52112 (Statistics for Data Science)",
    "section": "Summative assessment",
    "text": "Summative assessment\n\nExam: 100% of unit mark.",
    "crumbs": [
      "Overview of Statistics for Data Science"
    ]
  },
  {
    "objectID": "index.html#moodle-page",
    "href": "index.html#moodle-page",
    "title": "Lecture notes for MA52112 (Statistics for Data Science)",
    "section": "Moodle page",
    "text": "Moodle page\nPlease see the Moodle page for this unit for more a more detailed overview on the organisation and expectations for Statistics for Data Science this year.",
    "crumbs": [
      "Overview of Statistics for Data Science"
    ]
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "1  Probability Theory for Data Scientists",
    "section": "",
    "text": "1.1 Set theory Concepts\nEvents can be described in many different ways. We will use set theory and notation to describe events and operations on events. This can help later in the computation of probabilities.\nNote the definition of sigma-algebra does not explicitly require that the intersection of two sets in \\(\\mathcal F\\) is also in \\(\\mathcal F\\). However, this property follows from the other properties and De Morgan’s laws. If \\(A,B \\in \\mathcal F\\), then\n\\[\n\\cancel{A\\cup B \\in \\mathcal F \\implies (A\\cup B)^c = A^c \\cap B^c \\in \\mathcal F \\implies (A^c \\cap B^c)^c = A \\cup B \\in \\mathcal F}\n\\]\n\\[\nA^c \\in \\mathcal F\\,,B^c \\in \\mathcal F\n\\implies A^c \\cup B^c \\in \\mathcal F\n\\implies (A^c \\cup B^c)^c= A \\cap B\\in \\mathcal F\n\\]\n(corrected from previous version)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch1.html#set-theory-concepts",
    "href": "ch1.html#set-theory-concepts",
    "title": "1  Probability Theory for Data Scientists",
    "section": "",
    "text": "NoteDefinition: Sample Space, Event, and Empty Set\n\n\n\n\nDefinition 1.1 Consider an uncertain scenario. This might include a random experiment, a data-generating process or simply the future. We define the following concepts:\n\n\nSample Space (\\(\\Omega\\)): The set of all possible outcomes or results from the scenario. Sample spaces can be either countable or uncountable. If the elements of a sample space can be put into one-to-one correspondence with the set of integers, the sample space is countable. If the sample space contains only a finite number of elements, it is also countable. Otherwise is uncountable.\nEvent: A subset of the sample space. It represents a specific outcome or a collection of outcomes of interest.\nEmpty Set (\\(\\emptyset\\)): A set containing no elements. It represents an impossible event.\n\n\n\n\n\nExample 1.1 If we flip a coin twice then the sample space can be written as: \\[\n\\Omega =\\{HH,HT,TH,TT\\}\n\\]\nwhere \\(H\\) represents heads and \\(T\\) tails. This sample space is finite. An event (say \\(A\\)) could be at least one head appears, that is\n\\[A =\\{HT,TH,HH\\}\\subset \\Omega\\]\n\n\nExample 1.2 If we are analyzing customer purchase behavior for a single online transaction, the sample space could be the set of all possible combinations of items a customer might select from the store’s catalog. This sample space is in principle finite and therefore countable. However, if the catalog is very large, the sample space can be considered uncontably large for practical purposes. More on this later.\nAn event could be “customer buys at least one item from category X”, or “customer buys product Y”.\n\n\nExample 1.3 We measure the time (in seconds) it takes for a user to complete a task on a website. The time limit is predefined at 5 minutes. Then the sample space is \\(\\Omega = \\{0, 1, 2, 3,\\ldots, 300\\}\\) which is finite. If, however, we measure the time with arbitrary precision, then the sample space is the interval \\((0,300)\\) of real numbers. This sample space is uncountable.\nAn event could be “user completes the task in under 2 minutes”. In the former case, this corresponds to the set \\(A=\\{1,2\\ldots, 119 \\}\\). Int he latter case is the real interval \\(A=(0, 120)\\).\n\n\n\n\n\n\n\n\nNoteBasic Set Operations\n\n\n\n\nDefinition 1.2 Given events \\(A,B,C\\) in the sample space \\(\\Omega\\):\n\nUnion (\\(A \\cup B\\)): The event that \\(A\\) occurs, or \\(B\\) occurs, or both occur.\nIntersection (\\(A \\cap B\\)): The event that both \\(A\\) and \\(B\\) occur.\nComplement (\\(A^c\\)): The event that \\(A\\) does not occur. It is the set of all outcomes in \\(\\Omega\\) that are not in \\(A\\).\n\nThe following propertties hold for any events \\(A, B, C\\):\n\nCommutativity:\n\nUnion: \\(A \\cup B = B \\cup A\\)\nIntersection: \\(A \\cap B = B \\cap A\\)\n\nAssociativity:\n\nUnion: \\((A \\cup B) \\cup C = A \\cup (B \\cup C)\\)\nIntersection: \\((A \\cap B) \\cap C = A \\cap (B \\cap C)\\)\n\nDistributive Laws:\n\nIntersection over Union: \\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\)\nUnion over Intersection: \\(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\)\n\nDe Morgan’s Laws:\n\n\\((A \\cup B)^c = A^c \\cap B^c\\)\n\\((A \\cap B)^c = A^c \\cup B^c\\)\n\n\n\n\n\n\n\n\n\n\n\nNoteDisjoint Sets and Partitions of Sample Space\n\n\n\n\nDefinition 1.3  \n\nDisjoint Sets (Mutually Exclusive Events): Two sets \\(A\\) and \\(B\\) are disjoint if they have no elements in common, i.e., \\(A \\cap B = \\emptyset\\).\nPartition: A collection of non-empty, disjoint subsets (events) of \\(\\Omega\\) whose union is \\(\\Omega\\). That is \\(A_1,A_2, \\ldots\\) is a partition if\n\n\\[\n\\bigcup_{i} A_i = \\Omega \\quad \\text{and} \\quad A_i \\cap A_j = \\emptyset \\text{ for } i \\ne j\n\\]\n\n\n\n\n\n\n\n\n\nNoteRepresentation of events using set operations\n\n\n\n\nExample 1.4 When we flip a coin twice, the event \\(A\\) “at least one head appears” can be written in various ways. These include:\n\nthe union of three events \\(A = \\{HT\\} \\cup \\{TH\\} \\cup \\{HH\\}\\). That is, \\(A\\) occurs if we get heads on the first flip and tails on the second flip, or tails on the first flip and heads on the second flip, or heads on both flips. Note that these three events are disjoint as they do not share any outcomes.\nthe union \\(A = A_1 \\cup A_2\\) where \\(A_1 = \\{HT, HH\\}\\) is the event “head on first flip” and \\(A_2 = \\{TH, HH\\}\\) is the event “head on second flip”. Note that \\(A_1\\) and \\(A_2\\) are not disjoint as they both contain the outcome \\(HH\\).\nthe complement \\(A = B^c\\) where \\(B = \\{TT\\}\\) is the event “no heads appear”.\n\nThree different partitions of the sample space are given by:\n\nThe trivial partition where each event contains a single outcome: \\[\n\\mathcal P_1=\\{\\{HT\\},\\{TH\\},\\{HH\\},\\{TT\\}\\}\n\\]\nThe partition: \\[\n\\mathcal P_{equal}=\\{\\{HH,TT\\}, \\{HT,TH\\}\\}\n\\] that is, when we flip the coin twice, either we get the same results in both throws OR different ones.\nThe partition where we group the outcomes based on the number of heads:\n\n\\[\n\\mathcal P_{heads} =\\{\\{TT\\}, \\{HT,TH\\}, \\{HH\\}\\}\n\\]\nthat is, when we flip the coin twice, we can get no heads, one head or two heads.\n\n\n\n\n\n\n\n\n\nNoteSigma Algebra\n\n\n\n\nDefinition 1.4 A collection \\(\\mathcal{F}\\) of subsets of \\(\\Omega\\) is a sigma algebra (or \\(\\sigma\\)-algebra) if it satisfies the following properties:\n\n\\(\\Omega \\in \\mathcal{F}\\) (The sample space is in the collection).\nIf \\(A \\in \\mathcal{F}\\), then \\(A^c \\in \\mathcal{F}\\) (The collection is closed under complementation).\nIf \\(A_1, A_2, \\dots\\) are in \\(\\mathcal{F}\\), then \\[\n\\bigcup_i A_i \\in \\mathcal{F}\n\\]\n\nthat is, the collection is closed under arbitray number of unions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExamples of sigma-algebras\n\n\n\n\nExample 1.5 The trivial sigma algebra is clearly \\(\\mathcal F_0=\\{\\emptyset, \\Omega\\}\\) which does not seem very useful.\nThe partition \\(\\mathcal P_{equal}=\\{\\{HH,TT\\}, \\{HT,TH\\}\\}\\) above, is not a sigma-algebra as it does not contain the empty set. If we add the empty set, then is still not a sigma algebra as it is not closed under union. The union of the only two elements is \\(\\Omega\\). If we include \\(\\Omega\\) then we have the sigma algebra:\n\\[\n\\mathcal F_{equal}=\\{\\emptyset, \\Omega, \\{HH,TT\\}, \\{HT,TH\\}\\}\n\\]\nThe partition \\(\\mathcal P_{heads}\\) above is also not a sigma algebra but if we add all possible unions then we obtain the sigma algebra:\n\\[\n\\begin{aligned}\n\\mathcal F_{heads}& =\\{\\emptyset, \\Omega, \\{TT\\}, \\{HT,TH\\}, \\{HH\\}, \\{HT,TH,HH\\},\\\\\n& \\{HT,TH,TT\\}, \\{HH,TT\\}\\}\n\\end{aligned}\n\\]\nThe set \\[\n\\mathcal G =\\{\\emptyset,\\Omega,\\{HT\\},\\{TH\\},\\{HH\\},\\{TT\\}\\}\n\\]\nobatined from \\(\\mathcal P_1\\) is neither a partition nor a sigma algebra as it is not closed under union. For example, \\(\\{HT\\}\\cup \\{TH\\}=\\{HT,TH\\}\\notin \\mathcal G\\). However, if we add all possible unions of the elements of \\(\\mathcal G\\) we obtain the power set of \\(\\Omega\\), that is the set of all subsets of \\(\\Omega\\):\n\\[\n\\begin{aligned}\n\\mathcal F_{max} &= \\{\\emptyset,  \\{HT\\},\\{TH\\},\\{HH\\},\\{TT\\},\\\\\n&  \\{HT,TH\\}, \\{HT,HH\\}, \\{HT,TT\\}, \\{TH,HH\\}, \\{TH,TT\\}, \\{HH,TT\\}\\\\\n&  \\{HT,TH,HH\\}, \\{HT,TH,TT\\}, \\{HT,HH,TT\\}, \\{TH,HH,TT\\},\\Omega \\}\n\\end{aligned}\n\\]\nThis is the largest possible sigma-algebra for this sample space. It has \\(2^4=16\\) elements since the sample space has 4 elements. In general, if the sample space has \\(n\\) elements, then its power set has \\(2^n\\) elements.\nAlso generally, if we have a finite partition of \\(\\Omega\\) then the collection of all unions of sets in the partition (including the empty set) is a sigma-algebra.\nNote that different sigma algebras serve for different purposes. For example, the sigma algebra \\(\\mathcal F_{equal}\\) is useful if we are only interested in whether the two coin flips are the same or different. The sigma algebra \\(\\mathcal F_{heads}\\) is useful if we areinterested in the number of heads. The power set \\(\\mathcal F_{max}\\) is a sigma algebra that me be more useful if we are interested in all possible events.\nIn this example we also observe that:\n\\[\n\\mathcal F_0 \\subset \\mathcal F_{equal}\n\\subset \\mathcal F_{heads}\n\\subset \\mathcal F_{max}\n\\]\nso that \\(\\mathcal F_0\\) and \\(\\mathcal F_{max}\\) are the smallest and largest sigma algebras possible for this sample space.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch1.html#probability",
    "href": "ch1.html#probability",
    "title": "1  Probability Theory for Data Scientists",
    "section": "1.2 Probability",
    "text": "1.2 Probability\nWe will start by defining probability in an intuitive way. Later we will give a more formal mathematical definition .\n\n1.2.1 Types of Probability\nThere are several ways to think about probability. These include\n\nClassical Probability: Assumes all possible outcomes in a finite sample space are equally likely. That is, for any event \\(A\\) with \\(n(A)\\) outcomes in a sample space \\(\\Omega\\) with \\(n(\\Omega)\\) equally likely outcomes, the probability of \\(A\\) is: \\[ P(A) = \\frac{n(A)}{n(\\Omega)} \\]\n\nExample 1.6 Under this framework, the probability of rolling an even number on a die is assigned to be \\(P(\\text{rolling an even number}) = \\frac{3}{6}\\). More, generally this is equivalent to say the die is fair. Another example is when we assign the probability of rain tomorrow, locally at 10 AM, to be 1/2 as there are only two possible outcomes: rain or no rain.\n\nEmpirical (or Frequentist) Probability: Based on observed frequencies from repeated experiments. As the number \\(N\\) of experiment repetitions increases, the probability of an event \\(A\\) approaches the true probability: \\[ P(A) \\approx \\frac{\\text{Number of times A occurred}}{N} \\]\n\nExample 1.7 If we do not what the probability of heads when flipping a coin is. We can we flip the coin 1000 times and if it lands heads 537 times, we would say the empirical probability of heads is \\(0.537\\). Furthermore we might say that the true probability of heads is \\(\\approx 0.537\\) and the important aspect of thios framework is that, in theory, the more times we flip the coin the closer the empirical proportion will be to the true probability. Finally, according to historical data for our location, it has rained 33.6% of the days out of the last 10 years. The empirical probability of rain tomorrow locally at 10 AM is 0.336.\n\nSubjective Probability: Based on personal belief or judgment, often used when objective data is scarce.\n\nExample 1.8 I had a look through the window and is a bit overcast, then I believe the probability of rain tomorrow locally at 10 AM is 0.7. On the other hand, if I am a weather expert from the point of atmospheric physics, I might believe the probability of rain tomorrow locally at 10 AM is 0.9.\n\n\n\n\n1.2.2 Formal definition of probability\nAfter we have chosen a sigma algebra \\(\\mathcal F\\) that contains events we are interested in, we can define probabilities for all the events in a more formal way.\n\n\n\n\n\n\nNoteProbability Measure (Kolmogorov’s Axioms)\n\n\n\n\nDefinition 1.5 A probability measure \\(P\\) on a sample space \\(\\Omega\\) with a \\(\\sigma\\)-algebra \\(\\mathcal{F}\\) is a function \\(P: \\mathcal{F} \\to [0, 1]\\) that assigns a probability to each event in \\(\\mathcal{F}\\) and satisfies the following three axioms:\n\nNon-negativity: For any event \\(A \\in \\mathcal{F}\\), \\(P(A) \\ge 0\\). The probability of any event is non-negative.\nNormalization: \\(P(\\Omega) = 1\\). The probability of the entire sample space (the certain event) is 1.\nAdditivity (for disjoint events): If \\(A_1, A_2, \\dots, A_n\\) are disjoint events in \\(\\mathcal{F}\\) (i.e., \\(A_i \\cap A_j = \\emptyset\\) for \\(i \\ne j\\)), then \\[ P\\left(\\bigcup_{i=1}^n A_i\\right) = \\sum_{i=1}^n P(A_i) \\] For a countably infinite sequence of disjoint events, this extends to: \\[ P\\left(\\bigcup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty P(A_i) \\] The probability of the union of disjoint events is the sum of their individual probabilities.\n\n\n\n\n\n\n\n\n\n\nNoteProbability measure for the equality of two coin flips\n\n\n\n\nExample 1.9 For the sigma algebra \\(\\mathcal F_{equal}=\\{\\emptyset, \\Omega, \\{HH,TT\\}, \\{HT,TH\\}\\}\\) we can define a probability measure simply by specifying:\n\n\\(P(\\emptyset) = 0\\)\n\\(P(\\{HH,TT\\}) = 0.4\\)\n\nNote we can compute the probability of the other two events in \\(\\mathcal F_{equal}\\) using the axioms:\n\n\\(P(\\Omega) = 1\\) (by axiom 2)\n\\(P(\\{HT,TH\\}) = P(\\Omega) - P(\\{HH,TT\\}) = 1 - 0.4 = 0.6\\) (by axiom 3)\n\nThe assignment of probability of \\(\\{HH,TT\\}\\) to be \\(0.4\\) maybe frequentist or subjective but regardless of this, it generates is a valid probability measure as it satisfies all three axioms.\n\n\n\n\n\n\n\n\n\nNoteProbability measure for the number of heads in two coin flips\n\n\n\n\nExample 1.10 For the sigma algebra \\(\\mathcal F_{heads}\\) we can define a probability measure simply by specifying:\n\n\\(P(\\{HT,TH\\}) = 0.5\\)\n\\(P(\\{TT\\}) = 0.1\\)\n\nThe probabilities for the rest of the event in \\(\\mathcal F_{heads}\\) can be computed using axiom 3 as follows:\n\n\\(P(\\{HH\\}) = 1-0.1-0.5 = 0.4\\)\n\\(P(\\{HT,TH,HH\\}) = P(\\{HT,TH\\}) + P(\\{HH\\}) = 0.5 + 0.4 = 0.9\\)\n\\(P(\\{HT,TH,TT\\}) = P(\\{HT,TH\\}) + P(\\{TT\\})= 0.5 + 0.1 =0.6\\)\n\\(P(\\{HH,TT\\}) = 0.1 +0.4 = 0.5\\)\n\\(P(\\Omega) = 1\\) (Trivial but good to double check in practice)\n\\(P_{heads}(\\emptyset) = 1-1=0\\) (Trivial, always true)\n\nAs before the probability assignment maybe frequentist or subjective but regardless of this, it generates is a valid probability measure as it satisfies all three axioms.\n\n\n\n\n\n\n\n\n\nNoteProbability measure for power set\n\n\n\n\nExample 1.11 For the largest sigma algebra \\(\\mathcal F_{max}\\) we can define a probability measure simply by specifying probabilities for the four singletons or atoms:\n\n\\(P(\\{HH\\}) = 0.3\\)\n\\(P(\\{HT\\}) = 0.2\\)\n\\(P(\\{TH\\}) = 0.4\\)\n\nThe probabilities for the rest of the events in \\(\\mathcal F_{max}\\) can be computed using the axioms as follows:\n\n\\(P(\\{TT\\}) = 1-0.3-0.2-0.4 = 0.1\\)\n\\(P(\\{HT,TH\\}) = 0.2 + 0.4 = 0.6\\)\n\\(P(\\{HT,HH\\}) = 0.2 + 0.3 = 0.5\\)\n\\(P(\\{HT,TT\\}) = 0.2 + 0.1 = 0.3\\)\n\\(P(\\{TH,HH\\}) = 0.4 + 0.3 = 0.7\\)\n\\(P(\\{TH,TT\\}) = 0.4 + 0.1 = 0.5\\)\n\\(P(\\{HH,TT\\}) = 0.3 + 0.1 = 0.4\\)\n\\(P(\\{HT,TH,HH\\}) = 0.2 + 0.4 + 0.3 = 0.9\\)\n\\(P(\\{HT,TH,TT\\}) = 0.2 + 0.4 + 0.1 = 0.7\\)\n\\(P(\\{HT,HH,TT\\}) = 0.2 + 0.3 + 0.1 = 0.6\\)\n\\(P(\\{TH,HH,TT\\}) = 0.4 + 0.3 + 0.1 = 0.8\\)\n\nAs before the probability assignment maybe frequentist or subjective but regardless of this, it generates is a valid probability measure as it satisfies all three axioms.\n\n\n\n\n\n\n\n\n\nNoteSimple Probability Operations\n\n\n\n\nProposition 1.1 From the axioms, we can derive several useful properties:\n\nProbability of the Complement: For any event \\(A \\in \\mathcal{F}\\), \\[ P(A^c) = 1 - P(A) \\]\nProbability of the empty set: \\(P(\\emptyset) = 0\\).\nProbability of the Union of Two Events (General): For any two events \\(A, B \\in \\mathcal{F}\\): \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\] This is known as the addition rule. It accounts for the overlap between events.\n\n\n\n\n\n\n\n\n\n\nNoteProbability of the union\n\n\n\n\nExample 1.12 \\[\n\\begin{aligned}\nP(\\{HT,TH,TT\\}\\cup\\{HH,TT\\})&=P(\\{HT,TH,TT\\})+P(\\{HH,TT\\})-P(\\{TT\\})\\\\\n& =  0.6+0.5-0.1\\\\\n& =1      \n\\end{aligned}\n\\]\nclearly this is correct as the union of these two events is \\(\\Omega\\).\n\n\n\n\n\n\n\n\n\nNoteBoole and Bonferroni inequalities\n\n\n\n\nTheorem 1.1  \n\nBoole’s inequality For any events \\(A_1, A_2, \\ldots, A_n\\) in \\(\\mathcal F\\): \\[ P\\left(\\bigcup_{i=1}^n A_i\\right) \\le \\sum_{i=1}^n P(A_i) \\] This inequality provides an upper bound for the probability of the union of events.\nBonferroni Inequality: For any events \\(A_1, A_2, \\ldots, A_n\\) in \\(\\mathcal F\\): \\[ P\\left(\\bigcap_{i=1}^n A_i\\right) \\ge 1 - \\sum_{i=1}^n P(A_i^c) \\] This inequality provides a lower bound for the probability of the intersection of events.\n\n\n\n\nThese inequalities, specially Bonferroni’s will be useful later. Booles inequality can be proved by induction and Bonferroni’s inequality follows from Booles inequality and the properties of complements. These facts can be verified by the reader.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch1.html#conditional-probability",
    "href": "ch1.html#conditional-probability",
    "title": "1  Probability Theory for Data Scientists",
    "section": "1.3 Conditional Probability",
    "text": "1.3 Conditional Probability\n\n\n\n\n\n\nNoteConditional Probability\n\n\n\n\nDefinition 1.6 The conditional probability of event \\(A\\) given that event \\(B\\) has occurred, denoted \\(P(A|B)\\), is defined as: \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] provided that \\(P(B) &gt; 0\\). This measures the probability of event \\(A\\) occurring, knowing that event \\(B\\) has already happened.\n\n\n\n\n\n\n\n\n\nNoteExample of Conditional Probability\n\n\n\n\nExample 1.13 What is the probability of getting heads on the first coin flip GIVEN that at least one head appears in two flips? This can be expressed as \\(P(A|B)\\) where \\(A=\\{HT,HH\\}\\) is “head on first flip” and \\(B=\\{HT,TH,HH\\}\\) is “at least one head appears”. We have:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} =\\frac{P(A)}{P(B)}= \\frac{P(\\{HT,HH\\})}{P(\\{HT,TH,HH\\})}\n\\]\nsince \\(A\\subset B\\) in this case. We notice a subtlety here. The event \\(A =\\{HT,HH\\}\\) (head on the first flip) is not a member of the sigma-algebra \\(\\mathcal F_{heads}\\). So cannot use the probability measure \\(P\\) from Example 1.10 to compute this conditional probability. However, it is a member of the sigma algebra (the power set) \\(\\mathcal F_{max}\\) so we might need to use the probabilities using such sigma algebra (see Example 1.11) as follows:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(\\{HT,HH\\})}{P(\\{HT,TH,HH\\})} =\n\\frac{0.5}{0.9} \\approx 0.556\n\\]\n\nOn a more practical situation, if \\(A\\) is “a user makes a purchase” and \\(B\\) is “a user clicks on an advertisement”, then \\(P(A|B)\\) is the probability that a user makes a purchase GIVEN that they clicked on the advertisement. This is a key metric for evaluating ad campaign effectiveness.\n\n\n\nTwo very useful consequences of the above are: the law of total probability that combines the notion of partition with that of conditional probability and Bayes rule that allows us to reverse conditional probabilities.\n\n\n\n\n\n\nNoteLaw of Total Probability\n\n\n\n\nProposition 1.2 Let \\(A_1, A_2, \\dots\\) be a partition of the sample space \\(\\Omega\\) (recall Definition 1.3). Then for any event \\(B \\in \\mathcal{F}\\): \\[ P(B) = \\sum_{i} P(B|A_i) P(A_i) \\]\n\n\n\n\n\n\n\n\n\nNoteBayes’ Rule\n\n\n\n\nProposition 1.3 For events \\(A\\) and \\(B\\) where \\(P(B) &gt; 0\\): \\[ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \\] If \\(A_1, \\dots, A_n\\) form a partition of \\(\\Omega\\), and \\(P(A_i) &gt; 0\\) for all \\(i\\), then Bayes’ Rule can be written using the Law of Total Probability for the denominator: \\[ P(A|B) = \\frac{P(B|A) P(A)}{\\sum_{i=1}^n P(B|A_i) P(A_i)} \\]\n\n\n\nThe proof of this result is staightforward and left to the reader.\n\n\n\n\n\n\nNoteExample of Law of Total Probability and Bayes’ Rule\n\n\n\n\nExample 1.14 (Medical Testing) Suppose a rare disease affects 1 in 10,000 people. A test for this disease is 99% accurate:\n\nIf a person has the disease, the test correctly identifies it 99% of the time (True Positive).\nIf a person does not have the disease, the test correctly identifies it 99% of the time (True Negative).\n\nLet \\(D\\) be the event that a person has the disease, and \\(T^+\\) be the event that the test is positive. The probabilities we know are:\n\n\\(P(D) = \\frac{1}{10000} = 0.0001\\) (Prevalence)\n\\(P(T^+|D) = 0.99\\) (Sensitivity - True Positive Rate)\n\\(P(T^-|D^c) = 0.99\\) (Specificity - True Negative Rate)\n\nBefore we proceed we note the probability specifications above are emprical.\nSuppose we want to find \\(P(D|T^+)\\), the probability that a person actually has the disease given a positive test result.\nFirst, we need \\(P(T^+)\\). A positive test can occur in two ways:\n\n(\\(D \\cap T^+\\)) or\n(\\(D^c \\cap T^+\\))\n\ne.g. a partition of \\(A\\). We also have:\n\n\\(P(T^+|D^c) = 1 - P(T^-|D^c) = 1 - 0.99 = 0.01\\) (False Positive Rate)\n\\(P(D^c) = 1 - P(D) = 1 - 0.0001 = 0.9999\\)\n\nUsing the law of total probability:\n\\[\n\\begin{aligned}\nP(T^+) &=P(T^+ \\cap D)+P(T^+\\cap D^c)\\\\\n& =P(T^+|D)P(D) + P(T^+|D^c)P(D^c)\\\\\n&= (0.99)(0.0001) + (0.01)(0.9999)\\\\\n&= 0.000099 + 0.009999 = 0.010098\n\\end{aligned}\n\\]\nNow, using Bayes’ Theorem: \\[\n\\begin{aligned}\nP(D|T^+) &= \\frac{P(T^+|D) P(D)}{P(T^+)} \\\\\n&=   \\frac{(0.99)(0.0001)}{0.010098} \\approx 0.0098\n\\end{aligned}\n\\]\nThis may look counter-intuitive. Even with a positive test, there’s only about a 0.98% (less than 1%) chance the person actually has the disease! In particular it is a rare disease. This highlights the importance of understanding base rates and conditional probabilities in interpreting results.\n\n\n\nLets now code the previous example in Python. We code a function that returns \\(P(D|T^+)\\) given the prevalence of the disease, the sensitivity and the specificity of the test. WE vary the prevalence to see how it affects the result.\n\n\nCode\nimport numpy as np\ndef bayes_medical_test(prevalence, sensitivity, specificity):\n    P_D = prevalence  # Prevalence of the disease\n    P_T_given_D = sensitivity  # Sensitivity (True Positive Rate)\n    P_T_given_not_D = 1 - specificity  # False Positive Rate\n    P_not_D = 1 - P_D  # Probability of not having the disease\n    # Calculate P(T+)\n    P_T = (P_T_given_D * P_D) + (P_T_given_not_D * P_not_D)\n    # Calculate P(D|T+) using Bayes' Theorem\n    P_D_given_T = (P_T_given_D * P_D) / P_T\n    return P_D_given_T\n# Example usage\nprevalence = 1 / 10000  # 1 in 10,000\nsensitivity = 0.99  # 99% sensitivity\nspecificity = 0.99  # 99% specificity\nresult_10k = bayes_medical_test(prevalence, sensitivity, specificity)\nprint(f\"P(D|T+) = {result_10k:.4f}\")\n\nprevalence = 1 / 1000  # 1 in 1,000\n\nresult_1k = bayes_medical_test(prevalence, sensitivity, specificity)\nprint(f\"P(D|T+) = {result_1k:.4f}\")\n\nprevalence = 1 / 100  # 1 in 100\n\nresult_100 = bayes_medical_test(prevalence, sensitivity, specificity)\nprint(f\"P(D|T+) = {result_100:.4f}\")\n\n\nP(D|T+) = 0.0098\nP(D|T+) = 0.0902\nP(D|T+) = 0.5000\n\n\nWe can see how the prevalence of the disease affects the probability \\(P(D|T^+)\\) significantly. As the disease becomes more common, the probability that a person actually has the disease given a positive test result increases.\nThe Law of Total probability allows us to calculate the probability of an event \\(A\\) by considering the different ways it can occur through the events in a partition.\n\n\n\n\n\n\nNoteCustomer churn\n\n\n\n\nExample 1.15 Suppose we have three models, \\(M_1\\), \\(M_2\\), and \\(M_3\\), that are used to predict customer churn. Let \\(P(M_1)=0.5\\), \\(P(M_2)=0.3\\), \\(P(M_3)=0.2\\) be the probabilities that each model is the “best” for a given customer. Let \\(A\\) be the event “customer churns”. If we know the probability of churn given each best model (e.g., \\(P(A|M_1)=0.1\\), \\(P(A|M_2)=0.2\\), \\(P(A|M_3)=0.15\\)), the Law of Total Probability allows us to find the overall probability of churn:\n\\[\n\\begin{aligned}\nP(A) &= P(A|M_1)P(M_1) + P(A|M_2)P(M_2) + P(A|M_3)P(M_3) \\\\\n&= (0.1)(0.5) + (0.2)(0.3) + (0.15)(0.2)\\\\\n& = 0.05 + 0.06 + 0.03 = 0.14\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch1.html#independence",
    "href": "ch1.html#independence",
    "title": "1  Probability Theory for Data Scientists",
    "section": "1.4 Independence",
    "text": "1.4 Independence\n\n1.4.1 Independence of events\nFirst intuitively, two events, \\(A\\) and \\(B\\), are considered independent if the occurrence of one event does not affect the probability of the other event occurring. Formally,\n\n\n\n\n\n\nNoteIndependent Events\n\n\n\n\nDefinition 1.7 Tvents \\(A\\) and \\(B\\) are independent if: \\[ P(A \\cap B) = P(A) \\times P(B) \\] or equivalently, if either of the following conditions hold:\n\n\\(P(A|B) = P(A)\\)\n\\(P(B|A) = P(B)\\)\n\n\n\n\nThis means that knowing that event \\(B\\) has occurred gives us no new information about the probability of event \\(A\\) occurring, and vice versa.\n\n\n\n\n\n\nNoteIndependent event when flipping a coin twice\n\n\n\n\nConsider the following events when flipping a coin twice:\n\n\\(A=\\{HT, HH\\}\\) the first flip is heads\n\\(B=\\{TT, HH\\}\\) the two flips are the same\n\nThen using the probabilities in Example 1.11 we have:\n\\[\nP(A\\cap B) = P(\\{HH\\}) = 0.3 \\neq P(\\{HT,HH\\})P(\\{TT,HH\\}) = 0.5\\times 0.4  = 0.2\n\\]\nTherefore these two events are not independent. Of course, the assignment of probabilities here play a role. In this way, if we had assigned \\(P(\\{HH\\})=0.2\\) then the events would have been independent.\n\n\n\nAn obvious consequence of Definition 1.6 of conditional probability is the so-called multiplication rule.\n\n\n\n\n\n\nNoteMultiplication Rule\n\n\n\n\nProposition 1.4 For any two events \\(A\\) and \\(B\\) \\[ P(A \\cap B) = P(A) \\times P(B|A) = P(B) \\times P(A|B) \\]\n\n\n\n\n\n\n\n\n\nNoteDrawing Cards without Replacement\n\n\n\n\nExample 1.16 Imagine drawing two cards from a standard deck without replacement. Let \\(A\\) be the event that the first card is a Heart. \\(P(A) = \\frac{13}{52}\\). Let \\(B\\) be the event that the second card is a Heart. Since the first card is not replaced, these events are dependent. The probability of the second card being a Heart depends on the first card drawn. \\(P(B|A)\\) (the probability the second card is a Heart, given the first was a Heart) is \\(\\frac{12}{51}\\) (as there are 12 Hearts left and 51 total cards). So, the probability of drawing two Hearts in a row is \\(P(A \\cap B) = P(A) \\times P(B|A) = \\frac{13}{52} \\times \\frac{12}{51}\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe outcome of flipping a coin maybe independent of the outcome of any previous coin flips. If you flip a coin and get heads, the probability of getting heads on the next flip should remain as before. Of course, this a simplifying assumption that may not hold in practice. In this course we will make these kind of assumption specially when it involves sequences of events. Not assuming independence for sequences of events make things more complicated for what we want to achieve in this course.\n\n\n\n\n\n\n\n\nNoteIndependence of many events\n\n\n\n\nDefinition 1.8 A collection of events \\(A_1, A_2, \\ldots, A_n\\) are mutually independent if for every subset of size \\(k\\), e.g. \\(\\{A_{i_1}, A_{i_2}, \\ldots, A_{i_k}\\}\\) (\\(k\\) such that \\(2 \\le k \\le n\\)) we have that:\n\\[ P(A_{i_1} \\cap A_{i_2} \\cap \\ldots \\cap A_{i_k}) = P(A_{i_1}) \\times P(A_{i_2}) \\times \\ldots \\times P(A_{i_k}) \\]\n\n\n\nFor example, three events \\(A\\), \\(B\\), and \\(C\\) are mutually independent if all the following conditions hold:\n\n\\(P(A \\cap B) = P(A)P(B)\\)\n\\(P(A \\cap C) = P(A)P(C)\\)\n\\(P(B \\cap C) = P(B)P(C)\\)\n\\(P(A \\cap B \\cap C) = P(A)P(B)P(C)\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch1.html#random-variables",
    "href": "ch1.html#random-variables",
    "title": "1  Probability Theory for Data Scientists",
    "section": "1.5 Random Variables",
    "text": "1.5 Random Variables\nSo far we have talked about events, which are subsets of the sample space. In many applications, especially in data science, we are interested in quantifying outcomes numerically. This is where random variables come into play.\n\n\n\n\n\n\nNoteRandom Variable\n\n\n\n\nDefinition 1.9 A random variable \\(X\\) is a function that maps outcomes from the sample space \\(\\Omega\\) to real numbers. That is, \\(X: \\Omega \\to \\mathbb{R}\\). It quantifies the outcomes of a random phenomenon numerically.\nThe set of all possible values that \\(X\\) can take is called the image or range of the random variable, denoted as \\(X(\\Omega)\\).\n\n\n\n\n\n\n\n\n\nNoteRandom variable examples\n\n\n\n\nExample 1.17 If \\(\\Omega\\) is the set of all possible customer orders, a random variable \\(X\\) could be “the total dollar amount spent in an order”. For each order (an outcome in \\(\\Omega\\)), \\(X\\) assigns a specific monetary value. As another example: for a user’s session on a website, \\(X\\) could be “the number of pages visited” or the “overall time spent in the website”.\n\n\n\nNote a random variable is a function defined on the sample space \\(\\Omega\\) rather than on a sigma algebra, so for each outcome \\(\\omega \\in \\Omega\\), there is a corresponding real number \\(X(\\omega)\\). However, events in a sigma algebra can be defined in terms of random variables. For example, the event “the total amount spent in an order is greater than 50 dollars” can be expressed as \\(\\{X &gt; 50\\}\\).\n\n\n\n\n\n\nNoteRandom variable: Number of equal coin flips\n\n\n\n\nExample 1.18 When we flip a coin twice, the sample space is \\(\\Omega = \\{TT, HT, TH, HH\\}\\). We can define a very simple random variable \\(X\\) as the “number of times the flips are the same”. The mapping would be:\n\n\\(X(\\{TT\\}) = 1\\) (both flips are the same)\n\\(X(\\{HT\\}) = 0\\) (flips are different)\n\\(X(\\{TH\\}) = 0\\) (flips are different)\n\\(X(\\{HH\\}) = 1\\) (both flips are the same)\n\nThe possible values of \\(X\\) are \\(X(\\Omega)=\\{0, 1\\}\\).\n\n\n\n\n\n\n\n\n\nNoteRandom variable: Number of heads in two coin flips\n\n\n\n\nExample 1.19 When we flip a coin twice, the sample space is \\(\\Omega = \\{TT, HT, TH, HH\\}\\). We can define a random variable \\(X\\) as the “number of heads” in the two flips. The mapping would be:\n\n\\(X(\\{TT\\}) = 0\\) (no heads)\n\\(X(\\{HT\\}) = 1\\) (one head)\n\\(X(\\{TH\\}) = 1\\) (one head)\n\\(X(\\{HH\\}) = 2\\) (two heads)\n\nThe possible values of \\(X\\) are \\(\\{0, 1, 2\\}\\). This random variable quantifies the outcome of the coin flips in terms of the number of heads observed. Also note the order in which the heads appear does not matter for this random variable.\n\n\n\nThe above two random variables are discrete random variables as they take on a finite or countable number of values, that is \\(X(\\Omega)\\) is finite or countable. There are also continuous random variables that can take on any value in a continuous range.\n\n\n\n\n\n\nNoteContinuous vs Discrete Random Variables\n\n\n\n\nExample 1.20 Going back to Example 1.3 we have already defined a random variable \\(X\\) as the time to complete a task in a website with a limit of 5 minutes. If we round to the nearest second, then the possible values of \\(X\\) are \\(\\{0,1, 2, 3, \\ldots, 300\\}\\) and \\(X\\) is a discrete random variable. However, if we do not round then \\(X\\) can take any value in the interval \\((0,300)\\) and \\(X\\) is a continuous random variable.\n\n\n\nThe definition of continuous random variables requires a bit more than simply having an uncountably infinite image set \\(X(\\Omega)\\) . The definition is a bit thechnical as it involves the notion of probability density function.\n\n\n\n\n\n\nNoteDiscrete and continuous random variables\n\n\n\n\nDefinition 1.10 We say a random variable \\(X\\) is\n\ndiscrete if it takes on a finite or countably infinite number of distinct values. That is if the image set \\(X(\\Omega)\\) is either finite or countably infinite.\n\nThe function: \\[\nf_X(x) = P(X=x):=P(\\{\\omega\\,:\\,X(\\omega)=x\\})\\quad  \\mbox{for } x\\in X(\\Omega)\n\\]\nis called the probability mass function (PMF) of the discrete random variable \\(X\\). The PMF satisfies:\n\n\\(f_X(x) \\ge 0\\) for all \\(x \\in X(\\Omega)\\).\n\\(\\sum_{x \\in X(\\Omega)} f_X(x) = 1\\)\ncontinuous if there exists a function \\(f_X(x)\\) such that for any two numbers \\(a\\) and \\(b\\) with \\(a &lt; b\\): \\[ P(a \\le X \\le b):=P(\\{\\omega\\,:\\, a \\leq X(\\omega)\\leq b\\}) = \\int_a^b f_X(x) dx \\] where\n\\(f_X(x) \\ge 0\\) for all \\(x\\) and\n\\(\\int_{-\\infty}^{\\infty} f_X(x) dx = 1\\).\n\nThe function \\(f_X(x)\\) is called the probability density function (PDF) of the random variable \\(X\\).\n\n\n\nThe idea is that there are no “gaps”, which would correspond to real numbers which have a finite probability of occurring. Instead, continuous random variables never take an exact prescribed value, that is \\(P(X=x)=0\\) for all \\(x\\) but there is a positive probability that its value will lie in particular intervals which can be arbitrarily small.\n\n\n\n\n\n\nNoteCumulative Distribution Function (CDF)\n\n\n\n\nDefinition 1.11 The cumulative distribution function (CDF) of a random variable \\(X\\), denoted by \\(F_X(x)\\), is the function \\(F: \\mathcal R \\to [0,1]\\) defined by \\[\nF_X(x) = P(X \\le x):=P(\\{\\omega\\,:\\,X(\\omega)\\leq x\\})\n\\]\nfor any real number \\(x\\). The CDF gives the probability that the random variable \\(X\\) takes on a value less than or equal to \\(x\\).\n\n\n\nWe note that\n\\[\n\\begin{aligned}\nP(a \\leq X &lt;b ) &= F_X(b) - F_X(a)\\\\\n\\rule{0in}{4ex}    &= \\int_a^b f_X(x) dx  \n\\end{aligned}\n\\]\nso that\n\\[\nf_X(x) = \\frac{d}{dx} F_X(x)\\qquad \\forall x \\in \\mathbb R\n\\]\n\n\n\n\n\n\nNoteCDF of a discrete random variable\n\n\n\n\nExample 1.21 Consider a discrete random variable \\(X\\) with possible values in the set \\(\\{0, 1, 2, 3\\}\\). Assume we probability mass function (pmf) is given by: \\[\nf_X(x) = P(X=x) =\n\\begin{cases}\n0.1 & \\text{if } x = 0 \\\\\n0.3 & \\text{if } x = 1 \\\\\n0.4 & \\text{if } x = 2 \\\\\n0.2 & \\text{if } x = 3 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nthen the CDF is given by\n\\[\nF_X(x) = P(X \\le x) =\n\\begin{cases}\n0 & \\text{if } x &lt; 0 \\\\\n0.1 & \\text{if } 0 \\le x &lt; 1 \\\\\n0.4 & \\text{if } 1 \\le x &lt; 2 \\\\\n0.8 & \\text{if } 2 \\le x &lt; 3 \\\\\n1.0 & \\text{if } x \\ge 3\n\\end{cases}\n\\]\n\n\n\nWe can plot the CDF in Python as follows:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-1, 4, 1000)\ny = np.piecewise(x, [x &lt; 0, (x &gt;= 0) & (x &lt; 1), (x &gt;= 1) & (x &lt; 2), (x &gt;= 2) & (x &lt; 3), x &gt;= 3], [0, 0.1, 0.4, 0.8, 1.0])\nplt.step(x, y, where='post')\n# emphasize the continuity from the right\nplt.scatter([0, 1, 2, 3], [0.1, 0.4, 0.8, 1.0], color='blue')  # filled circles\nplt.scatter([0, 1, 2,3], [0, 0.1, 0.4,0.8], color='white', edgecolor='blue')  # open circles\nplt.title('CDF of Discrete Random Variable')\nplt.xlabel('x')\nplt.ylabel('F(x)')\nplt.grid()\nplt.yticks(np.array([0,0.1, 0.4, 0.8, 1.0]))\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.1: CDF of a discrete random variable. Note the function is defined over all real numbers\n\n\n\n\n\nFigure 1.1 show the CDF is a step function with jumps at the points where the random variable takes values and is continuos from the right\n\n\n\n\n\n\nNoteProperties of Cumulative Distribution Functions\n\n\n\n\nFor any random variable \\(X\\), its CDF \\(F_X(x)\\) has the following properties:\n\nMonotonicity: \\(F_X(x)\\) is non-decreasing. For any \\(x_1 &lt; x_2\\), \\(F_X(x_1) \\le F_X(x_2)\\).\nLimits: \\(\\lim_{x \\to -\\infty} F_X(x) = 0\\) and \\(\\lim_{x \\to \\infty} F_X(x) = 1\\).\nRight-continuity: \\(F_X(x)\\) is right-continuous, meaning \\(\\lim_{h \\to 0^+} F_X(x+h) = F_X(x)\\) for all \\(x\\).\n\n\n\n\nThe properties above hold for both discrete and continuous random variables. For discrete random variables, the CDF is a step function (continuous from the right), while for continuous random variables, the CDF is a continuous function.\n\n\n\n\n\n\nNoteCDF of a continuous random variable\n\n\n\n\nExample 1.22 Consider a random variable \\(X\\) representing the time (in hours) a server remains operational before crashing. \\(X\\) can take any non-negative real value. Assume the probability density function (pdf) is given by: \\[\nf_X(x)=\n\\begin{cases}\n\\frac{1}{100} e^{-x/100} & \\text{if } x \\ge 0 \\\\\n\\rule{0in}{3ex}0 & \\text{if } x &lt; 0\n\\end{cases}\n\\]\nThis probability distribution is called an exponential distribution with a mean of 100 hours. We will define and talk about mean later. The CDF is computed as follows:\n\\[\nF_X(x) = P(X \\le x) = \\int_{-\\infty}^x f_X(t) dt =\n\\begin{cases}\n0 & \\text{if } x &lt; 0 \\\\\n1 - e^{-x/100} & \\text{if } x \\ge 0\n\\end{cases}\n\\]\nThe CDF \\(F_Y(y) = P(Y \\le y)\\) would give the probability that the server operates for at most \\(y\\) hours. For instance, \\(F_Y(10)\\) would be the probability the server fails within the first 10 hours.\n\n\n\nWe can plot the CDF and the PDF in Python as follows:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(-50, 400, 1000)\npdf = np.piecewise(x, [x &lt; 0, x &gt;= 0], [0, lambda x: (1/100) * np.exp(-x/100)])\ncdf = np.piecewise(x, [x &lt; 0, x &gt;= 0], [0, lambda x: 1 - np.exp(-x/100)])\nplt.subplots(2,1, sharex=True)\nplt.subplot(2, 1, 1)\nplt.plot(x, pdf, label='PDF', color='blue')\nplt.title('PDF')\nplt.ylabel('f(x)')\nplt.grid()\nplt.subplot(2, 1, 2)\nplt.plot(x, cdf, label='CDF', color='orange')\nplt.title('CDF')\nplt.xlabel('x')\nplt.ylabel('F(x)')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.2: PDF and CDF of of an Exponential Random Variable\n\n\n\n\n\n\n\n\n\n\n\nNoteSupport of a Random Variable\n\n\n\n\nDefinition 1.12 The support of a random variable is the set of values where its probability distribution is non-zero.\n\nFor a discrete random variable, the support is the set of values \\(x\\) such that \\(f_X(x) &gt; 0\\).\nFor a continuous random variable, the support is the set of values \\(x\\) where the PDF \\(f_X(x) &gt; 0\\).\n\nIn terms of the sample space \\(\\Omega\\), the support is equal to \\(X(\\Omega)\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch1.html#common-probability-distributions",
    "href": "ch1.html#common-probability-distributions",
    "title": "1  Probability Theory for Data Scientists",
    "section": "1.6 Common Probability Distributions",
    "text": "1.6 Common Probability Distributions\n\n1.6.1 Discrete distributions\nSome common discrete probability distributions include:\n\n\n\n\n\n\nNoteDiscrete Uniform Random Variable\n\n\n\n\nDefinition 1.13 Let \\(X\\) be a random variable that can take on any of \\(k\\) equally likely values. The PMF is given by: \\[\nf_X(x|k) =\n\\begin{cases}\n\\frac{1}{k} & \\text{if } x \\in \\{1, 2, \\ldots, k\\} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nWe only need to specify \\(k\\) to specify the distribution, hence the notations \\(f_X(x|k)\\) for the PMF.\nNote the choice of support is arbitrary. We could have chosen any \\(k\\) distinct values for the support, we choose the first \\(k\\) integers for simplicity/convenience.\nExamples of random variables modelled this way is the outcome of rolling a fair \\(k=6\\)-sided die or the outcome of randomly selecting one item from a set of \\(k\\) distinct items. The notion of classical probability in Section 1.2.1 is equivalent to assuming a discrete uniform distribution to the random variable that assigns a real number to each ouctome.\n\n\n\n\n\n\n\n\n\nNoteGeneral discrete random variable\n\n\n\n\nDefinition 1.14 Let \\(X\\) be a discrete random variable with possible values in the set \\(\\{x_1, x_2, \\ldots,x_k\\}\\). The PMF is given by: \\[\nf_X(x|p_1,\\ldots,p_{k-1}) = P(X=x) =\n\\begin{cases}\np_i & \\text{if } x = x_i  \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nwhere \\(p_i &gt; 0\\) for all \\(i\\) and \\(\\sum_i p_i = 1\\).\nWe only need to specify \\(k-1\\) probabilities as the last one is determined by the fact that the probabilities must sum to 1. Hence the notation \\(f_X(x|p_1,\\ldots,p_{k-1})\\) for the PMF.\n\n\n\n\n\n\n\n\n\nNoteBernoulli and Binomial Random Variables\n\n\n\n\nDefinition 1.15 Bernoulli Distribution: Models a single binary outcome (success/failure) with parameter \\(p\\) (probability of success). The PMF is given by: \\[\nf_X(x) =\n\\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nWe only need to specify \\(p\\) to specify the distribution. We could have chosen any two distinct values instead of 0/1, we choose 0/1 for simplicity/convenience.\nBinomial Distribution: If \\(n\\) identical Bernoulli trials are performed, define the events:\n\n\\(A_i\\): the \\(i\\)-th trial is a success (for \\(i = 1, 2, \\ldots, n\\)) with \\(P(A_i) = p\\).\n\nIf we assume the events \\(A_1,\\ldots, A_n\\) are mutually independent ( as in Definition 1.8), then the random variable \\(Y\\) defined as the number of successes in the \\(n\\) trials follows a Binomial distribution.\nThe event \\(\\{Y = y\\}\\) will occur only if, out of the events \\(A_1,\\ldots, A_n\\), exactly \\(y\\) of them occur, and necessarily \\(n - y\\) of them do not occur. For example, when \\(y=2\\), one particular outcome (one particular ordering of occurrences and nonoccurrences) of the n Bernoulli trials might be:\n\\[A_1, A_2, A_3^c, A_4^c, \\ldots, A_n^c\\]\nwhich has probability\n\\[p^2(1-p)^{n-2}\\]\nHowever, there are many such orderings that lead to the same event \\(\\{Y = 2\\}\\), for example: \\[A_2, A_5, A_1^c, A_3^c, A_4^c, \\ldots, A_n^c\\]\nwhich also has probability \\(p^2(1-p)^{n-2}\\). The number of such orderings is the number of ways of choosing 2 successes from \\(n\\) trials, which is given by the binomial coefficient \\(\\binom{n}{2}\\). In general, the number of ways of choosing \\(y\\) successes from \\(n\\) trials is given by the binomial coefficient \\(\\binom{n}{y}\\). Therefore, the PMF of the Binomial distribution is given by: \\[\nf_Y(y|n,p) = P(Y=y) = \\binom{n}{y} p^y (1 - p)^{n-y} \\quad \\text{for } y = 0, 1, \\ldots, n\n\\]\nWe only need to specify \\(n\\) and \\(p\\) to specify the distribution, hence the notation \\(f_Y(y|n,p)\\) for the PMF.\n\n\n\n\n\n\n\n\n\nNoteHypergeometric Random Variable\n\n\n\n\nDefinition 1.16 The hypergeometric distribution models the number of successes in a sequence of \\(n\\) draws from a finite population without replacement. So is similar to the Binomial distribution but without the assumption of independence of the Bernoulli trials.\nIt is easy to describe this distribution with a concrete example. Suppose we have an urn with:\n\na total of \\(n\\) balls\n\\(m\\) balls are red and\n\\(n-m\\) are green.\n\nWe select \\(k\\) balls at random (the \\(k\\) balls are taken all at once, a case of sampling without replacement) so that \\(k\\leq n\\). What is the probability that exactly \\(y\\) of the balls are red?\nThe corresponding random variable \\(Y\\) is the number of red balls in the sample of size \\(k\\).\nThe support of the random variable \\(Y\\) can be obtained using the following reasoning:\n\nTo obtain the minimum number of red balls, there are two cases:\n\nif there more green balls than those we can choose(\\(k\\leq \\textcolor{green}{n-m}\\)) and if we happen to choose all green balls then the number of red balls is 0, e.g. \\(\\textcolor{red}{Y}=0\\) and this is the smallest it can be.\nif \\(k&gt;\\textcolor{green}{n-m}\\) and we choose all green balls then all the remaining \\(k-\\textcolor{green}{(n-m)}\\) balls are necessarily red so \\(Y=k-\\textcolor{green}{(n-m)}&gt;0\\) and this is the smallest it can be.\n\n\nTherefore, the minimum number of red balls is \\(\\max\\{0, k-\\textcolor{green}{(n-m)}\\}\\).\n\nFor the maximum number of red balls, there are two cases:\n\nwe cannot choose more red balls than there are in the urn (e.g. \\(\\textcolor{red}{m}\\leq k\\)) so that the maximum number of red balls is \\(\\textcolor{red}{m}\\).\nwe cannot choose more red balls than the total number \\(k\\) of balls we are choosing (e.g. \\(k&gt;\\textcolor{red}{m}\\)) so that the maximum number of red balls is \\(k\\)\n\n\nTherefore, the maximum number of red balls is \\(\\min\\{\\textcolor{red}{m},k\\}\\).\nWe also have that:\n\nthe number of ways of choosing \\(k\\) balls from \\(n\\) is \\(\\binom{n}{k}\\)\nthe number of ways of choosing \\(y\\) red balls from the \\(\\textcolor{red}{m}\\) red balls is \\(\\binom{\\textcolor{red}{m}}{y}\\) and\nthe number of ways of choosing the remaining \\(k-y\\) balls from the \\(\\textcolor{green}{n-m}\\) green balls is \\(\\binom{\\textcolor{green}{n-m}}{k-y}\\). Therefore, the PMF of the Hypergeometric distribution is given by: \\[\nf_Y(\\textcolor{red}{y}|n,\\textcolor{red}{m},k) = P(\\textcolor{red}{Y}=\\textcolor{red}{y}) =\n\\begin{cases}\n\\frac{\\displaystyle\\binom{\\textcolor{red}{m}}{\\textcolor{red}{y}} \\binom{\\textcolor{green}{n-m}}{k-\\textcolor{red}{y}}}{\\displaystyle\\binom{n}{k}} \\quad \\text{ for } y = \\max\\{0, k-\\textcolor{green}{(n-m)}\\}, \\ldots, \\min\\{\\textcolor{red}{m},k\\}\n\\\\\n\\rule{0in}{5ex} \\quad \\text{ otherwise}\n\\end{cases}\n\\]\n\nThe equally likely implicit assumption can be justified if we can guarantee the balls are randomly choosen.\nWe only need to specify \\(n\\), \\(m\\) and \\(k\\) to specify the distribution, hence the notation \\(f_Y(y|n,m,k)\\) for the PMF.\n\n\n\n\n\n\n\n\n\nNotePoisson Random Variable\n\n\n\n\nDefinition 1.17 This random variable is relevant when we are modeling the ocurrences of an event in time or space. For example\n\nwaiting for a bus to arrive,\nwaiting for customers to arrive in a bank,\nnumber of damaged trees in a given area of a forest\nthe number of defects in a given length of communications cable\n\nThe number of occurrences in a given interval or area can sometimes be modeled by the Poisson distribution.\nThe Poisson distribution is based on the following assumptions:\n\nfor small time intervals, the probability of an event is proportional to the length of waiting time or area size\nThe number of events in disjoint time intervals or disjoint areas are independent.\nthe intensity \\(\\lambda\\) (average rate of occurrence) is constant over time or space.\n\nSo let define random variable \\(X\\) that models the number of events occurring in a fixed interval of time or area of space, given an intensity parameter \\(\\lambda\\) (which also relative to the length of time or area size). The PMF is given by: \\[\nf_X(x|\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\quad \\text{for }x = 0, 1, 2, \\ldots\n\\]\nWe only need to specify the rate \\(\\lambda\\) to specify the distribution, hence the notation \\(f_X(x|\\lambda)\\) for the PMF.\n\n\n\n\n\n\n\n\n\nNoteNegative Binomial Random Variable\n\n\n\n\nDefinition 1.18 The negative binomial distribution models the number of trials needed to achieve a fixed number of successes in a sequence of independent Bernoulli trials, each with the same probability of success \\(p\\).\nLet \\(X\\) be the random variable representing the number of trials needed to achieve \\(r\\) successes. The PMF is given by: \\[\nf_X(x|r,p) = \\binom{x-1}{r-1} p^r (1 - p)^{x-r} \\quad \\text{for } x = r, r+1, r+2, \\ldots\n\\]\nThis is because the \\(r\\)-th success must occur on the \\(x\\)-th trial, and the previous \\(x-1\\) trials must contain exactly \\(r-1\\) successes. The probability of any such sequence of trials is \\(p^r (1 - p)^{x-r}\\). The number of ways to choose which \\(r-1\\) trials out of the first \\(x-1\\) are successes is given by the binomial coefficient \\(\\binom{x-1}{r-1}\\).\nWe only need to specify \\(r\\) and \\(p\\) to specify the distribution, hence the notation \\(f_X(x|r,p)\\) for the PMF.\nThe specific case where \\(r=1\\) is called the geometric distribution, which models the number of trials until the first success. The PMF for the geometric distribution is given by: \\[\nf_X(x|p) = (1 - p)^{x-1} p \\quad \\text{for } x = 1, 2, 3, \\ldots\n\\]\nThere are alternative definitions of the Negative Binomial distribution. For example, the random variable \\(Y\\) is defined as the number of failures before the \\(r\\)-th success. The PMF in this case is given by: \\[\nf_Y(y|r,p) = \\binom{y+r-1}{r-1} p^r (1 - p)^{y}= \\binom{y+r-1}{y} p^r (1 - p)^{y}\\quad \\text{for } y = 0, 1, 2, \\ldots\n\\]\nClearly, the two random variables are related in that \\(Y=X-r\\).\n\n\n\n\n\n1.6.2 Continuous distributions\nSome common continuous probability distributions include:\n\n\n\n\n\n\nNoteContinuous Uniform\n\n\n\n\nDefinition 1.19 Models a continuous random variable such that intervals of the same length are equally likely. The support is the interval \\((a, b)\\) for \\(a&lt;b\\). The probability density fucntion (PDF) is given by: \\[\nf_X(x|a,b) =\n\\begin{cases}\n\\frac{1}{b - a} & \\text{if } a &lt; x &lt; b \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nWe only need to specify \\(a\\) and \\(b\\) to specify the distribution. The CDF for this distribution is given by: \\[\nF_X(x|a,b) =\n\\begin{cases}\n0 & \\text{if } x &lt; a \\\\\n\\frac{x - a}{b - a} & \\text{if } a \\le x \\le b \\\\\n1 & \\text{if } x &gt; b\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nNoteNormal (Gaussian) Random Variable\n\n\n\n\nDefinition 1.20 Models a continuous random variable with a bell-shaped curve, characterized by its mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The PDF is given by: \\[\nf_X(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\quad \\text{for } x \\in \\mathbb{R}\n\\] The CDF for this distribution is given by: \\[\nF_X(x) = \\int_{-\\infty}^x f_X(u)\\,du = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x - \\mu}{\\sigma \\sqrt{2}} \\right) \\right]\n\\] where \\(\\text{erf}\\) is the error function. Usually, we will express this CDF in terms of the CDF of the standard normal distribution (mean 0 and standard deviation 1) that we will denote by \\(\\Phi(x)\\). Then we can write: \\[\nF_X(x) = \\Phi\\left( \\frac{x - \\mu}{\\sigma} \\right)\n\\] where \\(\\Phi(x)\\) is the CDF of the standard normal distribution, that is\n\\[\n\\Phi(z)=\\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} du\n\\]\n\n\n\n\n\n\n\n\n\nNoteExponential Random Variable\n\n\n\n\nDefinition 1.21 THis random variable can be used to model the time (continuouslye.g. infinite precision) to the ocurrence of an event of interest or the time in between events of interest. It is fully characterised by the rate parameter \\(\\lambda\\). The PDF is given by: \\[\nf_X(x) =\n\\begin{cases}\n\\lambda e^{-\\lambda x} & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0\n\\end{cases}\n\\]\nThe CDF for this distribution is given by: \\[\nF_X(x) =\n\\begin{cases}\n0 & \\text{if } x &lt; 0 \\\\\n1 - e^{-\\lambda x} & \\text{if } x \\ge 0\n\\end{cases}\n\\]\n\n\n\n\n\n1.6.3 Joint Distributions\nWe can define more than one random variable on the same sample space.\n\n\n\n\n\n\nTipRandom Vectors\n\n\n\n\nDefinition 1.22 A random vector is a vector whose components are random variables defined on the same probability space. If we have \\(k\\) random variables \\(X_1, X_2, \\ldots, X_k\\) defined on the same sample space \\(\\Omega\\), we can define a random vector \\(\\mathbf{X}\\) as the function from \\(\\Omega\\) to \\(\\mathbb{R}^k\\) given by:\n\\[\n\\mathbf{X}(\\omega) := (X_1(\\omega), X_2(\\omega), \\ldots, X_n(\\omega))\n\\]\n\n\n\n\n\n\n\n\n\nNoteJoint Distribution of two random variables\n\n\n\n\nDefinition 1.23 The joint distribution of two random variables \\(X\\) and \\(Y\\) describes the probability distribution of their combined outcomes. It can be easily represented by the joint cumulative distribution function (CDF) defined as: \\[\nF_{X,Y}(x,y) = P(X \\leq x, Y \\leq y)\n\\] for any real numbers \\(x\\) and \\(y\\). This definition is irrespective of whether the random variables are discrete or continuous.\nThe joint distribution can also represented by the joint probability mass function (PMF) for discrete random variables or the joint probability density function (PDF) for continuous random variables.\nFor two discrete random variables \\(X\\) and \\(Y\\), the joint PMF is defined as: \\[\nf_{X,Y}(x,y) = P(X = x, Y = y)\n\\]\nfor all possible joint values \\((x, y)\\) in the image set \\((X,Y)(\\Omega)\\).\nFor two continuous random variables \\(X\\) and \\(Y\\), the joint PDF can be defined if there exists a function \\(f_{X,Y}(x,y)\\) such that for any two numbers any subset \\(A\\subset \\mathbb{R}^2\\): \\[\nP((X,Y)\\in A) = \\int _A \\int f_{X,Y}(x,y) \\, dx \\, dy\n\\]\nThe joint PDF can also be defined as the partial cross-derivative of their joint cumulative distribution function (CDF): \\[\n  f_{X,Y}(x,y) = \\frac{\\partial^2}{\\partial x \\partial y} P(X \\leq x, Y \\leq y)\n\\]\nThe joint PDF satisfies:\n\n\\(f_{X,Y}(x,y) \\ge 0\\) for all \\(x, y \\in \\mathbb{R}\\).\n\\(\\rule{0in}{4ex}\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx \\, dy = 1\\).\n\n\n\n\n\n\n\n\n\n\nNoteMarginal Distributions\n\n\n\n\nDefinition 1.24 The marginal distribution of a random variable is the probability distribution of that variable when considered independently of other variables. It is obtained by summing (for discrete variables) or integrating (for continuous variables) the joint distribution over the values of the other variables. For two discrete random variables \\(X\\) and \\(Y\\) with joint PMF \\(f_{X,Y}(x,y)\\), the marginal PMFs are given by: \\[\nf_X(x) = \\sum_{y} f_{X,Y}(x,y)\n\\] \\[\nf_Y(y) = \\sum_{x} f_{X,Y}(x,y)\n\\] For two continuous random variables \\(X\\) and \\(Y\\) with joint PDF \\(f_{X,Y}(x,y)\\), the marginal PDFs are given by: \\[\nf_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy\n\\] \\[\nf_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx\n\\]\nWe can define marginal CDFs in a similar manner. For two random variables \\(X\\) and \\(Y\\) with joint CDF \\(F_{X,Y}(x,y)\\), the marginal CDFs are given by: \\[\nF_X(x) = \\lim_{y \\to \\infty} F_{X,Y}(x,y)\n\\] \\[\nF_Y(y) = \\lim_{x \\to \\infty} F_{X,Y}(x,y)\n\\]\n\n\n\n\n\n\n\n\n\nNoteJoint Distribution in the case of two coin flips\n\n\n\n\nExample 1.23 When flipping a coin twice, we can define two random variables:\n\n\\(X_1\\): outcome of the first flip (1 for heads, 0 for tails)\n\\(X_2\\): outcome of the second flip (1 for heads, 0 for tails)\n\nFollowing from Example 1.11 The joint distribution of \\(X_1\\) and \\(X_2\\) can be represented in a table:\n\n\n\n\\(X_1 \\backslash X_2\\)\n0 (Tails)\n1 (Heads)\n\n\n\n\n0 (Tails)\n0.1\n0.4\n\n\n1 (Heads)\n0.2\n0.3\n\n\n\nThe joint PMF is given by:\n\n\\(f_{X_1,X_2}(0,0)=P(X_1=0, X_2=0) = P(\\{TT\\}) = 0.1\\)\n\\(f_{X_1,X_2}(0,1)=P(X_1=0, X_2=1) = P(\\{TH\\}) = 0.4\\)\n\\(f_{X_1,X_2}(1,0)=P(X_1=1, X_2=0) = P(\\{HT\\}) = 0.2\\)\n\\(f_{X_1,X_2}(1,1)=P(X_1=1, X_2=1) = P(\\{HH\\}) = 0.3\\)\n\nThe marginal distributions of \\(X_1\\) and \\(X_2\\) can be obtained by summing over the rows and columns respectively:\n\\[\nf_{X_1}(x_1) = \\sum_{x_2\\in\\{0,1\\}} f_{X_1,X_2}(x_1,x_2)\n=\\begin{cases}\nf_{X_1,X_2}(0,0) + f_{X_1,X_2}(0,1) = 0.1 + 0.4=0.5 & \\text{if } x_1 = 0 \\\\\nf_{X_1,X_2}(1,0) + f_{X_1,X_2}(1,1) = 0.2 + 0.3=0.5 & \\text{if } x_1 = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\\[\nf_{X_2}(x_2)= \\sum_{x_1\\in\\{0,1\\}} f_{X_1,X_2}(x_1,x_2)\n=\\begin{cases}\nf_{X_1,X_2}(0,0) + f_{X_1,X_2}(1,0) = 0.1 + 0.2 =0.3 & \\text{if } x_2 = 0 \\\\\nf_{X_1,X_2}(0,1) + f_{X_1,X_2}(1,1) = 0.4 + 0.3 = 0.7 & \\text{if } x_2 = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nNow consider another random variable \\(Z\\) be the indicator that the two flips are the same, that is \\(Z=1\\) if \\(X_1=X_2\\) and \\(Z=0\\) otherwise. The joint distribution of \\(X_1\\) and \\(Z\\) can be represented in a table:\n\n\n\n\\(X_1 \\backslash Z\\)\n0 (Different)\n1 (Same)\n\n\n\n\n0 (Tails)\n0.4\n0.1\n\n\n1 (Heads)\n0.2\n0.3\n\n\n\nThe joint PMF is given by:\n\n\\(f_{X_1,Z}(0,0)=P(X_1=0, Z=0) = P(\\{TH\\}) = 0.4\\)\n\\(f_{X_1,Z}(0,1)=P(X_1=0, Z=1) = P(\\{TT\\}) = 0.1\\)\n\\(f_{X_1,Z}(1,0)=P(X_1=1, Z=0) = P(\\{HT\\}) = 0.2\\)\n\\(f_{X_1,Z}(1,1)=P(X_1=1, Z=1) = P(\\{HH\\}) = 0.3\\)\n\nThe marginal distributions of \\(X_1\\) and \\(Z\\) can be obtained by summing over the rows and columns respectively:\n\\[\nf_{X_1}(x_1) = \\sum_{z\\in\\{0,1\\}} f_{X_1,Z}(x_1,z)=\n\\begin{cases}\nf_{X_1,Z}(0,0) + f_{X_1,Z}(0,1) = 0.4 + 0.1=0.5 & \\text{if } x_1 = 0 \\\\\nf_{X_1,Z}(1,0) + f_{X_1,Z}(1,1) = 0.2 + 0.3=0.5 & \\text{if } x_1 = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\\[\nf_Z(z)= \\sum_{x_1\\in\\{0,1\\}} f_{X_1,Z}(x_1,z)=\n\\begin{cases}\nf_{X_1,Z}(0,0) + f_{X_1,Z}(1,0) = 0.4 + 0.2 =0.6 & \\text{if } z = 0 \\\\\nf_{X_1,Z}(0,1) + f_{X_1,Z}(1,1) = 0.1 + 0.3 = 0.4 & \\text{if } z = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\nTipExamples of joint continuous Distribution\n\n\n\n\nExample 1.24 Consider two continuous random variables \\(X\\) and \\(Y\\) with the joint PDF given by: \\[\nf_{X,Y}(x,y) =\n\\begin{cases}\n2 & \\text{if } 0 &lt; x &lt; 1 \\text{ and } 0 &lt; y &lt; x \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nWe can verify that this is a valid joint PDF by checking that it is non-negative and integrates to 1 over the entire plane: \\[\n\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy \\, dx = \\int_0^1 \\int_0^x 2 \\, dy \\, dx = \\int_0^1 2x \\, dx = 1\n\\] The support of the joint distribution is the triangular region in the \\(xy\\)-plane where \\(0 &lt; x &lt; 1\\) and \\(0 &lt; y &lt; x\\).\nThe marginal distributions can be computed as follows: \\[\nf_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy = \\int_0^x 2 \\, dy = 2x \\quad \\text{for } 0 &lt; x &lt; 1\n\\] \\[\nf_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx = \\int_y^1 2 \\, dx = 2(1 - y) \\quad \\text{for } 0 &lt; y &lt; 1\n\\]\nWe can also obtain the joint cumulative distribution function (CDF) as follows. For any \\(0&lt;y&lt;x&lt;1\\) \\[\n\\begin{aligned}\nF_{X,Y}(x,y) &= P(X \\leq x, Y \\leq y)\\\\\n& = \\int_{-\\infty}^x \\int_{-\\infty}^y f_{X,Y}(u,v) \\, dv \\, du\\\\\n&=\\int_{0}^y \\int_{0}^u f_{X,Y}(u,v) \\, dv \\, du+\\int_{y}^x \\int_{0}^y f_{X,Y}(u,v) \\, dv \\, du\\\\\n&=\\int_{0}^y \\int_{0}^u 2 \\, dv \\, du+\\int_{y}^x \\int_{0}^y 2 \\, dv \\, du\\\\\n&= \\int_{0}^y 2u \\, du+\\int_{y}^x 2y \\, du\\\\\n&= y^2 + 2y(x-y) \\\\\n\\end{aligned}\n\\]\nThe marginals CDFs can be computed as follows:\n\\[\nF_X(x) = \\lim_{y \\to \\infty} F_{X,Y}(x,y) = F_{X,Y}(x,y) = x^2 + 2x(x-x) = x^2 \\quad \\text{for } 0 &lt; x &lt; 1\n\\]\n\\[\nF_Y(y) = \\lim_{x \\to \\infty} F_{X,Y}(x,y) = F_{X,Y}(1,y) = y^2 + 2y(1-y) = 2y - y^2 \\quad \\text{for } 0 &lt; y &lt; 1\n\\]\nor, alternatively, we can compute the marginal CDFs directly from the marginal PDFs as follows:\n\\[\nF_X(x) = \\int_{-\\infty}^x f_X(u) \\, du = \\int_0^x 2u \\, du = x^2 \\quad \\text{for } 0 &lt; x &lt; 1\n\\] \\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(v) \\, dv = \\int_0^y 2(1 - v) \\, dv = 2y - y^2 \\quad \\text{for } 0 &lt; y &lt; 1\n\\]\nNow consider another joint PDF given by:\n\\[\nf_{X,Y}(x,y) =\n\\begin{cases}\n6xy^2 & \\text{if } 0 &lt; x &lt; 1 \\text{ and } 0 &lt; y &lt; 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] We can verify that this is a valid joint PDF by checking that it is non-negative and integrates to 1 over the entire plane: \\[\n\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy \\, dx = \\int_0^1 \\int_0^1 6xy^2 \\, dy \\, dx = \\int_0^1 2x \\, dx = 1\n\\] The support of the joint distribution is the unit square in the \\(xy\\)-plane where \\(0 &lt; x &lt; 1\\) and \\(0 &lt; y &lt; 1\\).\nThe marginal distributions can be computed as follows:\n\\[\nf_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dy = \\int_0^1 6xy^2 \\, dy = 2x \\quad \\text{for } 0 &lt; x &lt; 1\n\\] \\[\nf_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\, dx = \\int_0^1 6xy^2 \\, dx = 3y^2 \\quad \\text{for } 0 &lt; y &lt; 1\n\\] Then we have\n\\[\nf_{X,Y}(x,y)=f_X(x)f_Y(y)\n\\]\nso the random variables \\(X\\) and \\(Y\\) are independent. THis also implies that the joint cumulative distribution function (CDF) is given by: \\[\nF_{X,Y}(x,y) = F_X(x) F_Y(y)\n\\] where \\(F_X(x)\\) and \\(F_Y(y)\\) are the marginal CDFs of \\(X\\) and \\(Y\\), respectively. We can compute these marginal CDFs as follows: \\[\nF_X(x) = \\int_{-\\infty}^x f_X(u) \\, du = \\int_0^x 2u \\, du = x^2 \\quad \\text{for } 0 &lt; x &lt; 1\n\\] \\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(v) \\, dv = \\int_0^y 3v^2 \\, dv = y^3 \\quad \\text{for } 0 &lt; y &lt; 1\n\\] Therefore, the joint CDF is given by: \\[\nF_{X,Y}(x,y) = F_X(x) F_Y(y) = x^2 y^3 \\quad \\text{for } 0 &lt; x &lt; 1 \\text{ and } 0 &lt; y &lt; 1\n\\]\n\n\n\n\n\n\n\n\n\nNoteIndependent Random Variables\n\n\n\n\nDefinition 1.25 Two discrete random variables \\(X\\) and \\(Y\\) are independent if for all \\(x\\) and \\(y\\) in their respective image sets: \\[ P(X = x, Y = y) = P(X = x) \\times P(Y = y) \\] for all \\(x\\) and \\(y\\). Equivalently, if either of the following conditions hold for all \\(x\\) and \\(y\\):\n\n\\(P(X = x | Y = y) = P(X = x)\\)\n\\(P(Y = y | X = x) = P(Y = y)\\)\n\nfor all \\(x\\) and \\(y\\).\nFor the case of continuous random variables, \\(X\\) and \\(Y\\) are independent if for all \\(x\\) and \\(y\\) in their respective image sets: \\[\nf_{X,Y}(x, y) = f_X(x) \\times f_Y(y)\n\\]\nwhere \\(f_{X,Y}(x, y)\\) is the joint probability density function of \\(X\\) and \\(Y\\), and \\(f_X(x)\\) and \\(f_Y(y)\\) are the marginal probability density functions of \\(X\\) and \\(Y\\), respectively.\n\n\n\n\n\n\n\n\n\nNoteExample of Independent Random Variables\n\n\n\n\nExample 1.25 We can check if \\(X_1\\) and \\(X_2\\) define d in Example 1.23 are independent random variables. We have: \\[\nP(X_1=0, X_2=0) = 0.1 \\neq P(X_1=0)P(X_2=0) = 0.5\\times 0.4 = 0.2\n\\] so they are not independent.\nWe can check if \\(X_1\\) and \\(Z\\) defined in Example 1.23 are independent random variables. We have: \\[\nP(X_1=0, Z=0) = 0.4 = P(X_1=0)P(Z=0) = 0.5\\times 0.6 = 0.3\n\\] so they are not independent.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch1.html#conditional-distributions",
    "href": "ch1.html#conditional-distributions",
    "title": "1  Probability Theory for Data Scientists",
    "section": "1.7 Conditional distributions",
    "text": "1.7 Conditional distributions\n\n\n\n\n\n\nNoteConditional Distribution of two random variables\n\n\n\n\nDefinition 1.26 The conditional distribution of a random variable \\(X\\) given another random variable \\(Y\\) describes the probability distribution of \\(X\\) when the value of \\(Y\\) is known. It is represented by the conditional probability mass function (PMF) for discrete random variables or the conditional probability density function (PDF) for continuous random variables. For two discrete random variables \\(X\\) and \\(Y\\), the conditional PMF of \\(X\\) given \\(Y=y\\) is defined as: \\[\nf_{X|Y}(x|y) = P(X = x | Y = y) = \\frac{P(X = x, Y = y)}{P(Y = y)} = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]\nfor all possible values \\(x\\) in the image set of \\(X\\) and for all \\(y\\) such that \\(P(Y = y) &gt; 0\\). For two continuous random variables \\(X\\) and \\(Y\\), the conditional PDF of \\(X\\) given \\(Y=y\\) is defined as: \\[\nf_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\] for all possible values \\(x\\) in the image set of \\(X\\) and for all \\(y\\) such that \\(f_Y(y) &gt; 0\\).\n\n\n\n\n\n\n\n\n\nNoteExample of Discrete Conditional Distribution\n\n\n\n\nExample 1.26 We can compute the conditional distribution of \\(X_1\\) given \\(Z\\) in Example 1.23. We have: \\[\nf_{X_1|Z}(x|z) = \\frac{f_{X_1,Z}(x,z)}{f_Z(z)}\n\\] for all possible values \\(x\\) in the image set of \\(X_1\\) and for all \\(z\\) such that \\(f_Z(z) &gt; 0\\). We have:\n\n\\(f_{X_1|Z}(0|0) = \\frac{f_{X_1,Z}(0,0)}{f_Z(0)} = \\frac{0.4}{0.6} = \\frac{2}{3}\\)\n\\(f_{X_1|Z}(1|0) = \\frac{f_{X_1,Z}(1,0)}{f_Z(0)} = \\frac{0.2}{0.6} = \\frac{1}{3}\\)\n\nthe other conditional PMF is:\n\n\\(f_{X_1|Z}(0|1) = \\frac{f_{X_1,Z}(0,1)}{f_Z(1)} = \\frac{0.1}{0.4} = \\frac{1}{4}\\)\n\\(f_{X_1|Z}(1|1) = \\frac{f_{X_1,Z}(1,1)}{f_Z(1)} = \\frac{0.3}{0.4} = \\frac{3}{4}\\)\n\n\n\n\n\n\n\n\n\n\nTipExample of Continuous Conditional Distribution\n\n\n\n\nExample 1.27 Consider the joint PDF given in Example 1.24: \\[\nf_{X,Y}(x,y) =\n\\begin{cases}\n2 & \\text{if } 0 &lt; x &lt; 1 \\text{ and } 0 &lt; y &lt; x \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] We can compute the conditional distribution of \\(Y\\) given \\(X=x\\). We have: \\[\nf_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\] for all possible values \\(y\\) in the image set of \\(Y\\) and for all \\(x\\) such that \\(f_X(x) &gt; 0\\). We have: \\[\nf_{Y|X}(y|x) = \\frac{2}{2x} = \\frac{1}{x} \\quad \\text{for } 0 &lt; y &lt; x\n\\] and 0 otherwise.\nThe other conditional distribution is: \\[\nf_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\] for all possible values \\(x\\) in the image set of \\(X\\) and for all \\(y\\) such that \\(f_Y(y) &gt; 0\\). We have: \\[\nf_{X|Y}(x|y) = \\frac{2}{2(1-y)} = \\frac{1}{1-y} \\quad \\text{for } y &lt; x &lt; 1\n\\] and 0 otherwise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch1.html#moments-variance-covariance-and-correlation",
    "href": "ch1.html#moments-variance-covariance-and-correlation",
    "title": "1  Probability Theory for Data Scientists",
    "section": "1.8 Moments, Variance, covariance and correlation",
    "text": "1.8 Moments, Variance, covariance and correlation\n\n\n\n\n\n\nNoteEXpectation and Variance of a Random Variable\n\n\n\n\nDefinition 1.27 The expectation, expected value or mean of a random variable \\(X\\), denoted by \\(E[X]\\) or \\(\\mu_X\\), is defined as:\n\nFor a discrete random variable: \\[ E[X] = \\sum_{x \\in X(\\Omega)} x \\cdot P(X = x) = \\sum_{x \\in X(\\Omega)} x \\cdot f_X(x) \\]\nFor a continuous random variable: \\[ E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) \\, dx \\]\n\nThe variance of a random variable \\(X\\), denoted by \\(Var(X)\\) is defined as: \\[\nVar(X) = E[(X - E[X])^2]\n\\]\nIt is easy to show that:\n\\[\nVar(X) = E[X^2] - (E[X])^2\n\\]\nThe standard deviation of \\(X\\), is the square root of the variance: \\(\\sigma_X = \\sqrt{Var(X)}\\)\n\n\n\nThe variance measures the spread or dispersion of the random variable around its mean. A higher variance indicates that the values of the random variable are more spread out, while a lower variance indicates that they are more concentrated around the mean.\n\n\n\n\n\n\nNoteExamples of Expectation and Variance\n\n\n\n\nExample 1.28  \n\nFor a discrete uniform random variable with parameter \\(k\\):\n\n\\(E[X] = \\frac{k + 1}{2}\\)\n\\(Var(X) = \\frac{k^2 - 1}{12}\\)\n\nFor a bernoulli random variable with parameter \\(p\\):\n\n\\(E[X] = p\\)\n\\(Var(X) = p(1 - p)\\)\n\nFor a binomial random variable with parameters \\(n\\) and \\(p\\):\n\n\\(E[X] = np\\)\n\\(Var(X) = np(1 - p)\\)\n\nFor a hypergeometric random variable with parameters \\(n\\), \\(m\\) and \\(k\\):\n\n\\(E[X] = k\\frac{m}{n}\\)\n\\(Var(X) = k\\frac{m}{n}\\frac{n-m}{n}\\frac{n-k}{n-1}\\)\n\nFor a Poisson random variable with parameter \\(\\lambda\\):\n\n\\(E[X] = \\lambda\\)\n\\(Var(X) = \\lambda\\)\n\nFor the negative binomial random variable with parameters \\(r\\) and \\(p\\):\n\n\\(E[X] = \\frac{r}{p}\\)\n\\(Var(X) = \\frac{r(1 - p)}{p^2}\\)\n\\(E[Y] = \\frac{r(1-p)}{p}\\)\n\\(Var(X) = \\frac{r(1 - p)}{p^2}\\)\n\nFor a uniform random variable on the interval \\([a, b]\\):\n\n\\(E[X] = \\frac{a + b}{2}\\)\n\\(Var(X) = \\frac{(b - a)^2}{12}\\)\n\nFor a normal random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\n\\(E[X] = \\mu\\)\n\\(Var(X) = \\sigma^2\\)\n\nFor an exponential random variable with rate parameter \\(\\lambda\\):\n\n\\(E[X] = \\frac{1}{\\lambda}\\)\n\\(Var(X) = \\frac{1}{\\lambda^2}\\)\n\n\nThese can be derived from the definitions above and the corresponding probability mass or density functions.\n\n\n\n\n\n\n\n\n\nTipExpected values of functions of random variables\n\n\n\n\nDefinition 1.28 The expected value of a function \\(g(X)\\) of a random variable \\(X\\) is given by:\n\nFor a discrete random variable: \\[\nE[g(X)] = \\sum_{x \\in X(\\Omega)} g(x) \\cdot P(X = x) = \\sum_{x \\in X(\\Omega)} g(x) \\cdot f_X(x)\n\\]\nFor a continuous random variable: \\[\nE[g(X)] = \\int_{-\\infty}^{\\infty} g(x) \\cdot f_X(x) \\, dx\n\\]\n\nFor joint random variables \\(X\\) and \\(Y\\), the expected value of a function \\(g: \\mathbb{R}^2\\to \\mathbb{R}\\) is given by:\n\nFor discrete random variables: \\[\n\\begin{aligned}\nE[g(X, Y)] &= \\sum_{(x,y) \\in (X,Y)(\\Omega)} \\sum\\, g(x, y) \\cdot P(X = x, Y = y)\\\\\n& = \\sum_{(x,y) \\in (X,Y)(\\Omega)} \\sum\\, g(x, y) \\cdot f_{X,Y}(x,y)\n\\end{aligned}\n\\]\nFor continuous random variables: \\[\nE[g(X, Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x, y) \\cdot f_{X,Y}(x,y) \\, dx \\, dy\n\\]\n\n\n\n\n\n\n\n\n\n\nNoteHIgher order moments and moment generating function\n\n\n\n\nDefinition 1.29 The \\(n\\)-th moment of a random variable \\(X\\) is defined as: \\[\nE[X^n] =\n\\begin{cases}\n\\displaystyle \\sum_{x \\in X(\\Omega)} x^n \\cdot P(X =\nx) & \\text{if } X \\text{ is discrete} \\\\\n\\rule{0in}{4ex} \\displaystyle\\int_{-\\infty}^{\\infty} x^n \\cdot f_X\n(x) \\, dx & \\text{if } X \\text{ is continuous}\n\\end{cases}\n\\] The moment generating function (MGF) of a random variable \\(X\\) is defined as: \\[\nM_X(t) = E[e^{tX}] =\n\\begin{cases}\n\\displaystyle\\sum_{x \\in X(\\Omega)} e^{tx} \\cdot P(X\n= x) & \\text{if } X \\text{ is discrete} \\\\\n\\rule{0in}{4ex} \\displaystyle \\int_{-\\infty}^{\\infty} e^{tx} \\cdot f\n_X(x) \\, dx & \\text{if } X \\text{ is continuous}\n\\end{cases}\n\\] The MGF can be used to compute the moments of a random variable. The \\(n\\) -th moment of \\(X\\) can be obtained by taking the \\(n\\)-th derivative of the MGF and evaluating it at \\(t=0\\): \\[\nE[X^n] = M_X^{(n)}(0) = \\left. \\frac{d^n}{dt^n} M_X(t) \\right|_{t=0}\n\\]\nNotes:\n\nWe are assuming here that the MGF exists in a neighborhood of \\(t=0\\).\nThe derivative formula above does not depend on whether \\(X\\) is discrete or continuous.\nThe first moment is the mean: \\(E[X] = M_X'(0)\\)\nThe second moment is \\(E[X^2] = M_X''(0)\\)\nThe variance can be computed as: \\(Var(X) = M_X''(0) - (M_X'(0))^2\\)\nThe MGF uniquely determines the distribution of a random variable, if it exists in a neighborhood of \\(t=0\\).\n\n\n\n\n\n\n\n\n\n\nNoteBernoulli MGF and Moments\n\n\n\n\nExample 1.29 For a Bernoulli random variable with parameter \\(p\\). The MGF is given by \\[M_X(t) = 1 - p + pe^t\\]\nDifferentiating and evaluating at \\(t=0\\), we have:\n\n\\(M_X'(t) = pe^t\\) so \\(E[X] = M\n_X'(0) = p\\)\n\\(M_X''(t) = pe^t\\) so \\(E[X^2\n] = M_X''(0) = p\\)\n\\(Var(X) = M_X''(0) - (M_X'(0))\n^2 = p - p^2 = p(1-p)\\)\n\n\n\n\n\n\n\n\n\n\nNotePoisson MGF and Moments\n\n\n\n\nExample 1.30 For a Poisson random variable with parameter \\(\\lambda\\). The MGF is given by \\[M_X(t) = e^{\\lambda(e^t - 1)}\\] Differentiating and evaluating at \\(t=0\\), we have: * \\(M_X'(t) = \\lambda e^t e^{\\lambda(e^t - 1)}\\) so \\(E[X] = M_X'(0) = \\lambda\\) * \\(M_X''(t) = \\lambda e^t e^{\\lambda(e\n^t - 1)} + \\lambda^2 e^{2t} e^{\\lambda(e^t - 1)}\\) so \\(E[X^2] = M_X''(0) = \\lambda + \\lambda^2\\) * \\(Var(X) = M_X''(0) - (M_X'(0))\n^2 = \\lambda + \\lambda^2 - \\lambda^2 = \\lambda\\)\n\n\n\n\n\n\n\n\n\nNoteMFG and Moments example\n\n\n\n\nExample 1.31 A random variable \\(X\\) has the following MFG: \\[M_X(t) = \\frac{3}{3 - t}\\,,\\qquad t&lt;3\\]\nWe can obtain the moments without knowledge of the PDF or CDF as follows:\n\n\\(M_X'(t) = \\frac{3}{(3 - t)^2}\\) so \\(E[X] = M_X'(0) = \\frac{1}{3}\\)\n\\(M_X''(t) = \\frac{6}{(3 - t)^3}\\) so \\(E[X^2] = M_X''(0) = \\frac{\n6}{27}\\)\n\\(Var(X) = M_X''(0) - (M_X'(0))\n^2 = \\frac{6}{27} - \\frac{1}{9} =\\frac{6}{27} - \\frac{3}{27}=\\frac{1}{9}\\)\n\nNow, let \\(X\\) be an exponential random variable with rate parameter \\(\\lambda\\). The MGF is given by \\[M_X(t) = \\int_0^\\infty e^{tx} \\lambda e^{-\\lambda x} \\, dx = \\frac{\\lambda}{\\lambda - t}\\] for \\(t &lt; \\lambda\\). So the only distribution with the MGF given above is an exponential distribution with rate parameter \\(\\lambda = 3\\).\n\n\n\n\n\n\n\n\n\nNoteExpectation and Independence\n\n\n\n\nProposition 1.5 If \\(X\\) and \\(Y\\) are independent random variables, then:\n\n\\(E[XY] = E[X]E[Y]\\)\nMore generally, if \\(g\\) and \\(h\\) are functions, then \\(E[g(X)h(Y)] = E[g(X)]E[h(Y)]\\)\nIn particular,\n\n\\[\nM_{X+Y}(t) =E[e^{t(X+Y)}] = E[e^{tX}e^{tY}] = E[e^{tX}]E[e^{tY}] = M_X(t)M_Y(t)\n\\]\nthis means that the MGF of the sum of independent random variables is the product of their MGFs.\n\n\n\n\n\n\n\n\n\nTipSum of Bernoulli Random Variables\n\n\n\n\nExample 1.32 Let \\(X_1, X_2, \\ldots, X_n\\) be independent Bernoulli random variables with parameter \\(p\\). Let \\(S_n = X_1 + X_2 + \\cdots + X_n\\) be their sum. We can compute the MGF of \\(S_n\\) as follows: \\[\nM_{S_n}(t) = E[e^{tS_n}] = E[e^{t(X_1 + X_2 + \\cdots + X_n)}] = E[e^{tX_1}e^{tX_2}\\cdots e^{tX_n}] = E[e^{tX_1}]E[e^{tX_2}]\\cdots E[e^{tX_n}] = (M_{X_1}(t))^n\n\\] where we used the independence of the \\(X_i\\)’s. Since each \\(X_i\\) is a Bernoulli random variable with parameter \\(p\\), we have: \\[\nM_{X_i}(t) = 1 - p + pe^t\n\\] for all \\(i\\). Therefore, we have: \\[\nM_{S_n}(t) = (1 - p + pe^t)^n\n\\] which is the MGF of a Binomial random variable with parameters \\(n\\) and \\(p\\). Therefore, we conclude that \\(S_n\\) follows a Binomial distribution with parameters \\(n\\) and \\(p\\).\n\n\n\n\n\n\n\n\n\nTipSum of Poisson Random Variables\n\n\n\n\nExample 1.33 Let \\(X_1, X_2, \\ldots, X_n\\) be independent Poisson random variables with parameters \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\), respectively. Let \\(S_n = X_1 + X_2 + \\cdots + X_n\\) be their sum. We can compute the MGF of \\(S_n\\) as follows: \\[\nM_{S_n}(t) = E[e^{tS_n}] = E[e^{t(X_1 + X_2 + \\cdots + X_n)}] = E[e^{tX_1}e^{tX_2}\\cdots e^{tX_n}] = E[e^{tX_1}]E[e^{tX_2}]\\cdots E[e^{tX_n}] = \\prod_{i=1}^n M_{X_i}(t)\n\\] where we used the independence of the \\(X_i\\)’s. Since each \\(X_i\\) is a Poisson random variable with parameter \\(\\lambda_i\\), we have: \\[\nM_{X_i}(t) = e^{\\lambda_i(e^t - 1)}\n\\] for all \\(i\\). Therefore, we have: \\[\nM_{S_n}(t) = \\prod_{i=1}^n e^{\\lambda_i(e^t - 1)} = e^{(\\sum_{i=1}^n \\lambda_i)(e^t - 1)}\n\\] which is the MGF of a Poisson random variable with parameter \\(\\sum_{i=1}^n \\lambda_i\\). Therefore, we conclude that \\(S_n\\) follows a Poisson distribution with parameter \\(\\sum_{i=1}^n \\lambda_i\\).\n\n\n\n\n\n\n\n\n\nTipSum of Exponential Random Variables\n\n\n\n\nExample 1.34 Let \\(X_1, X_2, \\ldots, X_n\\) be independent exponential random variables with rate parameter \\(\\lambda\\). Let \\(S_n = X_1 + X_2 + \\cdots + X_n\\) be their sum. We can compute the MGF of \\(S_n\\) as follows: \\[\nM_{S_n}(t) = E[e^{tS_n}] = E[e^{t(X_1 + X_2 + \\cdots + X_n)}] = E[e^{tX_1}e^{tX_2}\\cdots e^{tX_n}] = E[e^{tX_1}]E[e^{tX_2}]\\cdots E[e^{tX_n}] = (M_{X_1}(t))^n\n\\] where we used the independence of the \\(X_i\\)’s. Since each \\(X_i\\) is an exponential random variable with rate parameter \\(\\lambda\\), we have: \\[\nM_{X_i}(t) = \\frac{\\lambda}{\\lambda - t}\n\\] for all \\(i\\) and for \\(t &lt; \\lambda\\). Therefore, we have: \\[\nM_{S_n}(t) = \\left(\\frac{\\lambda}{\\lambda - t}\\right)^n\n\\] which is the MGF of a Gamma random variable with shape parameter \\(n\\) and rate parameter \\(\\lambda\\). Therefore, we conclude that \\(S_n\\) follows a Gamma distribution with shape parameter \\(n\\) and rate parameter \\(\\lambda\\).\nWe have not defined the Gamma distribution yet. WE do this here for completeness. The Gamma distribution with shape parameter \\(k\\) and rate parameter \\(\\theta\\) has the following PDF: \\[\nf_X(x) =\n\\begin{cases}\n\\frac{\\theta^k}{\\Gamma(k)} x^{k-1} e^{-\\theta x} & \\text{if } x &gt; 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]§ where \\(\\Gamma(k)\\) is the Gamma function defined as: \\[\n\\Gamma(k) = \\int_0^\\infty x^{k-1} e^{-x} \\, dx\n\\] The mean and variance of a Gamma random variable are given by: * \\(E[X] = \\frac{k}{\\theta}\\) * \\(Var(X) = \\frac{k}{\\theta^2}\\)\n\n\n\n\n\n\n\n\n\nNoteCovariance and Correlation\n\n\n\n\nDefinition 1.30 The covariance between two random variables \\(X\\) and \\(Y\\), denoted by \\(Cov(X, Y)\\) is defined as: \\[\nCov(X, Y) = E[(X - E[X])(Y - E[Y])]\n\\] It is easy to show that: \\[\nCov(X, Y) = E[XY] - E[X]E[Y]\n\\] The covariance measures the linear relationship between two random variables. A positive covariance indicates that the variables tend to increase or decrease together, while a negative covariance indicates that one variable tends to increase when the other decreases. The correlation coefficient between two random variables \\(X\\) and \\(Y\\), denoted by \\(\\rho_{X,Y}\\) or \\(Corr(X, Y)\\), is defined as: \\[\nCorr(X, Y)=\\rho_{X,Y} := \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}} = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}\n\\] where \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X\\) and \\(Y\\), respectively.\nThe correlation coefficient measures the strength and direction of the linear relationship between two random variables. It ranges from -1 to 1, where 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\n\n\n\n\n\n\n\n\n\nTipExample of Covariance and Correlation\n\n\n\n\nExample 1.35 Using the joint distribution of \\(X_1\\) and \\(Z\\) in Example 1.23, we can compute the covariance and correlation between \\(X_1\\) and \\(Z\\). We have:\n\n\\(E[X_1]=E[X_1^2] = P(X_1=1)= 0.5\\)\n\\(E[Z]=E[Z^2]=P(Z=1) = 0.4\\)\n\\(E[X_1Z] =P(X_1=1,Z_1=1)= 0.3\\)\n\nThen we have:\n\n\\(Cov(X_1, Z) = E[X_1Z] - E[X_1]E[Z] = 0.3 - 0.5 \\times 0.4 = 0.1\\)\n\\(Var(X_1)=E[X_1^2] - (E[X_1])^2 = 0.5(1- 0.5) = 0.25\\)\n\\(Var(Z) = E[Z^2] - (E[Z])^2 = 0.4(1-0.4) = 0.24\\)\n\\(Corr(X_1,Z)=\\rho_{X_1,Z} = \\frac{Cov(X_1, Z)}{\\sqrt{Var(X_1)Var(Z)}} = \\frac{0.1}{\\sqrt{0.25 \\times 0.24}} \\approx 0.408\\)\n\n\n\n\nWe can doublecheck the correlation result above by simulating a large number of coin flips and computing the sample correlation between \\(X_1\\) and \\(Z\\).\n\n\nCode\nimport numpy as np\nn = 10000\nx1 = np.random.binomial(1, 0.5, n)\nx2 = np.random.binomial(1, 0.7, n)\nz = (x1 == x2).astype(int)\nprint(\"Sample correlation between X1 and Z:\", np.corrcoef(x1, z)[0, 1])\n\n\nSample correlation between X1 and Z: 0.4012426182680603\n\n\n\n\n\n\n\n\nTipExample of Covariance and Correlation\n\n\n\n\nExample 1.36 Now using the joint distribution of \\(X\\) and \\(Y\\) in Example 1.24\n\\[\nf_{X,Y}(x,y) =\n\\begin{cases}\n2 & \\text{if } 0 &lt; x &lt; 1  \\text{ and } 0 &lt; y &lt; x \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nwe can compute the covariance and correlation between \\(X\\) and \\(Y\\). We have:\n\n\\(E[X]=\\int_0^1 x \\cdot 2x \\, dx = \\frac{2}{3}\\)\n\\(E[Y]=\\int_0^1 y \\cdot 2(1-y) \\, dy = 1-2/3=1/3\\)\n\\(E[X^2]=\\int_0^1 x^2 \\cdot 2x \\, dx = \\frac{1}{2}\\)\n\\(E[Y^2]=\\int_0^1 y^2 \\cdot 2(1-y) \\, dy = \\frac{2}{3}-\\frac{1}{2}=\\frac{1}{6}\\)\n\\(Var(X)=E[X^2] - (E[X])^2 = \\frac{1}{2} - \\left(\\frac{2}{3}\\right)^2 = \\frac{1}{18}\\)\n\\(Var(Y)=E[Y^2] - (E[Y])^2 = \\frac{1}{6} - \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{18}\\)\n\\(E[XY]=\\int_0^1 \\int_0^x xy \\cdot 2 \\, dy \\, dx = \\int_0^1 x^3 \\, dx = \\frac{1}{4}\\)\n\nThen we have:\n\n\\(Cov(X, Y) = E[XY] - E[X]E[Y] = \\frac{1}{4} - \\frac{2}{3} \\times \\frac{1}{3} = \\frac{1}{36}\\)\n\\(Corr(X,Y)=\\rho_{X,Y} = \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}} = \\frac{\\frac{1}{36}}{\\sqrt{\\frac{1}{18} \\times \\frac{1}{18}}} = \\frac{1}{2}\\)\n\n\n\n\nNow assume we have two independent random variables \\(X\\) and \\(Y\\) uniformly distributed on the interval \\([0,1]\\).\nThen for any \\(x\\) and \\(y\\) in the interval \\([0,1]\\) such that \\(x&gt;y\\), we have:\n\\[\n\\begin{aligned}\nF_{X,Y}(x,y|X&gt;Y) &= P(X \\leq x, Y \\leq y|X&gt;Y)\\\\\n\\rule{0in}{3ex}&=\\frac{P(X\\leq x,Y\\leq y,X&gt;Y)}{P(X&gt;Y)}\\\\\n\\rule{0in}{3ex}&=\\frac{\\displaystyle\\int_0^y \\int_0^u f_X(u)f_Y(v)dvdu+\n\\int_y^x \\int_0^y f_X(u)f_Y(v)dvdu}\n{\\displaystyle \\int_0^1 \\int_0^u f_X(u)f_Y(v)dvdu}\\\\\n\\rule{0in}{3ex}&=\\frac{\\displaystyle\\int_0^y \\int_0^u dvdu+\n\\int_y^x \\int_0^y dvdu}\n{\\displaystyle \\int_0^1 \\int_0^u dvdu}\\\\\n\\rule{0in}{5ex}&=\\frac{\\frac{y^2}{2}+y(x-y)}{1/2}\\\\\n\\rule{0in}{4ex}&=y^2+2y(x-y)\\,,\\quad 0&lt;y&lt;x&lt;1\\\\\n\\end{aligned}\n\\]\nwhich is the same cdf in Example 1.24.\nThis gives us a way to simulate the joint distribution of \\(X\\) and \\(Y\\) given \\(X&gt;Y\\). We can first simulate two independent uniform random variables \\(X\\) and \\(Y\\) on the interval \\([0,1]\\), and then keep only the pairs \\((X,Y)\\) such that \\(X&gt;Y\\). Wer show below a Python code to do this and in passing we double check the correlation result above.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 10000\nx = np.random.uniform(0, 1, n)\ny = np.random.uniform(0, 1, n)\nz = x&gt;y\ny = y[z]\nx = x[z]\n\n# can compute the mean of each variable\n\nmean_x = np.mean(x)\nmean_y = np.mean(y)\nvar_x = np.var(x)\nvar_y = np.var(y)\n# display the means and variances\nprint(\"Mean of X:\", mean_x)\nprint(\"Mean of Y:\", mean_y)\nprint(\"Variance of X:\", var_x)\nprint(\"Variance of Y:\", var_y)\nprint(\"Correlation:\", np.corrcoef(x, y)[0, 1])\n\n\nplt.scatter(x, y)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Scatter plot of X and Y')\nplt.show()\n\n\nMean of X: 0.6692973374384267\nMean of Y: 0.332832062026518\nVariance of X: 0.056157980650828575\nVariance of Y: 0.054698942996425304\nCorrelation: 0.49752576280284877\n\n\n\n\n\n\n\n\n\nThe scatter plot shows a negative correlation between \\(X\\) and \\(Y\\), which is consistent with our calculation of the correlation coefficient.\n\n\n\n\n\n\nNoteCovariance and correlation under independence\n\n\n\n\nProposition 1.6 If \\(X\\) and \\(Y\\) are independent random variables, then \\(E[XY] = E[X]E[Y]\\) and therefore\n\n\\(Cov(X, Y) = 0\\)\n\\(\\rho_{X,Y} = 0\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability Theory for Data Scientists</span>"
    ]
  },
  {
    "objectID": "ch2.html",
    "href": "ch2.html",
    "title": "2  Random Sample and Sampling Distributions",
    "section": "",
    "text": "2.1 Random sample\nStatistics is the science of collecting, analysing, and interpreting data. The earliest applications of statistics were on demographic and economic measures and were driven by the state (from where the name “statistics” comes).\nWe use statistics when we want to draw conclusions about a set of individuals which we are unable to examine in its entirety. We then define the population as the set of individuals that we want to draw conclusions about while the sample is defined as the portion of the population that we actually examine. The number of individuals in the sample corresponds to the sample size. The measured characteristic from each individual in the sample is a random variable and the collection of characteristics from all individuals in the sample is called a random sample. Each element in the random sample is an observation from the same population. The set of all possible values of these random variables is called the sample space.\nOften we wish to measure some unknown characteristic of the population. A characteristic of the population is called a parameter. The set of all possible values of the parameters is called the parameter space. We use the sample to infer the value of the parameter. Any quantity calculated from the sample is called a statistic. A statistic is therefore a random variable and its distribution is called the sampling distribution.\nOften we are interested in the joint distribution of our sample. Let \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim f(x\\mid\\theta)\\). Then the joint pdf/pmf of \\(X_1,\\ldots,X_n\\) is \\[\nf(x_1,\\ldots,x_n\\mid\\theta) = f(x_1\\mid\\theta)\\times\\cdots\\times f(x_n\\mid\\theta) = \\prod_{i=1}^n f(x_i\\mid\\theta),\n\\] where the first equality is true because the random variables are mutually independent.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Sample and Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "ch2.html#random-sample",
    "href": "ch2.html#random-sample",
    "title": "2  Random Sample and Sampling Distributions",
    "section": "",
    "text": "Example 2.1 Market research organisations conduct opinion polls regularly. Figure 1.1 shows the result of such a poll. This poll was conducted by the firm YouGov on 15 October 2021. The question asked participants to state whether they felt older, the same, or younger than their real age. We can see that 4621 adults from Great Britain responded to this question, so the sample size is \\(n=4621\\). This number is significantly lower than the adult population of Great Britain, but it would have been impractical for YouGov to poll every adult.\nFrom the results of the poll we can see that 16% of the responders feel older than their real age, 32% feel the same, and 47% feel younger. The remaining 5% said they don’t know. Although these results are derived from the sample, if we assume that the sample is properly chosen, then we can claim that the corresponding proportions in the whole population would be similar.\n\n\n\n\n\n\nFigure 2.1: An example of a poll Source: YouGov\n\n\n\n\n\nExample 2.2 The Office for National Statistics (ONS) wishes to measure the unemployment rate in the UK. To that end, it chooses people of working age within the UK and asks them whether they are employed or seeking employment. The proportion among those asked who are seeking employment can be used to estimate the unemployment rate. Figure 1.2 shows a typical warning appearing on ONS’s webpage regarding uncertainty in their estimates of population measures.\nIn this example the population consists of all individuals able to work in the UK. The parameter we wish to estimate is the unemployment rate \\(p\\) which is a proportion so the parameter space is the set \\([0,1]\\). Because the ONS cannot ask every individual, it asks a subset of the population. The individuals asked consist of the sample. The proportion in the sample seeking employment is a statistic because it is calculated from the sample and not the whole population.\nSuppose \\(n\\) individuals were asked and let \\(X_i\\) denote the response of the \\(i\\)th individual, \\(i=1,\\dots,n\\). We let \\(X_i=1\\) if the \\(i\\)th individual is seeking employment and \\(0\\) if not so in this case the sample space is the set \\(\\{0,1\\}\\). The random sample is the set \\(\\{X_1,\\dots,X_n\\}\\). The proportion in the sample is also the mean of the \\(X_i\\)’s, denoted by \\(\\bar X\\). Each \\(X_i\\) is distributed as \\(X_i\\sim\\text{Bernoulli}(p)\\), so the sampling distribution of \\(\\bar X\\) is the distribution of the sample proportion, \\(\\text{Bin}(n,p)/n\\).\n\n\n\n\n\n\nFigure 2.2: ONS uncertainty note (Source: www.ons.gov.uk)\n\n\n\n\n\n\n\n\n\n\nNoteRandom sample\n\n\n\n\nDefinition 2.1 The random variables \\(X_1,\\ldots,X_n\\) are called a random sample of size \\(n\\) from the population \\(f(x\\mid\\theta)\\) depending on a parameter \\(\\theta\\) if \\(X_1,\\ldots,X_n\\) are mutually independent random variables and the probability density/mass function (pdf/pmf) of each \\(X_i\\) is the same function \\(f(x\\mid\\theta)\\). The variables \\(X_1,\\ldots,X_n\\) are also called independent and identically distributed (iid) random variables. We write \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim f(x\\mid\\theta)\\).\n\n\n\n\n\nExample 2.3 Let \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim \\text{Exponential}(\\mu)\\), where \\(\\mu\\) denotes the mean of the distribution. For example \\(X_1,\\ldots,X_n\\) may correspond to the failure times (measured in years) for \\(n\\) identical circuit boards that are put to test and used until they fail and \\(\\mu\\) denotes the average lifetime. Note that with this notation, the rate parameter is \\(\\lambda=1/\\mu\\).\nEach \\(X_i\\) has pdf \\(f(x\\mid\\mu) = \\frac{1}{\\mu}\\exp\\!\\left(-\\frac{x}{\\mu}\\right)\\), so the joint pdf of the sample is \\[\nf(x_1,\\ldots,x_n\\mid\\mu) = \\prod_{i=1}^n f(x_i\\mid\\mu) = \\frac{1}{\\mu^n}\\exp\\!\\left(-\\frac{1}{\\mu}\\sum_{i=1}^n x_i\\right).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Sample and Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "ch2.html#statistics-and-their-sampling-distributions",
    "href": "ch2.html#statistics-and-their-sampling-distributions",
    "title": "2  Random Sample and Sampling Distributions",
    "section": "2.2 Statistics and their sampling distributions",
    "text": "2.2 Statistics and their sampling distributions\nIn statistical inference, we are interested in describing the distribution of the population. In most cases, a suitable calculation using the sampled values can help.\n\n\n\n\n\n\nNoteStatistic and its sampling distribution\n\n\n\n\nDefinition 2.2 Let \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim f(x\\mid\\theta)\\). A function \\(T=T(X_1,\\ldots,X_n)\\) of the variables \\(X_1,\\ldots,X_n\\), which does not depend on \\(\\theta\\), is called a statistic. The statistic is itself a random variable. The probability distribution of \\(T\\) is called its sampling distribution.\n\n\n\nIn other words, any quantity that is calculated using the sample is a statistic. Another way to think of the sampling distribution is as the distribution of all possible values of \\(T\\) for all possible random samples of size \\(n\\) from the population \\(f(x\\mid\\theta)\\).\n\nExample 2.4 Let \\(X_1,\\ldots,X_n\\) be a random sample of size \\(n\\). Two of the most frequently used statistics are the sample mean, \\(\\bar X\\), and the sample variance \\(S^{2}\\) defined by \\[\n\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i, \\qquad\nS^{2} = \\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2.\n\\]\nSuppose \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim \\mathcal N(\\mu,\\sigma^2)\\). Then, the sampling distribution of \\(\\bar X\\) is \\(\\bar X \\sim \\mathcal N(\\mu,\\sigma^2/n)\\), i.e., the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\), and the sampling distribution of \\(S^{2}\\) is \\(\\frac{(n-1)S^{2}}{\\sigma^2}\\sim \\chi^2_{n-1}\\), i.e., the chi-squared distribution with \\(n-1\\) degrees of freedom times the constant \\(\\sigma^2/(n-1)\\). Moreover, \\(\\bar X\\) and \\(S^{2}\\) are independent in the case of normal populations.\n\nThe sampling distribution is not always easy to derive, either because the distribution of the population is unknown or because the statistic does not have a straightforward expression. Sometimes we can state asymptotic results as the sample size increases.\n\n\n\n\n\n\nImportantLaw of large numbers\n\n\n\n\nTheorem 2.1 Let \\(X_1,\\ldots,X_n\\) be a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2&lt;\\infty\\). Then, the sample mean \\(\\bar X\\) approximates the population mean \\(\\mu\\) when the sample size \\(n\\) is large.\nFormally, for any small error \\(\\varepsilon&gt;0\\), \\[\n\\Pr\\left(\\,|\\bar X-\\mu|\\ge \\varepsilon\\,\\right) \\to 0 \\quad \\text{as } n\\to\\infty.\n\\]\n\n\n\n\n\n\n\n\n\nTipNot examinable\n\n\n\n\nProof. This is easily proved by Chebyshev’s inequality: for any random variable \\(Y\\) with variance, \\(\\Pr\\{|Y|\\ge r\\} \\le \\frac{\\operatorname{Var}(Y)}{r^2}\\) for all \\(r&gt;0\\). Hence, \\[\n\\Pr\\{\\,|\\bar X-\\mu|\\ge \\varepsilon\\,\\} \\le \\frac{\\operatorname{Var}(\\bar X)}{\\varepsilon^2} = \\frac{\\sigma^2/n}{\\varepsilon^2} = \\frac{\\sigma^2}{n\\,\\varepsilon^2} \\to 0 \\quad \\text{as } n\\to\\infty.\n\\]\nThe law of large numbers simply states that the probability of small deviations of the sample mean from the population mean can be made very small if we choose a large enough sample size.\n\n\n\n\nExample 2.5 Suppose \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim \\text{Exponential}(\\mu)\\). Then \\(\\mathbb E[X_i]=\\mu\\), therefore \\(\\mathbb E[\\bar X]=\\mu\\). The law of large numbers says that the probability that \\(|\\bar X-\\mu|\\) exceeds a small number \\(\\varepsilon\\) can become arbitrarily small by increasing the sample size \\(n\\). This is illustrated by the following Python code.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport scipy\nimport scipy.stats as st\nimport numpy as np\n\nN = 10000  # Max sample size\nmu = 1     # The mean (scale) parameter\nx = st.expon.rvs(size=N, scale=mu)\nxbar = (x.cumsum()) / (np.arange(1, N+1))\n\nplt.plot(xbar, label='Running mean $\\\\bar X_n$')  # xbar at n = 1,2,...,N\nplt.axhline(mu, ls='--', color='k', label='True mean $\\\\mu$')\nplt.xlabel('n')\nplt.ylabel('Mean')\nplt.legend()\nplt.title('Law of Large Numbers for Exponential($\\\\mu$)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExample 2.6 The game of roulette — and why the house always wins. In the game of roulette, a wheel consisting of 37 pockets, numbered 0 to 36, is spun and a ball is dropped onto it (see Figure 1.3). The ball will eventually come to rest in one of the numbered pockets. Players can bet money on the outcome of the spin and win money if they guess correctly.\nSuppose a player bets £1 on a specific number \\(x\\). This player will win £35 if the ball lands in \\(x\\), otherwise, they lose their bet of £1. In other words, their profit is \\(+35\\) if they win the bet and \\(-1\\) if they lose the bet. Let \\(X\\) denote the outcome of the wheel spin, and let \\(W\\) denote the player’s winnings after one bet. The expected value of \\(W\\) is \\[\n\\mathbb E[W] = 35\\,\\Pr\\{X=x\\} - 1\\cdot\\Pr\\{X\\ne x\\} = 35\\cdot\\frac{1}{37} - 1\\cdot\\frac{36}{37} = \\frac{35-36}{37} = -\\frac{1}{37} \\approx -0.027.\n\\]\nWe observe that the expected winnings, from a player’s point of view, are negative. This does not mean that a player loses money at every bet, and in fact, it is possible that any one player will win big. However, in a typical day, there are thousands of bets taking place. The average winnings from these bets will converge to the distribution mean of \\(-0.027\\), so collectively every player loses about 2.7 pence per £1 bet on average.\n\n\n\n\n\n\nFigure 2.3: Roulette wheel\n\n\n\n\nThe sample mean is ubiquitous in statistics and it is important to know its sampling distribution. The next theorem summarises the large-sample behaviour of the sample mean.\n\n\n\n\n\n\nImportantCentral limit theorem\n\n\n\n\nTheorem 2.2 Let \\(X_1,\\ldots,X_n\\) be a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2&lt;\\infty\\). Then, the sampling distribution of the sample mean \\(\\bar X\\) can be approximated by the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\), i.e., \\(\\mathcal N(\\mu,\\sigma^2/n)\\), for large sample size \\(n\\).\nFormally, let \\(Z_n = \\sqrt{n} (\\bar X-\\mu)/\\sigma\\). Then, for any \\(z\\in\\mathbb R\\), \\[\n\\Pr\\{Z_n&lt;z\\} \\to \\Phi(z) \\quad \\text{as } n\\to\\infty,\n\\] where \\(\\Phi\\) denotes the CDF of the \\(\\mathcal N(0,1)\\) distribution.\n\n\n\nIn other words, the central limit theorem says that the CDF of \\(\\bar X\\) and the CDF of \\(\\mathcal N(\\mu,\\sigma^2/n)\\) are visually indistinguishable for large sample size. Since in many cases we cannot come up with the sampling distribution of the sample mean, the approximate normal distribution can be used assuming that the sample size is large.\n\n\n\n\n\n\nTipNot examinable\n\n\n\n\nProof. We will prove this theorem by showing that the moment generating function (mgf) of \\(Z_n\\), \\(M_n(t)\\), converges, as \\(n\\to\\infty\\), to the moment generating function of \\(\\mathcal N(0,1)\\). Since the mgf determines the distribution of the random variable uniquely, it follows that the limiting distribution of \\(Z_n\\) is \\(\\mathcal N(0,1)\\). Without loss of generality, we can assume \\(\\mu=0\\). In this case \\(Z_n = \\sqrt{n}\\,\\bar X/\\sigma = \\sum X_i/(\\sqrt{n}\\,\\sigma)\\). If \\(\\mu\\ne 0\\), we can apply the theorem to the random variables \\(Y_i = X_i-\\mu\\) and then substitute \\(\\bar Y\\) with \\(\\bar X-\\mu\\).\nLet \\(M_X(t)\\) denote the mgf of \\(X_i\\), i.e., \\(M_X(t)=\\mathbb E[e^{tX_i}]\\). By the properties of the mgf, the mgf of \\(Z_n\\) is \\[\nM_n(t) = \\mathbb E\\big[e^{t Z_n}\\big] = \\prod_{i=1}^n M_X\\!\\left(\\tfrac{t}{\\sqrt{n}\\,\\sigma}\\right) = \\left\\{ M_X\\!\\left(\\tfrac{t}{\\sqrt{n}\\,\\sigma}\\right) \\right\\}^{n}.\n\\] The mgf of \\(\\mathcal N(0,1)\\) is \\(M(t)=\\exp(t^2/2)\\). We will show that \\(\\lim_{n\\to\\infty} \\log M_n(t) = \\log M(t)\\), i.e., \\[\n\\lim_{n\\to\\infty} n\\, \\log M_X\\!\\left(\\tfrac{t}{\\sqrt{n}\\,\\sigma}\\right) = \\tfrac{t^2}{2}.\n\\] Let \\(u=1/\\sqrt{n}\\) and consider the limit \\(\\lim_{u\\to 0} \\dfrac{\\log M_X\\!\\left(\\tfrac{tu}{\\sigma}\\right)}{u^2}\\). Using L’Hôpital’s rule twice (and the facts \\(M_X(0)=1\\), \\(M_X'(0)=\\mathbb E[X_i]=0\\), \\(M_X''(0)=\\mathbb E[X_i^2]=\\sigma^2\\)), we obtain the desired limit \\(t^2/2\\).\n\n\n\n\nExample 2.7 Suppose \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim \\text{Exponential}(\\mu)\\). Then \\(\\mathbb E[X_i]=\\mu\\) and \\(\\operatorname{Var}(X_i)=\\mu^2\\). By the central limit theorem, the distribution of \\(\\bar X\\) is approximately \\(\\mathcal N(\\mu,\\mu^2/n)\\) for large \\(n\\). This is illustrated by the following Python code.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as st\n\nN = 10000  # Number of repetitions\nn = 50     # Sample size for each repetition\nmu = 1.0   # The mean (scale) parameter\n\nx = st.expon.rvs(size=(N, n), scale=mu)\nxbar = x.mean(axis=1)  # Sample mean across rows\n\nxx = np.linspace(xbar.min(), xbar.max(), 200)\nplt.hist(xbar, density=True, bins=50, alpha=0.5, facecolor='gray', label='Simulated $\\\\bar X$')\nplt.plot(xx, st.norm.pdf(xx, mu, mu/np.sqrt(n)), 'r-', lw=2, label='Normal approx')\nplt.legend()\nplt.title('Sampling distribution of $\\\\bar X$ vs Normal approximation')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Sample and Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "ch2.html#exercises",
    "href": "ch2.html#exercises",
    "title": "2  Random Sample and Sampling Distributions",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\n\nA coffee shop buys roasted coffee from a supplier. In order to assess the quality of the supplied coffee, the manager of the shop conducts a tasting experiment where she selects a small portion of coffee beans from different batches and tastes the coffee from each portion. For each portion she gives a score in the scale \\(1,2,\\ldots,10\\) with 10 corresponding to coffee of the best taste and uses the results to assess the quality of the coffee. Identify the population, parameter, and statistic.\nRead the abstract of the article: Dietary Intake of Marine n-3 Fatty Acids, Fish Intake, and the Risk of Coronary Disease among Men by Ascherio and others published in The New England Journal of Medicine in 1995. Identify the population, parameter, sample, and statistic.\nLet \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim \\mathcal N(\\mu,\\sigma^2)\\). Derive the sampling distribution of \\(\\bar X\\) given in Example 1.4.\nLet \\(X_1,\\ldots,X_n\\;\\text{iid}\\sim \\text{Bernoulli}(p)\\).\n\nDerive the sampling distribution of \\(\\bar X\\), i.e., for \\(x\\in\\{0/n,1/n,2/n,\\ldots,n/n\\}\\) find the probability \\(\\Pr\\{\\bar X=x\\}\\).\nHint. Let \\(W=\\sum_{i=1}^n X_i\\) so that \\(\\bar X=W/n\\). First find the distribution of \\(W\\) and use that to find \\(\\Pr\\{\\bar X=x\\}\\).\nDerive the asymptotic distribution of \\(\\bar X\\) from the central limit theorem.\nDraw a graph of the exact and approximate CDFs when \\(n=20\\) and \\(p=0.4\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Random Sample and Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "ch3.html",
    "href": "ch3.html",
    "title": "3  Decision Theory",
    "section": "",
    "text": "3.1 Mathematical formulation\nDecision theory is the branch of statistics and probability concerned with making decisions based on data. In many aspects of real life, we are asked to make a decision, which will impact our future in some way, with limited information. This chapter is about putting a mathematical framework around this concept and using that to analyse decisions.\nIn decision theory, the person making the decision, the decision-maker, is given a set of possible actions, \\(\\mathcal{A}\\), and data, \\({\\mathbf{x}}\\). It is rarely the case that we can make decisions having complete information. The unknown state of nature is represented by a parameter, \\(\\theta \\in \\Theta\\), and the task is to choose the best possible action \\(a \\in \\mathcal{A}\\), according to some loss function \\(L(\\theta,a)\\).\nTo set the mathematical framework, we first define the terms that comprise a decision problem.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "ch3.html#sec-math_form",
    "href": "ch3.html#sec-math_form",
    "title": "3  Decision Theory",
    "section": "",
    "text": "Example 3.2 Below are some examples of decision problems in various areas.\n\nWhen building flood defences around rivers, the decision is how high to build them. Higher defences protect against extreme rainfalls, however, they cost more. The decision in this case is the height of the barrier, the loss is a function of the construction cost and the economic damage in the event of barrier failure, while the state of nature would be the probability of flooding in any year. The data in this case consist of river heights in previous years.\nWhen playing poker, the decisions are whether to fold, call, or raise the bid, and if so, by how much (which is a number between 0 and our current money). The unknown state of nature is the probability of winning. The data are the player’s hand and the bids, while the uncertainty comes from not knowing the opponents’ hands. The loss in this case is the money that we will lose from playing the game.\nA football coach must decide who and how they should play. The uncertainty comes from how the opposing team plays. The unknown state of nature is the probabilities that each team scores a goal. The data in this case consist of the performances of the teams in previous matches and the loss function represents the final score of the match.\nFacing a pandemic, the government must decide what measures are appropriate ranging from complete indifference to total lockdown. The data consist of the daily infection numbers and advice from scientific advisors, while the uncertain parameters are the reproduction rate of the disease. The loss in this case can be measured in terms of the number of deaths combined with the impact of the measures to the economy.\n\n\n\n\n\n\n\n\n\nNoteDecision problem\n\n\n\n\nDefinition 3.1 A decision problem consist of the following elements.\n\nA parameter \\(\\theta \\in \\Theta\\), where \\(\\Theta\\) is the parameter space. The parameter represents the unknown state of nature.\nA set of data \\({\\mathbf{x}}\\in \\mathcal{X}\\), where \\(\\mathcal{X}\\) is the sample space. The data are assumed to be a random sample from a population depended on the unknown parameter \\(\\theta\\), with distribution \\(f({\\mathbf{x}}|\\theta)\\).\nAn action \\(a \\in \\mathcal{A}\\), where \\(\\mathcal{A}\\) is the action space, i.e., the set of possible actions that we can take.\nA loss function \\(L: \\Theta \\times \\mathcal{A} \\mapsto \\mathbb{R}\\), such that \\(L(\\theta,a)\\) denotes the loss when the true parameter value is \\(\\theta\\) and the decision-maker chooses action \\(a\\). We prefer lower values of \\(L\\).\n\n\n\n\n\nExample 3.3 The three classical examples of loss functions are the quadratic, absolute, and 0-1 loss. The first two are mainly used in the context of parameter estimation, while the latter in hypothesis testing.\n\nThe quadratic loss is defined by \\[L(\\theta,a) = (\\theta - a)^2.\\]\nThe absolute loss is defined by \\[L(\\theta,a) = |\\theta - a|.\\]\nThe 0-1 loss is defined by \\[L(\\theta,a) =\n      \\begin{cases}\n        0 & \\text{ if $a = \\theta$,} \\\\ 1 & \\text{ if $a \\neq \\theta$.}\n      \\end{cases}\\]\n\nOf course, other loss functions are possible, depending on the scenario.\n\n\n\n\n\n\n\nNote\n\n\n\nOften in life, we evaluate the wisdom of our actions only after observing a single outcome. The loss function should not capture the loss incurred from a single event; instead, it should reflect the average loss across multiple occurrences of such events. This means the loss function is designed to measure the overall success of an action by taking into account repeated outcomes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "ch3.html#sec-decision-rule",
    "href": "ch3.html#sec-decision-rule",
    "title": "3  Decision Theory",
    "section": "3.2 Decision rule",
    "text": "3.2 Decision rule\nIn practice, the true state of nature, \\(\\theta\\), is unknown. The data, \\({\\mathbf{x}}\\) provide some information about \\(\\theta\\) that we want to utilise to inform about our action. The solution to the decision problem is obtained by finding a decision rule \\(d: \\mathcal{X} \\mapsto \\mathcal{A}\\), such that \\(d({\\mathbf{x}})\\) incorporates the data in some way to determine the appropriate action to take. You can think of the decision rule as the strategy for choosing an action, given data \\({\\mathbf{x}}\\).\n\nExample 3.4 A traveller buying a flight ticket is considering whether to also buy travel insurance that pays up £1000 in the event of a flight cancellation. The travel insurance costs £50. In the event that the flight is cancelled, she will loose her hotel deposit which is £500. The available actions in this case are \\(\\mathcal{A} = \\{0,1\\}\\) with 1 representing “buy travel insurance” and 0 being “don’t buy travel insurance”.\nTo assess the probability of her flight being cancelled, \\(\\theta\\), she decides to look into how many times, in the past 10 years, a similar flight was cancelled. Let \\(x\\) be the proportion of times a flight was cancelled. A decision rule \\(d(x)\\) may be\n\\[\n    d(x) = %% \\mathds{1}_{\\{x \\geq 0.10\\}} =\n    \\begin{cases}\n      1 & \\text{ if $x \\geq 0.10$,} \\\\ 0 & \\text{ if $x &lt; 0.10$,}\n\\end{cases}\n\\tag{3.1}\\] in other words, the traveller’s strategy is to buy travel insurance if they find that at least 10% of the past flights were cancelled, and not buy travel insurance otherwise.\nLet \\(y\\) denote the future event that the flight be cancelled. We set \\(y=1\\) if the flight is cancelled, and \\(y=0\\) if the flight is not cancelled. We define the function \\(l(y,a)\\) to denote the loss when we take action \\(a\\) and the event \\(y\\) occurs. Then, if we do buy insurance (\\(a=1\\)), our loss is \\(50\\) if the flight is not cancelled (the cost of the insurance), and \\(50 + 500 - 1000 = -450\\) if the flight is cancelled (the cost of the insurance plus the hotel deposit, but we receive a payment of 1000). On the other hand, if we do not buy insurance (\\(a=0\\)), and our flight is not cancelled, our loss is 0, however if the flight is cancelled our loss is \\(500\\) (the hotel deposit). Putting these together gives\n\\[\n\\begin{aligned}\n    l(y,a=0) &=\n    \\begin{cases}\n      0 & \\text{ if $y=0$,} \\\\ 500 & \\text{ if $y=1$,}\n    \\end{cases}\n    &\n    l(y,a=1) &=\n    \\begin{cases}\n      50 & \\text{ if $y=0$,} \\\\ -450 & \\text{ if $y=1$.}\n    \\end{cases}\n\\end{aligned}\n\\]\nAccording to our problem, \\({\\mathds{P}}(y = 1) = \\theta\\) and \\({\\mathds{P}}(y = 0) =\n  1-\\theta\\), so the loss function for this problem is computed as the expected value of \\(l(y,a)\\) over the distribution of \\(y\\):\n\\[\n\\begin{aligned}\n    L(\\theta,0) ={}& \\mathop{\\mathrm{{\\mathsf E}}}l(y,0) = 0 \\times \\mathds{P}(y=0) + 500 \\times {\\mathds{P}}(y=1) =\n    0\\times(1-\\theta) + 500 \\times \\theta = 500\\theta \\nonumber \\\\\n    L(\\theta,1) ={}& \\mathop{\\mathrm{{\\mathsf E}}}l(y,1) = 50 \\times {\\mathds{P}}(y=0) - 450\\times {\\mathds{P}}(y=1) =\n    50 \\times (1-\\theta) - 450 \\times \\theta = 50 - 500  \\theta.\n\\end{aligned}\n\\]\nThis can be combined as\n\\[\n  L(\\theta,a) =\n    \\begin{cases}\n      500 \\theta & \\text{ if $a=0$,} \\\\ 50 - 500\\theta & \\text{ if $a=1$.}\n\\end{cases}\n\\tag{3.2}\\]\nIn other words, if she does not buy insurance (\\(a=0\\)), the potential loss is \\(L(\\theta,a=0) = 500\\times\\theta\\), so the hotel cost times the probability of the flight being cancelled. Note that the actual loss is going to be 500 if the flight is cancelled, and 0 if the flight is not cancelled, but at this point we don’t know whether the flight will be cancelled or not, so \\(500\\times\\theta\\) is in fact the expected loss under a future cancellation event. Similarly, if she does buy insurance (\\(a=1\\)), then the loss is \\(L(\\theta,a=1) = 50 + (500 - 1000)\\times\\theta = 50 - 500\\times\\theta\\), where \\(50\\) is the insurance cost and will have to pay \\(500\\times\\theta\\) for the hotel cost but receive a payment of \\(1000\\times\\theta\\) from the insurance. The loss functions for the two decisions are shown in Figure 3.2 .\n\n\n\n\n\n\nFigure 3.2: Losses for Example 3.4. The two lines intersect at \\(\\theta=0.05\\). We can see that, at \\(\\theta &gt; 0.05\\), the losses from not buying insurance are greater than the losses from buying, so if we believe that there is a greater than 5% chance that the flight is cancelled, we will want to buy insurance because in this case we minimise our losses. If we believe that \\(\\theta &lt; 0.05\\), then it is the other way around.\n\n\n\nIt is clear that as \\(\\theta \\rightarrow 1\\), i.e., there is increasing chance that the flight will be cancelled, then \\(L(\\theta,a=0) \\rightarrow\n  500\\) and \\(L(\\theta,a=1) \\rightarrow -450\\), so the loss in this case is minimised when \\(a=1\\). Similarly, as \\(\\theta \\rightarrow 0\\), \\(L(\\theta,a=0) \\rightarrow 0\\) and \\(L(\\theta,a=1) \\rightarrow 50\\), so in this case the loss is minimised when \\(a=0\\).\n\nIt is also possible for a decision rule to ignore the data, or not use any data. One such rule in the context of Example 3.4 may be \\(d({\\mathbf{x}}) =\n0\\), i.e. don’t buy travel insurance no matter what proportion of flights were cancelled in the past.\n\n3.2.1 Deterministic and randomised decision rules\nA decision rule is called deterministic if it specifies a single action to take for given data. The decision rule in Example 3.4 is deterministic because we know which one action the traveller will take when \\(x \\geq 0.10\\) and which one action she will take when \\(x &lt; 0.10\\). However, not all decision rules need to do that. A decision rule may specify multiple actions for given data, with corresponding probabilities for each action. In this case, we call the decision rule randomised. An example of a randomised decision rule for Example 3.4 is\n\\[\n  d(x) =\n  \\begin{cases}\n    \\text{0 with probability $1/5$ and 1 otherwise}  & \\text{ if $x \\geq 0.10$,} \\\\ \\text{0 with probability $3/4$ and 1 otherwise} & \\text{ if $x &lt; 0.10$.}\n\\end{cases}\n\\tag{3.3}\\]\nIn other words, if the observed proportion of cancelled flights, \\(x\\), turns out to be 0.15, i.e., we are in the case \\(x \\geq 0.10\\), then the traveller will pick a random integer between 1 and 5, and if this integer is 1, then she will not buy insurance, but if the integer is 2, 3, 4, or 5, she will.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "ch3.html#sec-risk",
    "href": "ch3.html#sec-risk",
    "title": "3  Decision Theory",
    "section": "3.3 Risk",
    "text": "3.3 Risk\nA decision rule uses the observed data to choose an action. To evaluate different decision rules, we want to evaluate how well they fare if different data were observed. So we consider the hypothetical scenario where new data \\({\\mathbf{x}}\\) are obtained, from the same distribution as our observed data. In this case, we compare them in terms of their expected loss over repeated observations \\({\\mathbf{x}}\\), which we call the risk.\n\n\n\n\n\n\nNoteRisk\n\n\n\n\nDefinition 3.2 The risk of a decision rule \\(d\\) for a parameter value \\(\\theta\\), \\(R(\\theta,d)\\), based on data \\({\\mathbf{x}}\\sim f({\\mathbf{x}}|\\theta)\\) is defined as \\[R(\\theta,d) = \\mathop{\\mathrm{{\\mathsf E}}}_\\theta L(\\theta, d({\\mathbf{x}})),\\] i.e., the expected loss under the action obtained by following the decision rule \\(d\\).\n\n\n\nThe subscript in \\(\\mathop{\\mathrm{{\\mathsf E}}}_\\theta\\) indicates that the expectation is taken with respect to \\({\\mathbf{x}}\\sim f({\\mathbf{x}}|\\theta)\\). If the decision rule \\(d({\\mathbf{x}})\\) does not depend on the data \\({\\mathbf{x}}\\), i.e., it ignores the data \\({\\mathbf{x}}\\), then \\(R(\\theta,d) = L(\\theta,d({\\mathbf{x}}))\\). For example, if the traveller of Example 3.4 is going on a business trip, company policy might dictate that the traveller should buy travel insurance regardless of how likely it is for the flight to be cancelled. In this case, the traveller’s action is \\(d({\\mathbf{x}}) = 1\\) for all \\({\\mathbf{x}}\\in \\mathcal{X}\\).\n\nExample 3.5 (Example 3.4 continued) Suppose there were \\(n=900\\) flights in the past 10 years that were examined by the traveller. According to our model, each flight has a probability \\(\\theta\\) of being cancelled. Assuming that the event that a flight be cancelled is independent of whether previous flights were cancelled, the central limit theorem says that the proportion \\(x\\) of cancelled flights is distributed asymptotically as \\[x \\sim N\\left(\\theta,\\frac{\\theta(1-\\theta)}{n}\\right).\\] Therefore, \\[\n\\begin{aligned}\n    {\\mathds{P}}(x &lt; 0.10) & \\approx \\Phi\\left(\\frac{0.10 -\n        \\theta}{\\sqrt{\\dfrac{\\theta(1-\\theta)}{n}}}\\right) =\n    \\Phi\\left(\\frac{30(0.10 -\n        \\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right)\\\\\n    {\\mathds{P}}(x \\geq 0.10) & \\approx 1-\\Phi\\left(\\frac{30(0.10 -\n        \\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right) =\n    \\Phi\\left(\\frac{30(\\theta - 0.10)}{\\sqrt{\\theta(1-\\theta)}}\\right)\n\\end{aligned}\n\\]\nLet \\(d_1\\) denote the decision rule (4.1). According to this rule, the traveller will choose to buy insurance (\\(a=1\\)) with probability \\(\\Phi\\left(\\frac{30(\\theta - 0.10)}{\\sqrt{\\theta(1-\\theta)}}\\right)\\) and not buy (\\(a=0\\)) with probability \\(\\Phi\\left(\\frac{30(0.10 - \\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right)\\). Therefore, the risk of (4.1) according to the loss function (3.2) is\n\\[\n\\begin{aligned}\n    R(\\theta,d_1) &= L(\\theta,0) \\times {\\mathds{P}}(x &lt; 0.10) + L(\\theta,1) \\times\n    {\\mathds{P}}(x \\geq 0.10) \\nonumber \\\\\n    &= (500\\theta) \\times \\Phi\\left(\\frac{30(0.10 -\n        \\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right) + (50-500\\theta) \\times\n    \\Phi\\left(\\frac{30(\\theta -\n        0.10)}{\\sqrt{\\theta(1-\\theta)}}\\right)\n\\end{aligned}\n\\tag{3.4}\\]\nNow let \\(d_2\\) denote the randomised rule (3.3). According to this rule, the traveller will choose to buy insurance with probability \\(4/5\\) if \\(x \\geq 0.10\\) and with probability \\(1/4\\) if \\(x &lt; 0.10\\). So the risk of this rule is\n\\[\n\\begin{aligned}\n    R(\\theta,d_2) = & L(\\theta,0) \\times \\left\\{\\frac{3}{4}{\\mathds{P}}(x &lt; 0.10) +\n    \\frac{1}{5}{\\mathds{P}}(x \\geq 0.10)\\right\\}  \\\\\n    & + L(\\theta,1) \\times\n    \\left\\{\\frac{1}{4}{\\mathds{P}}(x &lt; 0.10) + \\frac{4}{5}{\\mathds{P}}(x \\geq 0.10)\\right\\}\n     \\\\\n    ={}& (500\\theta) \\times \\left\\{\\frac{3}{4} \\Phi\\left(\\frac{30(0.10 -\n          \\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right) + \\frac{1}{5}\n      \\Phi\\left(\\frac{30(\\theta -\n          0.10)}{\\sqrt{\\theta(1-\\theta)}}\\right) \\right\\} \\\\\n    &{}+ (50-500\\theta) \\times\n    \\left\\{\\frac{1}{4}\\Phi\\left(\\frac{30(0.10 -\n          \\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right)\n      + \\frac{4}{5}\\Phi\\left(\\frac{30(\\theta -\n          0.10)}{\\sqrt{\\theta(1-\\theta)}}\\right)\\right\\}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\nFigure 3.3: Risks for the two decision rules considered in Example 3.5.\n\n\n\nThe risks of the two decision rules for different \\(\\theta\\) values are shown in Figure 3.3. Both risks decrease as it becomes more certain that the flight will be cancelled. It can be seen that for most values of \\(\\theta\\), \\(d_1\\) has lower risk than \\(d_2\\), while \\(d_2\\) is better for values of \\(\\theta\\) between 0.05 and 0.10. In fact, both rules have the most risk when \\(\\theta = 0.085\\), and in this case \\(d_1\\) has the highest risk.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "ch3.html#sec:criteria",
    "href": "ch3.html#sec:criteria",
    "title": "3  Decision Theory",
    "section": "3.4 Criteria for choosing a good decision rule",
    "text": "3.4 Criteria for choosing a good decision rule\nBased on the discussion above, it is clear that we want to choose a decision rule that has low risk among a choice of different decision rules. Let \\(\\mathcal{D}\\) denote the set of decision rules that we are considering. For Example 3.4, there is no reason why we should only consider the decision rule (4.1) with a fixed threshold 0.10. We could consider a family of decision rules of the form \\(\\mathcal{D} = \\{d_t(x) = (\\text{1 if $x \\geq t$, 0 otherwise})\\}\\), \\(t \\in [0,1]\\). The problem then reduces to finding the optimal threshold according to some criterion.\nIt is apparent from Definition 3.2 and Example 3.5 that the risk of a decision rule is a function of the parameter \\(\\theta\\). It is possible that we are not able to find a decision rule that uniformly dominates all other decision rules for all values of \\(\\theta\\). On the other hand, we want to choose a decision rule that is optimal regardless of the true value of \\(\\theta\\). We present below two criteria that can be used for this purpose.\n\n3.4.1 Minimax criterion\nThe idea behind the minimax criterion is to safeguard against the worst possible situation. Consider, for example, an aeroplane manufacturer considering various aeroplane designs. The manufacturer wants to choose the design that is safest under the worst possible weather conditions.\nFor a decision rule \\(d\\), with risk \\(R(\\theta,d)\\), the maximum possible risk, \\(\\bar R (d)\\), is given by \\[\\bar R (d) = \\max_{\\theta \\in \\Theta} R(\\theta,d).\\] The value of \\(\\theta\\) that maximises \\(R(\\theta,d)\\) is the worst possible situation for the decision rule \\(d\\). Thus, we want to choose the decision rule among all those considered in the set \\(\\mathcal{D}\\) that is best under the worst possible conditions, i.e., we want to choose the decision rule with the lowest \\(\\bar R (d)\\). Such decision rule is called minimax, and is given by \\[\nd_\\mathrm{MM} = \\mathop{\\mathrm{argmin}}_{d \\in \\mathcal{D}} \\bar R (d) = \\mathop{\\mathrm{argmin}}_{d \\in \\mathcal{D}} \\max_{\\theta \\in \\Theta} R(\\theta,d).\n\\]\nThe notation \\(\\displaystyle\\mathop{\\mathrm{argmin}}_{d \\in \\mathcal{D}}\\) reads “the argument that minimises over \\(d\\in\\mathcal{D}\\)”, meaning “search over all decision rules \\(d \\in \\mathcal{D}\\) and pick the one that gives the smallest \\(\\bar R (d)\\)”. It is clear by looking at Figure 3.3 that if we have to choose only between \\(d_1\\) and \\(d_2\\) in Example 3.5, then, according to the minimax criterion, we would choose \\(d_2\\), because it has a lower maximum risk than \\(d_1\\). Note that it does not matter whether the maximum risk is attained at the same \\(\\theta\\) value.\n\nExample 3.6 Suppose we consider a family \\(\\mathcal{D}\\) of decision rules of the form \\[\n    d_t(x) =\n    \\begin{cases}\n      1 & \\text{ if $x \\geq t$,} \\\\ 0 & \\text{ if $x &lt; t$,}\n    \\end{cases}\n\\tag{3.5}\\]\nfor \\(t \\in [0,1]\\). In other words, we want to find the optimum threshold \\(t\\) such that the traveller decides to buy insurance if the proportion of cancelled flights, \\(x\\), exceeds that threshold, and not buy otherwise. Then, repeating the calculations from Example 3.5, with an arbitrary threshold \\(t\\), we have, by Equation 3.4,\n\\[\nR(\\theta,d_t) = (500\\theta) \\times \\Phi\\left(\\frac{30(t -\n        \\theta)}{\\sqrt{\\theta(1-\\theta)}}\\right) + (50-500\\theta) \\times\n    \\Phi\\left(\\frac{30(\\theta -\n        t)}{\\sqrt{\\theta(1-\\theta)}}\\right).\n\\tag{3.6}\\]\nThen, \\(\\bar R(d_t) = \\max_\\theta R(\\theta,d_t)\\).\n\n\n\n\n\n\nFigure 3.4: Maximum risk for the varying threshold of the decision rules in Example 3.6\n\n\n\nFinding \\(\\bar R(d_t)\\) is closed-form is not possible, but we can compute it numerically. A plot of \\(\\bar R(d_t)\\) for different choices of the threshold \\(t\\) is shown in Figure 3.4. It can be seen that the minimum is attained at \\(t = 0.05\\). For comparison, the risks of the minimax decision rule (\\(t=0.05\\)), and the original decision rule (\\(t=0.10\\)) are shown in Figure 3.5. It can be seen that both rules have similar risks, however, in the region of \\(\\theta\\) between 0.05 and 0.10 the minimax rule is better.\n\n\n\n\n\n\nFigure 3.5: Risks of the decision rules for thresholds \\(t=0.10\\) and \\(t=0.05\\) (minimax) in Example 3.6.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "ch3.html#sec:exercises",
    "href": "ch3.html#sec:exercises",
    "title": "3  Decision Theory",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nA patient is considering a number of treatment options available through her general practitioner (GP) between receiving medication or having a surgery. The costs of the different treatments vary as well as their likelihood of success. The GP has discussed with the patient the success rates of each treatment when used in other patients.\nDescribe the parameter, data, actions, and loss function for this problem.\nAn investor is considering whether or not to buy certain risky bonds. If he buys the bonds, they can be redeemed at maturity for a net gain of £500. There is probability \\(\\theta\\) that there will be a default on the bonds, in which case the investor is set to lose his investment of £1000. If the investor instead puts his money in a “safe” investment, he will receive a net gain of £300 over the same period.\n\nDefine appropriate actions, parameter, and parameter space for the problem.\nDerive the loss function for the problem.\nDescribe all randomised decision rules and find the minimax decision among them.\n\nA coin has probability \\(\\theta \\in [0,1]\\) of coming up heads \\((y=1)\\), and \\(1-\\theta\\) of coming up tails \\((y=0)\\). You are playing a game where if you guess the outcome of a coin flip correctly you receive a payment of £1, but if you guess wrongly, you loose £1.\n\nWhat are the parameter and parameter space for this problem?\nWhat is the action space for this problem?\nShow that the loss function, \\(L(\\theta,a)\\), for this problem is given by \\[L(\\theta,a) =\n      \\begin{cases}\n        2\\theta -1 & \\text{ if guessing ``tails'',} \\\\ 1-2\\theta  & \\text{ if guessing ``heads''.}\n      \\end{cases}\\]\n\n\nLet \\(x\\) be the outcome the coin flip from an earlier game. Consider the following two strategies for guessing the outcome of a future coin flip:\n\nStrategy 1: Guess the same as the outcome of the earlier coin flip.\nStrategy 2: Guess “heads” regardless of the outcome of the earlier coin flip.\n\n\nWrite a mathematical expression for the decision rules corresponding to these two strategies.\nBetween the two strategies, which one is the minimax decision rule?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "4  Parameter Estimation",
    "section": "",
    "text": "4.1 Point estimation\nIn statistical inference we are interested in making conclusions about the population of interest. Often this means making a statement about an unknown parameter describing the population. In this chapter we discuss methods using data that can infer the value of the unknown parameter.\nSuppose we are given a random sample from a population \\(f(x|\\theta)\\) depending on an unknown parameter \\(\\theta\\) with values in the parameter space \\(\\Theta\\), i.e., \\(\\theta \\in \\Theta\\). We wish to use the sample to infer the value of \\(\\theta\\) within \\(\\Theta\\). Any function of the sample which can be used for this purpose is an estimator for \\(\\theta\\).\nNote that an estimator, being a function of the random sample, is itself a random variable. We can therefore talk about the distribution of this estimator. We can potentially come up with several estimators so using their distribution we can evaluate their performance. Two of the most commonly used criteria for evaluating estimators are the bias and the mean squared error which we define below.\nA desirable property for an estimator is to be unbiased.\nThe MSE is always non-negative. It is desirable that the MSE be small. Figure 4.1 illustrates the bias and variability of four different estimators. The estimator with low bias and variability is in fact the one with the lowest MSE as the following lemma tells us.\nAccording to Proposition 4.1 , the MSE incorporates two components, one measuring the variability of the estimator and the other measuring its bias (accuracy). An estimator with low MSE has low combined variance and bias.\nWe discuss next a few classical estimation methods.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "ch4.html#sec-estim",
    "href": "ch4.html#sec-estim",
    "title": "4  Parameter Estimation",
    "section": "",
    "text": "NoteEstimator\n\n\n\n\nDefinition 4.1 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\) be a random sample from a population which depends on a parameter \\(\\theta \\in \\Theta\\). Any statistic \\(T = T(X_1,\\ldots,X_n)\\) taking values in a subset of \\(\\Theta\\), i.e., \\(T \\in \\Theta\\), is called an estimator for the parameter \\(\\theta\\). Suppose we observe \\(X_1 = x_1,\\ldots, X_n = x_n\\) and evaluate \\(t = T(x_1,\\ldots,x_n)\\). The value \\(t\\) corresponding to the observed values \\(x_1,\\ldots,x_n\\) is called an estimate of \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\nNoteBias\n\n\n\n\nDefinition 4.2 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\) and let \\(T = T(X_1,\\ldots,X_n)\\) be an estimator for \\(\\theta\\). The difference \\[\\mathrm{Bias}_\\theta(T) = \\mathop{\\mathrm{{\\mathsf E}}}T - \\theta ,\\] is called the bias of the estimator \\(T\\) for the parameter \\(\\theta\\). If \\(\\mathrm{Bias}_\\theta(T) = 0\\), then the estimator \\(T\\) is called unbiased for \\(\\theta\\), otherwise it is called biased for \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\nNoteMean squared error (MSE)\n\n\n\n\nDefinition 4.3 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\) and let \\(T = T(X_1,\\ldots,X_n)\\) be an estimator for \\(\\theta\\). The mean squared error (MSE) of the estimator \\(T\\) for the parameter \\(\\theta\\) is defined by \\[\\mathrm{MSE}_\\theta(T) = \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (T - \\theta)^2 \\right\\}.\\]\n\n\n\n\n\n\n\n\n\n\nFigure 4.1: Illustration of the bias and variability of an estimator.\n\n\n\n\n\n\n\n\n\nNoteMean squared error decomposition\n\n\n\n\nProposition 4.1 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\) and let \\(T = T(X_1,\\ldots,X_n)\\) be an estimator for \\(\\theta\\). Then \\[\\mathrm{MSE}_\\theta(T) = \\mathop{\\mathrm{{\\mathsf Var}}}T + (\\mathrm{Bias}_\\theta(T))^2\\]\n\n\n\n\nProof. By the definition of MSE, add and subtract \\(\\mathop{\\mathrm{{\\mathsf E}}}T\\) in the brackets, and note that \\(\\mathop{\\mathrm{{\\mathsf E}}}T\\) and \\(\\theta\\) are not random,\n\\[\\begin{aligned}\n    \\mathrm{MSE}_\\theta(T)\n    &= \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (T - \\theta)^2 \\right\\} \\\\\n    &= \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (T - \\mathop{\\mathrm{{\\mathsf E}}}T + \\mathop{\\mathrm{{\\mathsf E}}}T - \\theta)^2 \\right\\} \\\\\n    &= \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (T - \\mathop{\\mathrm{{\\mathsf E}}}T)^2 + 2 (T - \\mathop{\\mathrm{{\\mathsf E}}}T) (\\mathop{\\mathrm{{\\mathsf E}}}T - \\theta) + (\\mathop{\\mathrm{{\\mathsf E}}}T -\n      \\theta)^2 \\right\\} \\\\\n    &= \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (T - \\mathop{\\mathrm{{\\mathsf E}}}T)^2 \\right\\} + 2 \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (T - \\mathop{\\mathrm{{\\mathsf E}}}T) (\\mathop{\\mathrm{{\\mathsf E}}}T -\n      \\theta) \\right\\} + \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (\\mathop{\\mathrm{{\\mathsf E}}}T - \\theta)^2 \\right\\} \\\\\n    &= \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (T - \\mathop{\\mathrm{{\\mathsf E}}}T)^2 \\right\\} + 2 (\\mathop{\\mathrm{{\\mathsf E}}}T -\n      \\theta) \\mathop{\\mathrm{{\\mathsf E}}}\\left\\{ (T - \\mathop{\\mathrm{{\\mathsf E}}}T) \\right\\} + (\\mathop{\\mathrm{{\\mathsf E}}}T - \\theta)^2 \\\\\n    &= \\mathop{\\mathrm{{\\mathsf Var}}}T + 0 + (\\mathrm{Bias}_\\theta(T))^2 .  \n\\end{aligned}\n\\]\n\n\n\nExample 4.1 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta) {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{N}(\\mu,\\sigma^2)\\). The parameter space for \\(\\mu\\) is \\(\\mathbb{R}\\) and for \\(\\sigma^2\\) is \\([0,\\infty)\\). Then \\(\\bar X\\) is an estimator for \\(\\mu\\) because \\(\\bar X \\in \\mathbb{R}\\). Its bias is \\(\\mathrm{Bias}_\\mu(\\bar X) = \\mathop{\\mathrm{{\\mathsf E}}}\\bar X - \\mu = \\mu - \\mu = 0\\) and its variance is \\(\\mathop{\\mathrm{{\\mathsf Var}}}\\bar X = \\sigma^2/n\\). Therefore, its MSE is \\(\\mathrm{MSE}_\\mu(\\bar X) = \\mathop{\\mathrm{{\\mathsf Var}}}\\bar X + \\mathrm{Bias}_\\mu(\\bar X)^2 =\n  \\sigma^2/n\\).\n\n\nExample 4.2 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{Bernoulli}(p)\\). The parameter space for \\(p\\) is \\([0,1]\\). Then \\(\\bar X\\) is an estimator for \\(p\\) because \\(\\bar X \\in \\{0, 1/n, 2/n,\n  \\ldots, 1\\} \\subset [0,1]\\). Its bias is \\(\\mathrm{Bias}_p(\\bar X) = \\mathop{\\mathrm{{\\mathsf E}}}\\bar X - p = p - p = 0\\) and its variance is \\(\\mathop{\\mathrm{{\\mathsf Var}}}\\bar X = p(1-p)/n\\). Therefore, its MSE is \\(\\mathrm{MSE}_p(\\bar X) = p(1-p)/n\\). Because \\(p(1-p) \\in [0,\\frac 14]\\), with the lower bound attained when \\(p=0\\) or 1 and the upper bound attained when \\(p = {\\frac 12}\\), \\(\\mathrm{MSE}_p(\\bar X) \\in [0, \\frac 1{4n}]\\).\nConsider a different estimator given by \\(T = \\dfrac{2\\sum X_i + \\sqrt{n}}{2n + 2\\sqrt{n}}\\). Because \\(\\sum X_i \\in \\{0,1,\\ldots,n\\}\\), \\(T \\in [0,1]\\) so it is an estimator for \\(p\\). Its bias is \\(\\mathrm{Bias}_p(T) = \\dfrac{2np + \\sqrt{n}}{2n + 2\\sqrt{n}} - p = (1-2p)\n  \\dfrac{\\sqrt{n}}{2n + 2\\sqrt{n}} = \\dfrac{{\\frac 12}- p}{\\sqrt{n}+1}\\). So this estimator has no bias if \\(p={\\frac 12}\\) but has positive bias (overestimates \\(p\\)) if \\(p &lt; {\\frac 12}\\) and negative bias (underestimates \\(p\\)) if \\(p&gt;{\\frac 12}\\). The variance of this estimator is \\(\\mathop{\\mathrm{{\\mathsf Var}}}T =\n  \\dfrac{np(1-p)}{(n + \\sqrt{n})^2} = \\dfrac{p(1-p)}{(\\sqrt{n}+1)^2}\\) so \\(\\mathrm{MSE}_p(T) = \\dfrac{p(1-p) + ({\\frac 12}- p)^2}{(\\sqrt{n}+1)^2} =\n  \\dfrac{\\frac 14}{(\\sqrt{n}+1)^2}\\).\nIf we wish to choose between \\(\\bar X\\) and \\(T\\) in terms of their MSE, we see that for all \\(n \\geq 1\\), \\(0 &lt; \\mathrm{MSE}_p(T) &lt; \\frac 1{4n}\\) so it falls between the values of \\(\\mathrm{MSE}_p(\\bar X)\\). In particular, if in reality \\(p={\\frac 12}\\), then \\(T\\) will always have lower MSE than \\(\\bar X\\). For some other value of \\(p\\), say \\(p = \\frac 15\\) then \\(T\\) has lower MSE if \\(n \\leq 16\\) but otherwise \\(\\bar X\\) has lower MSE (see Figure 4.2).\n\n\n\n\n\n\n\nFigure 4.2: Comparison of \\(MSE_p(\n\\bar{X})\\) and \\(MSE_p(T)\\) for Example 4.2 for diﬀerent values of the parameter p.\n\n\n\n\n\n4.1.1 Method of moments estimator\nThe method of moments estimation is the simplest method for finding estimators. Consider a population \\(f(x|\\theta)\\), \\(\\theta \\in \\Theta\\) and define the \\(r\\)th moment by \\[\\mu_r = \\mathop{\\mathrm{{\\mathsf E}}}(X^r),\\ \\text{for $r=1,2,\\ldots$},\\] i.e., the expectation of the \\(r\\)th power of \\(X\\). In the case \\(r=1\\), \\(\\mu_1\n= \\mathop{\\mathrm{{\\mathsf E}}}X\\) corresponds to the mean of the population, while for \\(r=2\\), \\(\\mu_2\n= \\mathop{\\mathrm{{\\mathsf E}}}X^2\\), so \\(\\mathop{\\mathrm{{\\mathsf Var}}}X = \\mathop{\\mathrm{{\\mathsf E}}}X^2 - (\\mathop{\\mathrm{{\\mathsf E}}}X)^2 = \\mu_2 - \\mu_1^2\\).\nA convenient method for computing the moments is through the moment generating function (mgf). Recall \\(M_X(t) = \\mathop{\\mathrm{{\\mathsf E}}}\\exp(tX)\\) and \\[\\mu_r = \\dfrac{d^r}{dt^r} M_X(t) |_{t=0} .\\]\n\nExample 4.3 Let \\(X \\sim \\mathrm{Exponential}(\\mu)\\), i.e., \\(f(x|\\mu) = (1/\\mu)\n\\exp(-x/\\mu)\\). Then \\[\n\\begin{aligned}\n  M_X(t)\n  &= \\int_0^\\infty e^{tx} \\frac{1}{\\mu} e^{-\\frac{x}{\\mu}} {\\,\\mathrm{d}}x \\\\\n  &= \\frac{1}{\\mu} \\int_0^\\infty e^{-x(\\frac 1\\mu - t)} {\\,\\mathrm{d}}x \\\\\n  &= \\frac{1}{\\mu} (\\frac 1\\mu - t)^{-1} \\left[ -e^{-x(\\frac 1\\mu - t)}\n    \\right]_0^\\infty \\\\\n  &= (1 - t\\mu)^{-1} ,\\ \\text{assuming $t &lt; 1/\\mu$}.\\\\\n  \\Rightarrow M_X^{(1)}(t)\n  &= \\frac{d}{dt} M_X(t) = \\mu (1 - t\\mu)^{-2} \\\\\n  \\Rightarrow \\mu_1\n  &= M_X^{(1)}(0) = \\mu \\\\\n  \\Rightarrow M_X^{(2)}(t)\n  &= \\frac{d^2}{dt^2} M_X(t) = 2\\mu^2 (1 - t\\mu)^{-3} \\\\\n  \\Rightarrow \\mu_2\n  &= M_X^{(2)}(0) = 2\\mu^2\n\\end{aligned}\n\\]\n\nIt is apparent that the \\(r\\)th moment is a function of the parameter \\(\\theta\\) which we write as \\(\\mu_r(\\theta)\\) to make this dependence explicit. Note that \\(\\theta\\) may be a scalar or a \\(\\kappa\\)-dimensional vector \\(\\theta = (\\theta_1,\\ldots,\\theta_\\kappa)\\).\nNow suppose \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\) and consider the \\(r\\)th sample moment \\[m_r = \\frac{1}{n} \\sum_{i=1}^n X_i^r,\\ \\text{for $r=1,2,\\ldots$}.\\] In particular \\(m_1 = \\bar X\\) and \\(m_2 = \\frac 1n \\sum X_i^2\\). The sample moments are functions of the sample \\(\\mathbf{X} = \\{ X_1,\\ldots,X_n \\}\\) and we write \\(m_r(\\mathbf{X})\\) to make this dependence explicit.\n\n\n\n\n\n\nNoteMethod of moments estimator (MoM)\n\n\n\n\nDefinition 4.4 The method of moments estimates the \\(r\\)th moment by the corresponding sample moment, i.e., the method of moments estimator (MoM) for \\(\\theta\\), which we denote by \\(\\hat{\\theta}\\), is given by the solution of the following system of equations, \\[\\mu_r(\\hat{\\theta}) = m_r(\\mathbf{X}),\\ \\text{for $r=1,2,\\ldots$}.\\] Because there are \\(\\kappa\\) unknown parameters, we need \\(\\kappa\\) equations to be able to identify \\(\\hat{\\theta}\\) uniquely. These are selected among those equations corresponding to the lowest moments up to as many as needed to be able to solve for \\(\\hat{\\theta}\\).\n\n\n\n\nExample 4.4 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{Exponential}(\\mu)\\), \\(\\mu &gt; 0\\). Then \\(\\mu_1 = \\mu\\) so \\(\\hat{\\mu} = \\bar X\\) is the method of moments estimator.\n\n\nExample 4.5 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{N}(0,\\sigma^2)\\), i.e., normal with known mean 0 and unknown variance \\(\\sigma^2&gt;0\\). Then \\(\\mu_1 = 0\\) and \\(\\mu_2 = \\sigma^2\\). Note that the first moment does not depend on the parameter so the first equation, \\(\\mu_1 = \\bar X\\), is not helpful for estimating \\(\\sigma^2\\). Using the second equation we have \\(\\hat{\\sigma}^2 = m_2\\).\n\n\nExample 4.6 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{U}(0,\\theta)\\), i.e., uniform with known lower bound 0 and unknown upper bound \\(\\theta&gt;0\\). The pdf is \\(f(x|\\theta) = \\theta^{-1}\\), \\(x\\in(0,\\theta)\\), so \\(\\mu_1 = \\int_0^\\theta x \\theta^{-1} {\\,\\mathrm{d}}x = \\theta^{-1} \\left[\n    \\frac{x^2}{2} \\right]_0^\\theta = \\theta/2\\). Using the first moment equation we have \\(\\hat{\\theta}/2 = \\bar X \\Rightarrow \\hat{\\theta} =\n  2\\bar X\\).\n\n\nExample 4.7 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{Gamma}(\\alpha,\\beta)\\), i.e., gamma with shape \\(\\alpha&gt;0\\) and rate \\(\\beta&gt;0\\) (this distribution was defined in Example 1.34). In this case there are two parameters to estimate, i.e., \\(\\kappa=2\\). The mgf of this distribution is given by \\(M_X(t) = (1-\\frac{t}{\\beta})^{-\\alpha}\\). Then \\(\\mu_1 =\n  \\alpha/\\beta\\) and \\(\\mu_2 = \\alpha/\\beta^2 +\\alpha^2/\\beta^2\\). This leads to the following system of equations \\[\\hat{\\alpha}/\\hat{\\beta} = m_1, \\qquad \\hat{\\alpha}/\\hat{\\beta}^2 +\n    \\hat{\\alpha}^2/\\hat{\\beta}^2 = m_2.\\] By substituting \\(\\hat{\\alpha} = \\hat{\\beta}m_1\\) from the first equation into the second, we have \\(m_1/\\hat{\\beta} + m_1^2 = m_2 \\Rightarrow\n\\hat{\\beta} = m_1/(m_2 - m_1^2)\\) and \\(\\hat{\\alpha} = m_1^2/(m_2 - m_1^2)\\).\nAn obvious question to ask is are these actual estimators? In other words, are \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}&gt;0\\)? To check this we need to check whether \\(m_2 &gt; m_1^2\\) for all possible samples. But \\(0 &lt; \\sum (X_i - \\bar X)^2 = \\sum (X_i^2 - 2X_i\\bar X + \\bar X^2) = \\sum\nX_i^2 - 2 \\bar X \\sum X_i + n\\bar X^2 = \\sum X_i^2 - n\\bar X^2\\). So \\(\\sum X_i^2 &gt; n\\bar X^2 \\Rightarrow \\frac{1}{n} \\sum X_i^2 &gt; n\\bar X^2\n\\Rightarrow m_2 &gt; m_1^2\\) as required.\n\n\n\n4.1.2 Maximum likelihood estimator\nLet \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\), \\(\\theta \\in \\Theta\\). Then, the joint density/mass function of \\({\\mathbf{X}}= (X_1,\\ldots,X_n)\\) is given by \\[\n  f({\\mathbf{x}}|\\theta) = \\prod_{i=1}^n f(x_i|\\theta).\n\\tag{4.1}\\]\n\nIn (4.1), we see the parameter \\(\\theta\\) as fixed and evaluate the function at a given \\({\\mathbf{x}}\\). If instead(4.1) is viewed as a function of \\(\\theta\\) for a given sample \\({\\mathbf{x}}\\), then it is called a likelihood function and is denoted by \\(L(\\theta|{\\mathbf{x}})\\). We have the following definition.\n\n\n\n\n\n\nNoteLikelihood function\n\n\n\n\nDefinition 4.5 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\), \\(\\theta \\in \\Theta\\). Suppose we observe data \\({\\mathbf{x}}= (x_1,\\ldots,x_n)\\). Then, the likelihood function for \\(\\theta\\) is \\[L(\\theta|{\\mathbf{x}}) = \\prod_{i=1}^n f(x_i|\\theta).\\]\n\n\n\nIntuitively, the likelihood function tells us how likely the observed data are for that value of \\(\\theta\\). Therefore it makes sense to estimate \\(\\theta\\) by that value which makes the observed data appear more likely. Therefore we define the maximum likelihood estimator for the parameter \\(\\theta\\), the value \\(\\hat{\\theta}\\) for which \\(L(\\theta|{\\mathbf{x}})\\) is maximised, i.e., \\[\\hat{\\theta} = \\mathop{\\mathrm{argmax}}_{\\theta \\in \\Theta} L(\\theta|{\\mathbf{x}}) .\\]\nIn practice, it is usually easier to maximise the logarithm of the likelihood function instead of the likelihood function itself. We define the log-likelihood function, \\(\\ell(\\theta|{\\mathbf{x}}) = \\log L(\\theta|{\\mathbf{x}})\\). In this case the maximum likelihood estimator (MLE) becomes \\[\\hat{\\theta} = \\mathop{\\mathrm{argmax}}_{\\theta \\in \\Theta} \\ell(\\theta|{\\mathbf{x}}) .\\]\nNote that \\(\\theta\\) could be a vector, i.e., \\(\\theta =\n(\\theta_1,\\ldots,\\theta_\\kappa)\\). In some cases (but not always) the MLE can be obtained by solving a system of equations \\[\\frac{\\partial}{\\partial \\theta_r} \\ell(\\theta|{\\mathbf{x}}) = 0,\\ r = 1,\\ldots,\n  \\kappa.\\]\n\n\n\n\n\n\nNote\n\n\n\nIt is custom when writing the log-likelihood function to omit additive constants which do not depend on the parameters. This makes the expression for the log-likelihood brief and does not affect the MLE. It is important however to remain consistent throughout our calculations to avoid errors.\n\n\n\nExample 4.8 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{Exponential}(\\mu)\\), \\(\\mu &gt; 0\\). Then \\(f(x|\\mu) = (1/\\mu) \\exp(-x/\\mu)\\) so\n\\(L(\\mu|{\\mathbf{x}}) = \\prod \\{ (1/\\mu)\n  \\exp(-x_i/\\mu) \\} = (1/\\mu^n) \\exp(-\\sum x_i /\n  \\mu)\\) and\n\\(\\ell(\\mu|{\\mathbf{x}}) = -n\\log \\mu -\\sum x_i / \\mu\\).\nIn this case we can find the MLE by solving \\(\\frac{d\\ell}{d\\mu}=0\\):\n\\(\\frac{d\\ell}{d\\mu}= -n/\\hat\\mu + \\sum x_i / \\hat\\mu^2 = 0 \\Rightarrow -n\n  + \\sum x_i/\\hat{\\mu} = 0 \\Rightarrow \\hat{\\mu} = \\sum x_i/n = \\bar x\\). Note that this is identical to the MoM estimator in Example 4.4.\n\n\nExample 4.9 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{N}(0,\\sigma^2)\\), i.e., normal with known mean 0 and unknown variance \\(\\sigma^2&gt;0\\). Then\n\\(L(\\sigma^2|{\\mathbf{x}}) = \\prod (2\\pi\\sigma^2)^{-{\\frac 12}}\n  \\exp (-\\frac{x_i^2}{2\\sigma^2}) = (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp\n  (-\\frac{1}{2\\sigma^2} \\sum x_i^2)\\), so\n\\(\\ell(\\sigma^2|{\\mathbf{x}}) = -\\frac{n}{2} \\log(2\\pi\\sigma^2)\n  -\\frac{1}{2\\sigma^2} \\sum x_i^2\\).\nAgain we solve for \\(\\frac{d\\ell}{d\\sigma^2}=0\\):\n\\(\\frac{d\\ell}{d\\sigma^2}= -\\frac{n}{2} \\frac{1}{\\hat{\\sigma}^2} +\n  \\frac{1}{2(\\hat{\\sigma}^2)^2} \\sum x_i^2 = 0 \\Rightarrow -n+\n  \\frac{1}{\\hat{\\sigma}^2} \\sum x_i^2 = 0 \\Rightarrow \\hat{\\sigma}^2 = \\sum\n  x_i^2/n\\). Note that this is identical to the MoM estimator in Example 4.5.\n\n\nExample 4.10 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{U}(0,\\theta)\\), i.e., uniform with known lower bound 0 and unknown upper bound \\(\\theta&gt;0\\). The pdf is \\(f(x|\\theta) = \\theta^{-1}\\) for \\(x\\in(0,\\theta)\\), i.e., \\[\\begin{aligned}\n    f(x|\\theta) &= \\begin{cases} \\theta^{-1} & 0&lt;x&lt;\\theta, \\\\ 0 &\n    \\text{otherwise}. \\end{cases}\n\\end{aligned}\n\\] Then, \\[\\begin{aligned}\n    L(\\theta|{\\mathbf{x}}) &= \\displaystyle\\prod_{i=1}^n f(x_i|\\theta)\\\\\n    &= \\begin{cases} \\theta^{-n} & 0&lt;x_1,\\ldots,x_n &lt;\\theta \\\\ 0 &\n      \\text{otherwise} \\end{cases} \\\\ &= \\begin{cases} \\theta^{-n} & 0&lt;x_{(n)}\n      &lt;\\theta \\\\ 0 & \\text{otherwise} \\end{cases} \\\\\n    &=    \\begin{cases}\n      0 & \\text{if $\\theta &lt; x_{(n)}$}, \\\\\n      \\theta^{-n}  & \\text{if $\\theta \\geq x_{(n)}$} ,\n    \\end{cases}  \n\\end{aligned}\n\\]\nwhere \\(x_{(n)} = \\max\\{x_1,\\ldots,x_n\\}\\). In other words, the likelihood is 0 if at least one of the \\(x_i\\)’s falls outside the interval \\((0,\\theta)\\). If all of the \\(x_i\\)’s fall within \\((0,\\theta)\\), then the likelihood is \\(\\theta^{-n}\\). The statement “all of the \\(x_i\\)’s fall within \\((0,\\theta)\\)” is equivalent to “the largest of the \\(x_i\\)’s falls within \\((0,\\theta)\\)”. Because \\(\\theta^{-n}\\) is a decreasing function in \\(\\theta\\), the likelihood is then maximised when \\(\\hat{\\theta} = x_{(n)}\\). This can be verified by the plot in Figure 4.3\n\n\n\n\n\n\nFigure 4.3: Demonstration of the MLE for Example 4.10\n\n\n\n\n\nExample 4.11 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{Gamma}(\\alpha,\\beta)\\), i.e., gamma with shape \\(\\alpha&gt;0\\) and rate \\(\\beta&gt;0\\). In this case there are two parameters to estimate, i.e., \\(\\kappa=2\\). The pdf is given by \\[f(x|\\alpha,\\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1}\n    e^{-\\beta x}.\\] Then, \\[L(\\alpha,\\beta|{\\mathbf{x}}) = \\frac{\\beta^{n\\alpha}}{\\Gamma(\\alpha)^n} \\left(\\prod\n    x_i \\right)^{\\alpha-1} e^{-\\beta \\sum x_i},\\] so \\[\\ell(\\alpha,\\beta|{\\mathbf{x}}) = n\\alpha \\log \\beta -n \\log\\Gamma(\\alpha) +\n    (\\alpha-1) \\log\\left(\\prod x_i \\right) -\\beta \\sum x_i.\\] In this case we have a system of 2 equations: \\(\\frac{d\\ell}{d\\alpha}=0\\) and \\(\\frac{d\\ell}{d\\beta}=0\\):\n\\(\\frac{d\\ell}{d\\beta} = \\frac{n\\alpha}{\\beta} - \\sum x_i = 0\\), and\n\\(\\frac{d\\ell}{d\\alpha} = n\\log\\beta - n \\psi(\\alpha) + \\log\\left(\\prod x_i\n\\right) = 0\\), where \\(\\psi(\\alpha)\\) denotes the digamma function, \\(\\psi(\\alpha) = \\frac{d}{d\\alpha} \\log \\Gamma(\\alpha)\\).\nFrom the first equation we have \\(\\beta = \\alpha/ \\bar x\\) so if \\(\\alpha\\) were known we could estimate \\(\\beta\\) in this way. If \\(\\alpha\\) were unknown, we could substitute the expression for \\(\\beta\\) into the second equation to get an equation in terms of \\(\\alpha\\) only. This becomes\n\\(\\log\\alpha - \\log \\bar x - \\psi(\\alpha) + \\sum \\log x_i/n\n= 0\\) which does not have a closed form solution. In this case the MLE for \\(\\alpha\\) can be obtained numerically. Once the solution is computed, say \\(\\hat{\\alpha}\\), then it is plugged in the expression for \\(\\beta\\) to get \\(\\hat{\\beta} = \\hat{\\alpha}/\\bar x\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "ch4.html#sec-conn-with-decis",
    "href": "ch4.html#sec-conn-with-decis",
    "title": "4  Parameter Estimation",
    "section": "4.2 Connection with decision theory",
    "text": "4.2 Connection with decision theory\nParameter estimation can be put into a decision-theory framework, where the decision problem becomes estimating the unknown parameter. In this case, the action space \\(\\mathcal{A} = \\Theta\\), i.e., the available actions are the possible values of the parameter and the action we take is the estimate of the parameter. Estimators, \\(T(\\boldsymbol{x})\\), map the data to an estimate, so an estimator is a type of decision rule.\nConsider the squared-error loss, \\(L(\\theta,a) = (\\theta - a)^2\\). Under this loss, the further \\(a\\) is from \\(\\theta\\), the higher the loss. The risk associated with the estimator \\(T\\) under the squared-error loss is \\(R(\\theta,T) = E[L(\\theta,T(\\boldsymbol{x}))] = E[(\\theta - T(\\boldsymbol{x}))^2] =\n\\mathrm{MSE}_\\theta(T)\\). This result provides an alternative interpretation of the mean squared error, as the risk of an estimator for \\(\\theta\\) under squared-error loss.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "ch4.html#sec-exercises",
    "href": "ch4.html#sec-exercises",
    "title": "4  Parameter Estimation",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\nP 1. Consider the method of moments estimator of Example 4.6. Identify potential drawbacks of this estimator.\n\nVerify the formula from the mgf of the gamma distribution in Example 4.7 and use it to derive its mean and variance.\nExplain why the MLE of Example 4.10 is biased but do not derive its bias.\nLet \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{Bernoulli}(\\theta)\\), \\(\\theta \\in (0,1)\\). Derive the MLE for \\(\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5  Confidence Intervals and Hypothesis Testing",
    "section": "",
    "text": "5.1 Confidence intervals\nConsider a random sample \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\). An estimator \\(T=T(X_1,\\ldots,X_n)\\) of the parameter \\(\\theta\\), whatever its properties, will provide only a point estimate, \\(\\hat{\\theta}\\) which is likely to differ from the true value of \\(\\theta\\). The point estimator does not provide any information about the deviation of our estimator from the true parameter value. Ideally we would like to provide a range of values which we believe to contain the true parameter value with some known probability. This range of values is called a confidence interval and the probability that the interval contains the parameter is called the confidence level.\nTypical choices for the confidence level are 90%, 95%, or 99%, which correspond to \\(\\alpha\\) being 10%, 5%, and 1% respectively.\nA useful quantity for deriving confidence intervals is defined next.\nA general procedure for constructing a confidence interval for a given confidence level \\(1-\\alpha\\), which is applicable in many problems can be summarised in the following steps.\nSuppose now that \\(\\sigma^2\\) is unknown. Recall that \\[\nS^2=\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar X)^2\n\\] can be used as its estimator and \\[\\frac{(n-1) S^2}{\\sigma^2} \\sim \\mathcal{X}^2_{n-1}\\,.\n\\] (see Example 2.4). We consider the following statistic \\[Y = \\frac{\\bar X-\\mu}{S/\\sqrt{n}},\\] i.e., the same as before but with \\(\\sigma\\) replaced by \\(S = \\sqrt{S^2}\\). To derive the distribution of \\(Y\\), we use the following definition.\nIn Python this distribution is given by scipy.stats.t.\nWe can apply this definition in our problem. We know that \\(Z = \\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathrm{N}(0,1)\\) and that \\(W = (n-1) S^2/\\sigma^2 \\sim \\mathcal{X}^2_{n-1}\\) and that they are independent, so\n\\[\n\\begin{aligned}\n    Y\n    &= \\frac{Z}{\\sqrt{W/(n-1)}} \\\\\n    &= \\frac{\\frac{\\bar X - \\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{S^2/\\sigma^2}}\\,, \\quad \\mbox{(note $\\sigma$ cancels out)} \\\\\n    &= \\frac{\\bar X - \\mu}{S/\\sqrt{n}} \\sim \\mathrm{t}_{n-1}.\n\\end{aligned}\n\\]\nProceeding similarly with the known-variance case, we let \\[c_1 =t_{n-1;\\alpha/2} = -t_{n-1;1-\\alpha/2}\n\\] and \\[c_2 = t_{n-1;1-\\alpha/2}\\,\\] i.e., the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of \\(\\textrm{t}_{n-1}\\), then \\[\n\\bar X - t_{n-1;1-\\alpha/2} \\times \\frac{S}{\\sqrt{n}}\\leq \\mu \\leq \\bar X + t_{n-1;1-\\alpha/2} \\times \\frac{S}{\\sqrt{n}}\n\\] and therefore \\[\n\\left[ \\bar X - t_{n-1;1-\\alpha/2} \\times \\frac{S}{\\sqrt{n}},\\ \\bar X + t_{n-1;1-\\alpha/2} \\times \\frac{S}{\\sqrt{n}} \\right]\n\\] is a level \\(1-\\alpha\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown. Compare with the confidence for \\(\\mu\\) when \\(\\sigma\\) is known in Equation 5.1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confidence Intervals and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "ch5.html#sec-confint",
    "href": "ch5.html#sec-confint",
    "title": "5  Confidence Intervals and Hypothesis Testing",
    "section": "",
    "text": "NoteConfidence interval\n\n\n\n\nDefinition 5.1 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}f(x|\\theta)\\), \\(\\theta \\in \\Theta\\). The random interval \\([L,U]\\) with bounds the statistics \\(L=L(X_1,\\ldots,X_n)\\) and \\(U=U(X_1,\\ldots,X_n)\\) such that \\(L \\leq U\\) and \\(L,U \\in \\Theta\\) is called a confidence interval for the parameter \\(\\theta\\). If \\[\n{\\mathbb{P}}(L \\leq \\theta \\leq U) = 1-\\alpha,\n\\] for all \\(\\theta \\in \\Theta\\), the number \\(1-\\alpha\\), \\(\\alpha \\in (0,1)\\) is called the confidence level of the interval.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough in Definition 5.1 we define the confidence interval as a closed interval \\([L,U]\\), it will sometimes be more natural to quote the open interval \\((L,U)\\) when the random variables \\(L\\) and \\(U\\) are continuous and \\(L&lt;U\\).\n\n\n\n\n\n\n\n\n\nNotePivot quantity\n\n\n\n\nDefinition 5.2 Let \\({\\mathbf{X}}= \\{X_1,\\ldots,X_n\\}\\) be a random sample for a population depending on an unknown parameter \\(\\theta\\), and \\(T = T(X_1,\\ldots,X_n)\\) is some function of the sample. The random variable \\(Y = g(T,\\theta)\\), which is a function of \\(T\\) and \\(\\theta\\), is called a pivot quantity if its distribution does not depend on \\(\\theta\\).\n\n\n\n\n\nDerive a point estimator \\(T=T(X_1,\\ldots,X_n)\\) of the parameter \\(\\theta\\) and come up with a pivot quantity\n\\[\n      Y = g(T,\\theta)\n\\] of \\(T\\) and \\(\\theta\\) whose distribution does not depend of \\(\\theta\\).\nUsing the distribution of \\(Y\\), derive two quantiles, \\(c_1\\) and \\(c_2\\) with \\(c_1 \\leq c_2\\) such that \\[\n      {\\mathbb{P}}(c_1 \\leq Y \\leq c_2) = 1-\\alpha ,    \n\\] i.e., the probability that \\(Y\\) falls within \\(c_1\\) and \\(c_2\\) is \\(1-\\alpha\\), or equivalently, the probability that \\(Y\\) falls outside \\(c_1\\) and \\(c_2\\) is \\(\\alpha\\), i.e., \\[\n      {\\mathbb{P}}(Y &lt; c_1) + {\\mathbb{P}}(Y &gt; c_2) = \\alpha.\n\\] Note that the choice of \\(c_1\\) and \\(c_2\\) is not unique. We usually choose them so that \\[\n      {\\mathbb{P}}(Y&lt;c_1) = {\\mathbb{P}}(Y&gt;c_2) = \\alpha/2.   \n\\]\nRearrange the inequality \\(c_1 \\leq g(T,\\theta) \\leq c_2\\) so that it has the form \\(L \\leq \\theta \\leq U\\), where \\(L=L(X_1,\\ldots,X_n)\\) and \\(U=U(X_1,\\ldots,X_n)\\) do not depend on \\(\\theta\\) but do depend on \\(c_1\\) and \\(c_2\\), and \\(\\theta\\) is only in the middle. Then, \\[\n      {\\mathbb{P}}(L \\leq \\theta \\leq U) = {\\mathbb{P}}(c_1 \\leq Y \\leq c_2) = 1-\\alpha,        \n\\] so \\([L,U]\\) is a confidence interval for \\(\\theta\\) with significance level \\(1-\\alpha\\).\n\n\nExample 5.2 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{Exponential}(\\mu)\\), \\(\\mu&gt;0\\). As shown in Example 4.8, the MLE for \\(\\mu\\) is \\(\\bar X\\). To get a confidence interval, note that \\(\\bar X = \\sum X_i/n\\) and the distribution of \\(W = n\\bar X\\) is \\(\\mathrm{Gamma}(n,1/\\mu)\\), i.e., with shape \\(n\\) and scale \\(\\mu\\), so \\(Y = n\\bar X/\\mu \\sim \\mathrm{Gamma}(n,1)\\). Here the parametrisation of the Gamma is the same as in Example 4.7, that is Gamma(\\(\\alpha,\\beta\\)) where \\(\\alpha\\) is the shape parameter and \\(\\beta\\) is the rate parameter.\nFor a given significance level \\(1-\\alpha\\), let \\(c_1\\) and \\(c_2\\) be the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of \\(\\mathrm{Gamma}(n,1)\\) respectively which can be obtained in Python using\nscipy.stats.gamma.ppf([alpha/2,1-alpha/2],a=n,scale=1).\n\n\n\n\n\n\nFigure 5.1: Illustration of the choice for \\(c_1\\) and \\(c_2\\) for Example 5.2\n\n\n\nThen, \\(c_1 \\leq \\dfrac {n\\bar X} \\mu \\leq c_2 \\Rightarrow \\dfrac{1}{c_2} \\leq\n  \\dfrac \\mu {n\\bar X} \\leq\n  \\dfrac{1}{c_1} \\Rightarrow \\dfrac{n\\bar X}{c_2} \\leq \\mu \\leq\n  \\dfrac{n\\bar X}{c_1} \\Rightarrow \\left[ \\dfrac{n\\bar X}{c_2} ,\\\n    \\dfrac{n\\bar X}{c_1}\\right]\\) is the confidence interval for \\(\\mu\\).\nSuppose that we observe the following sample\n\\[\n0.02, 0.11, 0.11, 0.26, 0.28, 0.44, 0.81, 0.93.\n\\]\nThen, \\(n=8\\), and \\(\\bar x = 0.37\\). For a 95% confidence interval, using Python, we find:\n\nimport scipy.stats\nscipy.stats.gamma.ppf([0.025,0.975],a=8,scale=1)\n\narray([ 3.45383218, 14.42267536])\n\n\nso \\(c_1 = 3.45\\) and \\(c_2 = 14.42\\). We then calculate \\[\\begin{aligned}\n    L &= \\frac{n\\bar x}{c_2} = \\frac{8 (0.37)}{14.42} = 0.21 \\\\\n    U &= \\frac{n\\bar x}{c_1} = \\frac{8 (0.37)}{3.45} = 0.86\n\\end{aligned}\n\\] so the 95% confidence interval is \\([0.21,0.86]\\).\n\n\nExample 5.3 Let \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{N}(\\mu,1)\\), i.e., the normal distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2=1\\). We wish to derive a confidence interval for \\(\\mu\\).\nAn estimator for \\(\\mu\\) is \\(\\bar X\\). The distribution of \\(\\bar X\\) is \\(\\bar X \\sim \\mathrm{N}(\\mu, \\sigma^2/n)\\) so \\[\nY = \\frac{\\bar X - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mathrm{N}(0,1),  \n\\] which is a pivot quantity. Let \\(z_p\\) denote the argument in the CDF of the \\(\\mathrm{N}(0,1)\\) distribution, \\(\\Phi(z)\\) such that \\(\\Phi(z_p) = p\\) (see Figure 5.2). This can by obtained using scipy.stats.norm.ppf in Python, or using the standard normal distribution table. Note that, because of the symmetry of the standard normal distribution around 0, \\(z_p = -z_{1-p}\\).\n\n\n\n\n\n\nFigure 5.2: Illustration of the standard normal quantile \\(z_p\\) corresponding to left-tail probability \\(p\\). The plotted curve corresponds to the N(0,1) pdf and for given p, \\(z_p\\) satisfies \\(p= \\Phi(z_p)\\) where \\(\\Phi(z)\\) is the CDF of N(0,1).\n\n\n\nIf we let \\(c_1 = z_{\\alpha/2} = -z_{1-\\alpha/2}\\), \\(c_2 = z_{1-\\alpha/2}\\), then, with probability \\(1-\\alpha\\), \\[\n\\begin{aligned}\n    &-z_{1-\\alpha/2} \\leq \\frac{\\bar X - \\mu }{\\sigma/\\sqrt{n}} \\leq\n    z_{1-\\alpha/2} \\\\\n    \\Rightarrow{}& -\\bar X - z_{1-\\alpha/2} \\times \\sigma/\\sqrt{n}\n    \\leq -\\mu \\leq -\\bar X + z_{1-\\alpha/2} \\times\n    \\sigma/\\sqrt{n}\\\\\n    \\rule{0in}{3ex}\\Rightarrow{}& \\bar X - z_{1-\\alpha/2} \\times \\sigma/\\sqrt{n}\n    \\leq \\mu \\leq \\bar X + z_{1-\\alpha/2} \\times\n    \\sigma/\\sqrt{n}.\n\\end{aligned}\n\\] So, \\[\\begin{aligned}\n    \\left[ \\bar X - z_{1-\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}},\\\n      \\bar X + z_{1-\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}} \\right]  \n\\end{aligned}\n\\tag{5.1}\\]\nis a level \\(1-\\alpha\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is known.\nSuppose that we observe the sample \\[\n-1.90,-0.89,-0.87,-0.65,-0.32,-0.25, 0.90, 1.00, 1.18\n\\]\nThen, \\(n=9\\) and \\(\\bar x = -0.2\\) and recall we are assuming \\(\\sigma=1\\). For a 95% confidence interval, we find, using the standard normal distribution table (z table) that \\(z_{0.975} =\n  1.96\\) (see Figure 5.3). Then, \\[\\begin{aligned}\n    L &= \\bar x - z_{0.975} \\frac{\\sigma}{\\sqrt{n}} = -0.2 -(1.96)\n    \\frac{1}{3} = -0.85 \\\\\n    U &= \\bar x + z_{0.975} \\frac{\\sigma}{\\sqrt{n}} = -0.2 +(1.96)\n    \\frac{1}{3} = 0.45.  \n\\end{aligned}\\] So, a 95% confidence interval for \\(\\mu\\) is \\([-0.85,0.45]\\).\n\n\n\n\n\n\nFigure 5.3: Calculation of standard normal distribution quantiles. The circled number shows that \\(P(X&lt;1.96)=0.975\\) when \\(Z\\sim N(0,1)\\) i.e., \\(\\Phi(1.96)=0.975\\) so \\(z_p=1.96\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe lower and upper bounds of the confidence interval depend on the data as well as the desired significance level. The latter dependence is through the numbers \\(c_1\\) and \\(c_2\\). To make this dependence explicit, we can write \\(L({\\mathbf{X}},\\alpha)\\) and \\(U({\\mathbf{X}},\\alpha)\\) for the lower and upper bounds respectively. The width of the confidence interval is affected by the variation in the population, the sample size, and the desired confidence level.\n\nThe population variance, \\(\\sigma^2\\), is a measure of how different the members of the population are. If the population variance is large, then the variation within our sample will also be large, so the confidence interval will be wider.\nIf we take a large sample, i.e. if \\(n\\) is large, then the sample is more representative of the population, so we reduce the variability in the sample. Therefore, the confidence interval will be narrower.\nIf we decrease \\(\\alpha\\), i.e., if we desire a higher confidence level for our confidence interval, then the confidence interval should become wider.\n\n\n\n\n\n\n\n\n\n\nNoteStudent’s t distribution\n\n\n\n\nDefinition 5.3 Let \\(Z \\sim \\textrm{N}(0,1)\\) and let \\(W \\sim\n    \\mathcal{X}^2_\\nu\\) and suppose that \\(Z\\) and \\(W\\) are independent. Then the distribution of the random variable \\[\nY =\\frac{Z}{\\sqrt{W/\\nu}}\n\\] is called the Student’s \\(\\textrm{t}\\) distribution with \\(\\nu\\) degrees of freedom, written as \\(\\textrm{t}_\\nu\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe \\(\\textrm{t}_\\nu\\) distribution has similar shape as the \\(\\textrm{N}(0,1)\\) distribution. In particular it is symmetric around 0 and converges to \\(\\textrm{N}(0,1)\\) as \\(\\nu \\rightarrow \\infty\\). This is demonstrated in Figure 5.4.\n\n\n\n\n\n\n\n\n\nFigure 5.4: Density curves of the standard normal and \\(t_{\\nu}\\) distributions with degrees of freedom \\(\\nu = 2,5\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confidence Intervals and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "ch5.html#hypothesis-testing",
    "href": "ch5.html#hypothesis-testing",
    "title": "5  Confidence Intervals and Hypothesis Testing",
    "section": "5.2 Hypothesis testing",
    "text": "5.2 Hypothesis testing\nIn this section we discuss how, by using data, we can prove statements about the parameters of interest.\nConsider for instance the following scenario. Train companies regularly collect passenger data on customer satisfaction. One may ask whether the frequent ticket price increases cause any drop in average customer satisfaction. The population of interest is the train passengers and the parameter of interest is the average customer satisfaction. We are interested in assessing whether the ticket price increase has an impact on the average customer satisfaction.\n\n\n\n\n\n\nNoteStatistical hypothesis\n\n\n\n\nDefinition 5.4 A statistical hypothesis is a statement about the parameter value of the population under study.\n\n\n\n\n\n\n\n\n\nNoteTest of significance\n\n\n\n\nDefinition 5.5 The test of significance is a rule, based on data, for deciding which hypothesis is true between two competing hypotheses:\n\nthe null hypothesis (denoted by H\\(_0\\)), and\nthe alternative hypothesis (denoted by H\\(_1\\)).\n\nThe null hypothesis corresponds to the common belief about the parameter in question. It is interpreted as no change in the value of the parameter (the status quo).\nThe alternative hypothesis corresponds to a new claim which we wish to prove. It is interpreted as a change in the value of the parameter.\nThe outcome of a test of significance is the decision whether to reject or not the null hypothesis.\n\n\n\n\nExample 5.4 A company is selling bathroom toiletries and cosmetics. Their daily sales is a normally distributed random variable \\(\\mathrm{N}(\\mu,\\sigma^2)\\) with mean \\(\\mu = 2000\\) products. They believe that their plan of giving a free sample of one type of their products when you buy another of their products will increase their average daily sales by 100.\n\nWhat are the null and alternative hypotheses?\nThe claim is that with the offer the average daily sales will become 2100 (increase by 100). This statement determines the alternative hypothesis (a new claim). The null hypothesis corresponds to no change in the average daily sales, i.e. they will remain at 2000. Therefore, the two hypotheses are: H\\(_0\\): \\(\\mu=2000\\) and H\\(_1\\): \\(\\mu=2100\\).\nIf the claim was that the new offer will increase the sales (without saying by how much), what would the two hypotheses be?\nIn this case the claim is that the average daily sales will be some number \\(\\mu &gt; 2000\\) while the null hypothesis is as before. Then H\\(_0\\): \\(\\mu=2000\\) and H\\(_1\\): \\(\\mu&gt;2000\\).\nIf the claim was that the new offer will have some impact on the sales, what would the two hypotheses be?\nIn this case the claim is that the average daily sales will be some number different from \\(2000\\) while the null hypothesis is as before. Then H\\(_0\\): \\(\\mu=2000\\) and H\\(_1\\): \\(\\mu \\neq 2000\\).\nWhat can a statistician do to confirm the claim that the new offer improves sales?\nSuppose we want to compare H\\(_0\\): \\(\\mu=2000\\) vs H\\(_1\\): \\(\\mu&gt;2000\\). The company may consider the following experiment. Introduce the offer for a period of time, say \\(n=30\\) days, and record the number of sales on each day. Let \\(x_1,\\ldots,x_n\\) be the number of sales for each day, i.e. the sample, and let \\(\\bar x\\) be the sample average. If \\(\\bar x\\) turns out to be significantly larger than 2000, then there is evidence that the sales have improved.\nThere is no easy answer to what “significantly larger” means. That’s a matter of personal opinion. For some people if \\(\\bar x &gt;\n      2010\\) is enough to indicate increase in average daily sales but others may require \\(\\bar x &gt; 2100\\). If we set this “critical value” for \\(\\bar x\\) too low, say 2010, then there is the danger of a false positive conclusion, i.e. claiming that the average sales increased while in reality they stayed the same and the fact that \\(\\bar x &gt;\n      2010\\) was just due to variability in the sample. On the other hand, if the critical value is set to a large number, say 2100, then there is the possibility of a false negative conclusion, i.e. claiming that the average daily sales haven’t increased when in reality they did but not by that much as to bring \\(\\bar x\\) to exceed 2100.\n\n\nIn order to draw a conclusion in the example above, we had to summarise the data into one number, in that case \\(\\bar x\\), and compare this number against a critical boundary and depending on whether \\(\\bar x\\) exceeded or not this critical boundary then we decide which hypothesis to accept. This brings us to the concept of the test statistic and its critical value.\n\n\n\n\n\n\nNoteTest statistic and critical value\n\n\n\n\nDefinition 5.6 The test statistic associated with a hypothesis test is the statistic, i.e. a number derived from the sample (see Definition 2.2), which is used to make a decision in a hypothesis test. The value of the test statistic is compared against some predetermined number called the critical value of the statistic. If the value of the test statistic exceeds the critical value then our decision is to reject the H\\(_0\\).\n\n\n\nAs with any decision we make, we may reach the wrong conclusion. These are commonly referred to as “false positive” and “false negative” conclusions but in the language of statistics they are called Type I error and Type II error.\n\n\n\n\n\n\nNote\n\n\n\n\nType I error\n\nMeans that our decision is to reject H\\(_0\\) when in reality the H\\(_0\\) is true. We can also say we accept \\(H_1\\) when in reality H\\(_0\\) is true.\n\nType II error\n\nMeans that our decision is to accept H\\(_0\\) when in reality the H\\(_0\\) is false. We can also say we accept \\(H_0\\) when in reality H\\(_1\\) is true.\n\n\n\n\nWe would like to minimise the probability of reaching the wrong decision or maximise the probability of reaching the correct decision. Therefore for every hypothesis test we need to know the probabilities \\({\\mathbb{P}}(\\text{Type~I Error})\\) and \\({\\mathbb{P}}(\\text{Type~II Error})\\). We define\n\n\n\n\n\n\nNotePower of a test\n\n\n\n\nDefinition 5.7 For every hypothesis test, we define the power to be\n\\[\n    \\mathrm{power} = 1 - {\\mathbb{P}}(\\text{Type~II Error})\n    =1-P(\\mbox{Accept } H_0|H_1 \\mbox{ true})=P(\\mbox{Accept } H_1|H_1 \\mbox{ true}).\n\\]\n\n\n\nThe power can be interpreted as the probability of correctly rejecting H\\(_0\\) or correcyly accepting \\(H_1\\). So we want to have a test with high power. When the alternative hypothesis is a range of values, the power can be defined for all those values, so, in this case, it is represented by a function on \\(\\theta\\).\n\nExample 5.5 In the context of Example 5.4, suppose that the standard deviation is known to be \\(\\sigma=300\\) and we want to test\n\\[\nH_0:\\, \\mu=2000\\quad \\mbox{vs.}\\quad H_1:\\, \\mu&gt; 2000\n\\]\nIn a sample of \\(n=30\\) days we decide to use a critical value of 2070, i.e., we\n\\[\n\\mbox{reject the $H_0$ if }\\bar{X} &gt; 2070\n\\]\n\nWhat is the probability of Type I error?\nBy the definition of Type I error, the probability is \\({\\mathbb{P}}(\\text{Type~I error}) = {\\mathbb{P}}(\\text{Reject H$_0$}|\\text{H$_0$ is\n        true})\\).\nThe statement “Reject H\\(_0\\)” is equivalent to \\(\\bar X &gt; 2070\\) and the statement “H\\(_0\\) is true” is equivalent to \\(\\mu=2000\\). Therefore, \\({\\mathbb{P}}(\\text{Type~I error}) = {\\mathbb{P}}(\\bar X &gt; 2070|\\mu=2000)\\).\nIn order to compute this probability, we need to know the distribution of the random variable inside the parentheses, namely \\(\\bar X\\). This distribution is: \\[\n\\bar{X} \\sim \\mathrm{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\] that is, the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). In this case \\(\\mu=2000\\) and \\(\\sigma^2/n = 300^2/30\\). Then we have \\[\n{\\mathbb{P}}(\\bar X &gt; 2070|\\mu=2000) =\n      1-{\\mathbb{P}}(\\bar X \\leq 2070|\\mu=2000).\n\\] and \\[\n{\\mathbb{P}}(\\bar X \\leq 2070|\\mu=2000) = {\\mathbb{P}}\\left(\\frac{\\bar X - 2000}{300/\\sqrt{n}} \\leq\n      \\frac{2070-2000}{300/\\sqrt{30}}\\right)=\\Phi\\left(\\frac{2070-2000}{300/\\sqrt{30}}\\right)\n\\] recall Definition 1.20. Then, \\(z = \\frac{2070-2000}{300/\\sqrt{30}} = 1.28\\), which corresponds to probability \\(\\Phi(1.28) = 0.8997\\). Therefore,\n\\[\n{\\mathbb{P}}(\\text{Type~I error}) = 1-0.8997 = \\mathbf{0.1003}\\,.\n\\]\nWhat is the probability of Type II error if the true mean is 2100?\nBy the definition of Type II error, the probability is \\({\\mathbb{P}}(\\text{Type~II error}) = {\\mathbb{P}}(\\text{Accept H$_0$}|\\text{H$_0$ is\n        false})\\).\nAs before, the statement “Accept H\\(_0\\)” is equivalent to \\(\\bar X\n      \\leq 2070\\) and the statement “H\\(_0\\) is false” in this case is equivalent to \\(\\mu=2100\\). Therefore, \\({\\mathbb{P}}(\\text{Type~I error}) =\n      {\\mathbb{P}}(\\bar X \\leq 2070|\\mu=2100)\\). The distribution of \\(\\bar X\\) in this case is normal with mean \\(\\mu=2100\\) and variance \\(\\sigma^2/n = 300^2/30\\).\nThen, \\(z = \\frac{2070-2100}{300/\\sqrt{30}} = -0.55\\), which corresponds to probability \\(\\Phi(-0.55) = 0.2912\\). Therefore,\n\\({\\mathbb{P}}(\\text{Type~II error}) = \\mathbf{0.2912}\\) and the corresponding \\(\\mathrm{power} = 1-0.2912 = \\mathbf{0.7088}\\). Note that if we assume a different value for \\(\\mu\\) under H\\(_1\\), we will get a different probability of Type II error and therefore a different power.\nWhat is the power function?\nTake any \\(\\mu\\) such that \\(\\mu &gt; 2000\\). Then, as before \\(z =\n      \\frac{2070-\\mu}{300/\\sqrt{30}}\\), so, the power function is given by \\(\\mathrm{power}(\\mu) = 1 - \\Phi(\\frac{2070-\\mu}{300/\\sqrt{30}}) =\n      \\Phi(\\frac{\\mu-2070}{300/\\sqrt{30}})\\), because of the property \\(\\Phi(z)=1-\\Phi(z)\\) which is derived from the symmetry of the pdf of the standard normal distribution.\nThis function is plotted in Figure 5.5. We observe that the further \\(\\mu\\) is from 2000, the higher the power. This can be interpreted as being more likely to reject H\\(_0\\) correctly when the true mean is further from 2000.\n\n\n\n\n\n\n\nFigure 5.5: The power function for Example 5.5\n\n\n\nIn this example, \\(\\bar X\\) is the test statistic and the number 2070 is the critical value \\(c\\). The probabilities of Type I and Type II errors can be illustrated in Figure 5.6. The bell curve on the left is the distribution of \\(\\bar X\\) under H\\(_0\\) and the dark grey area is the probability of Type I error. Similarly, the bell curve on the right is the distribution of \\(\\bar X\\) under H\\(_1\\) when the mean is 2100 and the light grey area is the probability of Type II error. Both probabilities correspond to the critical value \\(c=2070\\).\n\n\n\n\n\n\nFigure 5.6: Illustration of the probabilities of Type I (dark gray area) and Type II (light gray area) errors for Example 5.5\n\n\n\n\nNow let’s see what happens if the critical value of 2070 changes.\nIf we increase the critical value, the probability of Type I error (dark grey area) becomes smaller. This means that it becomes less likely to reject H\\(_0\\) erroneously (a false positive). However, we also increase the probability of Type II error (light grey area) so it becomes more likely to accept H\\(_0\\) when we shouldn’t (a false negative). Unfortunately this is a usual impediment when performing hypothesis tests. In practice, we demand the probability of Type I error to be comfortably small typically around 5% which determines the critical value. This concept gives rise to the notion of significance level and \\(p\\)-value.\n\n\n\n\n\n\nNoteSignificance level\n\n\n\n\nDefinition 5.8 It is desirable to have a rule for rejecting H\\(_0\\) with small probability of Type I error. In practice this probability is set to a fixed, prescribed, value denoted by \\(\\alpha\\) (the greek letter “alpha”) called the level of significance and a rule with the property \\({\\mathbb{P}}(\\text{Type~I error}) = \\alpha\\) is sought.\nThe smaller the value of \\(\\alpha\\), the more evidence needed to reject the H\\(_0\\). Typical values for \\(\\alpha\\) are 1%, 5% and 10%.\n\n\n\n\nExample 5.6 In Example 5.5, we say that the rule “reject H\\(_0\\) if \\(\\bar X &gt;\n  2070\\)” has \\({\\mathbb{P}}(\\text{Type I error}) \\approx 10\\%\\). Therefore, if we want a rule with significance level \\(\\alpha = 10\\%\\), we need to choose a critical value of \\(c = 2070\\).\nSuppose now that we are seeking for a rule of the form “reject H\\(_0\\) if \\(\\bar X &gt; c\\)”, where \\(c\\) must be chosen according to a significance level \\(\\alpha = 5\\%\\). This means that \\[\n\\begin{aligned}\n    0.05 &= {\\mathbb{P}}(\\text{Type I error}) \\\\\n    &= {\\mathbb{P}}(\\text{Reject H$_0$} \\mid \\text{H$_0$ true}) \\\\\n    &= {\\mathbb{P}}(\\bar X &gt; c \\mid \\mu = 2000) \\\\\n    \\Rightarrow 0.95 &= {\\mathbb{P}}(\\bar X \\leq c \\mid \\mu = 2000) \\\\\n    &= \\Phi\\left(\\frac{c-2000}{300/\\sqrt{30}}\\right) \\\\\n    \\Rightarrow z_{0.95} &= \\frac{c-2000}{300/\\sqrt{30}} \\\\\n    \\Rightarrow c &= 2000 + z_{0.95} \\times 300/\\sqrt{30} \\\\\n    &= 2090,\n\\end{aligned}\n\\] where we used the fact that \\(z_{0.95} = 1.645\\) because \\(\\Phi(1.645) =\n  0.95\\). Therefore, using a critical value of 2090 produces a test with \\({\\mathbb{P}}(\\text{Type I error}) = 0.05\\), or, in other words, a level of significance \\(\\alpha = 5\\%\\).\nUsing \\(c=2090\\) instead of \\(c=2070\\) reduces the probability of wrongly reject the null hypothesis by half. What about the probability of correctly rejecting H\\(_0\\) when the true mean value is \\(\\mu = 2100\\)?\nAs in Example 5.5, \\({\\mathbb{P}}(\\text{Type II error}) = \\Phi\\left(\\dfrac{2090 -\n      2100}{300/\\sqrt{30}}\\right) = \\Phi(-0.1826) = 0.4276\\) and \\(\\text{power} = 1 - 0.4276 = 0.5724\\). So as expected, although using the critical value \\(c=2090\\) compared to \\(c=2070\\) reduced the probability of Type I error, it increases the probability of Type II error, and consequently, reduces the power of the test.\n\n\n\n\n\n\n\nNoteP-value\n\n\n\n\nDefinition 5.9 The \\(p\\)-value is the smallest significance level which we can set and still be able to reject H\\(_0\\) with the given data. By definition, the \\(p\\)-value is a probability which depends on the sample we are analysing. If \\(\\text{$p$-value} &lt; \\alpha\\) then the data provide enough evidence to reject H\\(_0\\).\n\n\n\nThe two definitions suggest two paths one can follow to conduct a hypothesis test. In both cases, one the data and a significance level are provided. Then we could either\n\nUse the data to derive the test statistic and use the significance level to derive the critical value. If the value of the test statistic exceeds the critical value then we reject H\\(_0\\), otherwise we accept it.\nUse the data to derive the test statistic and from there derive the corresponding \\(p\\)-value. If the \\(p\\)-value is smaller than the significance level then we reject H\\(_0\\), otherwise we accept it.\n\nThese two paths are depicted below.\n\n\n\n\n\n\nFigure 5.7\n\n\n\nIt shouldn’t matter which of the two approaches we take to perform the hypothesis test as they are equivalent, meaning that the lead to the same conclusion regarding the rejection of the null hypothesis.\n\nExample 5.7 Following Example 5.6, suppose we seek to reject the null hypothesis with significant level \\(\\alpha = 5\\%\\). Then, according to that example, we must choose a critical value of \\(c=2090\\).\nSuppose next that we collect \\(n=30\\) observations and found that the average sales were \\(\\bar x = 2050\\). Then, according to our hypothesis test, we do not reject the null hypothesis because \\(2050 \\ngtr 2090\\), so we cannot conclude that the average sales increased.\nWhat is the smallest level of significance \\(\\alpha\\) that we could set, such that, the critical value \\(c\\) that corresponds to that level would allow us to reject the null hypothesis? Since \\(\\alpha = 5\\%\\) does not allow us to reject the null hypothesis, it is clear that we should seek for a higher significance level, as we are more relaxed about wrongly reject the null hypothesis. So we expect to find \\(\\text{$p$-value} &gt; 0.05\\). Indeed, if we choose a significance level \\(p\\) such that the corresponding critical value \\(c\\) falls exactly at \\(c=2050\\), then \\(p\\) is the \\(p\\)-value. This is illustrated in Figure 1.6.\n\n\n\n\n\n\nFigure 5.8: Illustration of \\(p\\)-value for Example 5.7\n\n\n\nReversing the calculations in Example 5.6, if \\(p\\) is the \\(p\\)-value, \\(z_{1-p}\\) is the quantile that achieves a critical value of 2050, so \\[\\begin{aligned}\n    2050 &= 2000 + z_{1-p} \\times 300 / \\sqrt{30} \\\\\n    \\Rightarrow z_{1-p} &= \\frac{2050 - 2000}{300 / \\sqrt{30}} \\\\\n    &= 0.9129 \\\\\n    \\Rightarrow 1-p &= \\Phi(0.9129) \\\\\n    \\Rightarrow p &= 1 - \\Phi(0.9129) = 1-0.82 = 0.18.  \n\\end{aligned}\n\\] So \\(\\text{$p$-value} = 0.18\\). Accordingly, we can see that, since \\(\\text{$p$-value} &gt; 5\\%\\), we do not reject the null hypothesis at the 5% level. In fact, \\(\\text{$p$-value} &gt; 10\\%\\), so we do not reject the null hypothesis at the 10% level either.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confidence Intervals and Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "ch5.html#sec-exercises-5",
    "href": "ch5.html#sec-exercises-5",
    "title": "5  Confidence Intervals and Hypothesis Testing",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\n\nConfidence intervals:\n\n\nLet \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{N}(\\mu,\\sigma^2)\\) where both \\(\\mu\\) and \\(\\sigma^2\\) are unknown parameters. Using the fact that, the sample variance, \\(S^2\\), is distributed as \\((n-1)S^2/\\sigma^2 \\sim \\mathcal{X}^2_{n-1}\\), derive a level \\(1-\\alpha\\) confidence interval for \\(\\sigma^2\\). (\\(\\mathcal{X}^2_{k}\\) denotes the chi-squared distribution with \\(k\\) degrees of freedom, which is available in python as scipy.stats.chi2. It is a special case of the gamma distribution with shape \\(k/2\\) and rate \\(1/2\\).)\nSuppose that the following data were observed \\[-1.90,-0.89,-0.87,-0.65,-0.32,-0.25, 0.90, 1.00, 1.18\\] For these data \\(n=9\\), \\(\\bar x = -0.2\\), and \\(S^2 = 1.073\\). Calculate a 95% confidence interval for \\(\\sigma^2\\).\n\n\nLet \\(X_1,\\ldots,X_n {{}\\mathbin{\\stackrel{\\mathsf{iid}}{\\sim}}{}}\\mathrm{U}(0,\\theta)\\), \\(\\theta &gt; 0\\). By considering an appropriate pivot construct a level \\(1-\\alpha\\) confidence interval for \\(\\theta\\). Hint. Let \\(W_i = X_i/\\theta\\).\nSuppose that the following data were observed \\[0.90, 1.00, 1.18, 1.90, 2.20.\\] Calculate a 95% confidence interval for \\(\\theta\\). Does the confidence interval contain the maximum likelihood estimator for \\(\\theta\\)?\n\n\n\n\nHypothesis testing:\n\nThe author of a weight-loss diet claims that an average adult, weighting 100 Kg, who follows the proposed diet, will lose 20 Kg after 1 month. What are the null and alternative hypotheses?\nThe author of a weight-loss diet claims that an average adult, weighting 100 Kg, who follows the proposed diet, will lose weight after 1 month. What are the null and alternative hypotheses?\nThe author of a weight-loss diet claims that an average adult, weighting 100 Kg, who follows the proposed diet, will notice a change in their weight after 1 month. What are the null and alternative hypotheses?\nThe author of a weight-loss diet claims that an average adult, weighting 100 Kg, who follows the proposed diet, will lose weight after 1 month. An experiment was conducted to verify this claim. Three adults, who weighted 100 Kg, followed the diet for one month and their weights at the end of the month were recorded. The experimenters would accept the author’s claim if the sample mean \\(\\bar X\\) of the three measured weights is less than 90. Suppose that the population standard deviation is \\(\\sigma=15\\).\n\nThe three people’s weights after the end of the month were: 82, 86, and 93. What is the experimenters’ conclusion?\nAccording to the central limit theorem, what is the asymptotic distribution of the sample mean of \\(n=3\\) measurements from a population with mean \\(\\mu=100\\) and standard deviation \\(\\sigma=15\\)?\n Use the central limit theorem to calculate the probability of Type I error of the experimenters’ decision rule.\nUse the central limit theorem to calculate the probability of Type II error of the experimenters’ decision rule assuming that the average weight after one month is 85.\nPropose a rule of the form “accept the author’s claim if \\(\\bar X &lt;\n    c\\)” (in other words find \\(c\\)) such that the probability of Type I error is 10%.\nSuppose that in the sample we find that \\(\\bar x = 86\\). Find the \\(p\\)-value. What is your conclusion at significance level \\(\\alpha = 5\\%\\)?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confidence Intervals and Hypothesis Testing</span>"
    ]
  }
]