---
title: "MA52112: Statistics for Data Science"
subtitle: "Exercise sheet 2 (Probability) - Solutions"
format: 
  html:
    embed-resources: true
---


1. Let the joint PDF of a random vector $(X,Y)$ be given by:

$$
f(x,y) = 
\begin{cases} 
      1 & 0 < x< 1 \,,\quad x <  y < x+1 \\
      0 & \text{otherwise}
\end{cases}
$$


a) sketch the support of the joint distribution.

We can do this in Python:

```{python}
import numpy as np
import matplotlib.pyplot as plt
# Define the support
x = np.linspace(0, 1, 100)
y1 = x
y2 = x + 1
# Plot the support
plt.figure(figsize=(8, 6))
plt.fill_between(x, y1, y2, color='lightblue', alpha=0.5, label='Support of f(x,y)')
plt.plot(x, y1, color='blue')
plt.plot(x, y2, color='blue')
plt.xlim(-0.1, 1.1)
plt.ylim(-0.1, 2.1)
plt.xlabel('x')
plt.ylabel('y')
plt.axhline(0, color='black',linewidth=0.5, ls='--')
plt.axvline(0, color='black',linewidth=0.5, ls='--')
# add grid lines at 0,1,2

plt.grid()
plt.xticks(np.array([0,1]))
plt.yticks(np.array([0,1,2]))

plt.legend()
plt.show()
```


b) Verify that $f(x,y)$ is a valid joint PDF.


To verify that $f(x,y)$ is a valid joint PDF, we need to check two conditions:

* Non-negativity: $f(x,y) \geq 0$ for all $(x,y)$.
* Normalization: The integral of $f(x,y)$ over the entire space must equal 1.

* For non-negativity: From the definition of $f(x,y)$, we see that it is equal to 1 in the region where $0 < x < y < x+1$ and 0 elsewhere. Therefore, $f(x,y) \geq 0$ for all $(x,y)$.

* For the normalization:
We need to compute the double integral of $f(x,y)$ over the entire space:
$$
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x,y) \, dy \, dx
$$
Since $f(x,y)$ is non-zero only in the region where $0 < x < 1$ and $x < y < x+1$, we can limit our integration to this region:
$$
\int_{0}^{1} \int_{x}^{x+1} 1 \, dy \, dx
$$
Calculating the inner integral:
$$
\int_{x}^{x+1} 1 \, dy = y \Big|_{y=x}^{y=x+1} = (x+1) - x = 1
$$
Now, we substitute this result into the outer integral:
$$
\int_{0}^{1} 1 \, dx = x \Big|_{0}^{1} = 1 - 0 = 1
$$
Since both conditions are satisfied, $f(x,y)$ is a valid joint PDF.

c) Find the marginal PDFs of $X$ and $Y$.
To find the marginal PDFs of $X$ and $Y$, we need to integrate the joint PDF $f(x,y)$ over the appropriate variable.

* Marginal PDF of $X$:
$$
f_X(x) = \int_{-\infty}^{\infty} f(x,y) \, dy
$$
Since $f(x,y)$ is non-zero only in the region where $0 < x < 1$ and $x < y < x+1$, we can limit our integration to this region:
$$
f_X(x) = \int_{x}^{x+1} 1 \, dy = y \Big|_{y=x}^{y=x+1} = (x+1) - x = 1
$$
Thus, the marginal PDF of $X$ is:
$$
f_X(x) =
\begin{cases} 
      1 & 0 < x < 1 \\
      0 & \text{otherwise}
\end{cases}
$$

So the marginal PDF of $X$ is uniform on the interval (0, 1).

* Marginal PDF of $Y$:
$$
f_Y(y) = \int_{-\infty}^{\infty} f(x,y) \, dx
$$
Since $f(x,y)$ is non-zero only in the region where $0 < x < 1$ and $x < y < x+1$, we can limit our integration to this region. For a fixed $y$, the limits for $x$ are from $\max(0, y-1)$ to $\min(1, y)$:
$$
f_Y(y) = \int_{\max(0, y-1)}^{\min(1, y)} 1 \, dx
$$
Calculating this integral:
1. For $0 < y < 1$:
$$
f_Y(y) = \int_{0}^{y} 1 \, dx = y - 0 = y
$$
2. For $1 \leq y < 2$:
$$
f_Y(y) = \int_{y-1}^{1} 1 \, dx = 1 - (y-1) = 2 - y
$$
3. For $y \leq 0$ or $y \geq 2$:
$$
f_Y(y) = 0
$$
Thus, the marginal PDF of $Y$ is:
$$
f_Y(y) =
\begin{cases} 
      y & 0 < y < 1 \\
      2 - y & 1 \leq y < 2 \\
      0 & \text{otherwise}
\end{cases}
$$
So the marginal PDF of $Y$ is a triangular distribution on the interval (0, 2). Plot this PDF to see its a triangle.

d) Are $X$ and $Y$ independent? Justify your answer.

Clearly, $X$ and $Y$ are not independent since the support of $Y$ depends on the value of $X$. 

We can further check if the joint PDF $f(x,y)$ can be expressed as the product of the marginal PDFs $f_X(x)$ and $f_Y(y)$ for all $(x,y)$ in the support of the joint distribution.
We have:
$$
f(x,y) =
\begin{cases} 
      1 & 0 < x < 1 \,,\quad x <  y < x+1 \\
      0 & \text{otherwise}
\end{cases}
$$
$$
f_X(x) =
\begin{cases} 
      1 & 0 < x < 1 \\
      0 & \text{otherwise}
\end{cases}
$$
$$
f_Y(y) =
\begin{cases} 
      y & 0 < y < 1 \\
      2 - y & 1 \leq y < 2 \\
      0 & \text{otherwise}
\end{cases}
$$
Now, we need to check if:
$$
f(x,y) = f_X(x) \cdot f_Y(y)
$$
For $0 < x < 1$ and $x < y < x+1$, we have:
$$
f_X(x) \cdot f_Y(y) = 1 \cdot f_Y(y) = f_Y(y)
$$

However, $f_Y(y)$ varies depending on the value of $y$. Specifically:

- If $0 < y < 1$, then $f_Y(y) = y$, which is not equal to 1.
- If $1 \leq y < 2$, then $f_Y(y) = 2 - y$, which is also not equal to 1.
- If $y \leq 0$ or $y \geq 2$, then $f_Y(y) = 0$, which is not equal to 1.

4) Find the conditional distribution of $Y$ given $X=x$.

For any $0<x<1$ we can obtain the conditional distribution of $Y$ given $X=x$ as follows:
$$
f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)} =
\begin{cases} 
      1 & x <  y < x+1 \\
      0 & \text{otherwise}
\end{cases}
$$

So the conditional distribution of $Y$ given $X=x$ is uniform on the interval $(x, x+1)$.

This means that, conditional on $X=x$ we can actually write $Y$ as:
$$
Y|x = x + U
$$
where $U$ is a uniform random variable on the interval (0, 1) . The equality above means the two random variables have the same distribution.

f) Calculate the correlation coefficient between $X$ and $Y$.

To calculate the correlation coefficient between $X$ and $Y$, we need to compute the following quantities:
- The means of $X$ and $Y$: $E[X]$ and $E[Y]$.
- The variances of $X$ and $Y$: $Var(X)$ and $Var(Y)$.
- The covariance between $X$ and $Y$: $Cov(X,Y)$.


* Calculate $E[X]$:
$$
E[X] = \int_{0}^{1} x f_X(x) \, dx = \int_{0}^{1} x \cdot 1 \, dx = \frac{x^2}{2} \Big|_{0}^{1} = \frac{1}{2}
$$

* Calculate $E[Y]$:
$$
E[Y] = \int_{0}^{1} y f_Y(y) \, dy + \int_{1}^{2} y f_Y(y) \, dy
= \int_{0}^{1} y^2 \, dy + \int_{1}^{2} y(2 - y) \, dy
= \frac{y^3}{3} \Big|_{0}^{1} + \left( y^2 - \frac{y^3}{3} \right) \Big|_{1}^{2}
= \frac{1}{3} + \left( 4 - \frac{8}{3} - 1 + \frac{1}{3} \right) = \frac{1}{3} + \left( 3 - \frac{7}{3} \right) = \frac{1}{3} + \frac{2}{3} = 1
$$

* Calculate $Var(X)$:
$$
E[X^2] = \int_{0}^{1} x^2 f_X(x) \, dx = \int_{0}^{1} x^2 \cdot 1 \, dx = \frac{x^3}{3} \Big|_{0}^{1} = \frac{1}{3}
$$
$$
Var(X) = E[X^2] - (E[X])^2 = \frac{1}{3} - \left(\frac{1}{2}\right)^2 = \frac{1}{3} - \frac{1}{4} = \frac{4}{12} - \frac{3}{12} = \frac{1}{12}
$$

* Calculate $Var(Y)$:
$$
E[Y^2] = \int_{0}^{1} y^2 f_Y(y) \, dy + \int_{1}^{2} y^2 f_Y(y) \, dy
= \int_{0}^{1} y^3 \, dy + \int_{1}^{2} y^2(2 - y) \, dy
= \frac{y^4}{4} \Big|_{0}^{1} + \left( \frac{2y^3}{3} - \frac{y^4}{4} \right) \Big|_{1}^{2}
= \frac{1}{4} + \left( \frac{16}{3} - 4 - \frac{2}{3} + \frac{1}{4} \right) = \frac{1}{4} + \left( \frac{14}{3} - 4 + \frac{1}{4} \right) = \frac{1}{4} + \left( \frac{14}{3} - \frac{16}{4} + \frac{1}{4} \right) = \frac{1}{4} + \left( \frac{14}{3} - \frac{15}{4} \right) = \frac{1}{4} + \left( \frac{56 - 45}{12} \right) = \frac{1}{4} + \frac{11}{12} = \frac{3}{12} + \frac{11}{12} = \frac{14}{12} = \frac{7}{6}
$$
$$
Var(Y) = E[Y^2] - (E[Y])^2 = \frac{7}{6} - 1^2 = \frac{7}{6} - 1 = \frac{1}{6}
$$

* Calculate $Cov(X,Y)$:
To calculate $Cov(X,Y)$, we need to compute $E[XY]$:
$$
E[XY] = \int_{0}^{1} \int_{x}^{x+1} xy f(x,y) \, dy \, dx = \int_{0}^{1} \int_{x}^{x+1} xy \cdot 1 \, dy \, dx
= \int_{0}^{1} x \left( \frac{y^2}{2} \Big|_{y=x}^{y=x+1} \right) dx = \int_{0}^{1} x \left( \frac{(x+1)^2}{2} - \frac{x^2}{2} \right) dx = \int_{0}^{1} x \left( \frac{x^2 + 2x + 1 - x^2}{2} \right) dx = \int_{0}^{1} x \left( \frac{2x + 1}{2} \right) dx = \int_{0}^{1} \left( x^2 + \frac{x}{2} \right) dx = \frac{x^3}{3} + \frac{x^2}{4} \Big|_{0}^{1} = \frac{1}{3} + \frac{1}{4} = \frac{4}{12} + \frac{3}{12} = \frac{7}{12}
$$
$$
Cov(X,Y) = E[XY] - E[X]E[Y] = \frac{7}{12} - \left(\frac{1}{2} \cdot 1\right) = \frac{7}{12} - \frac{1}{2} = \frac{7}{12} - \frac{6}{12} = \frac{1}{12}
$$

* Calculate the correlation coefficient $\rho_{X,Y}$:
$$
\rho_{X,Y} = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} = \frac{\frac{1}{12}}{\sqrt{\frac{1}{12} \cdot \frac{1}{6}}} = \frac{\frac{1}{12}}{\sqrt{\frac{1}{72}}} = \frac{\frac{1}{12}}{\frac{1}{\sqrt{72}}} = \frac{1}{12} \cdot \sqrt{72} = \frac{\sqrt{72}}{12} = \frac{\sqrt{36 \cdot 2}}{12} = \frac{6\sqrt{2}}{12} = \frac{\sqrt{2}}{2}
$$
Thus, the correlation coefficient between $X$ and $Y$ is $\rho_{X,Y} = \frac{\sqrt{2}}{2} \approx 0.707$. This indicates a positive correlation between the two variables.




We can check in Python by simulating a large number of samples from the joint distribution and calculating the sample correlation coefficient. Note how we simulate from the joint by:

* First, we sample $X$ from its marginal distribution (uniform on (0, 1)).

* Then, for each sampled value of $X$, we sample $Y$ from its conditional distribution given $X$ (uniform on $(X, X+1)$).


```{python}
import numpy as np
import matplotlib.pyplot as plt
# Number of samples
n_samples = 10000
# Generate samples for X uniformly in (0, 1)
X = np.random.uniform(0, 1, n_samples)
# Generate samples for Y given X (see argument above)
Y = X + np.random.uniform(0, 1, n_samples)
# Calculate the sample correlation coefficient
correlation_coefficient = np.corrcoef(X, Y)[0, 1]
print(f"Sample correlation coefficient: {correlation_coefficient}")
# Plot the joint distribution
plt.figure(figsize=(8, 6))
plt.scatter(X, Y, alpha=0.1)
plt.title('Scatter plot of samples from joint distribution')
plt.xlabel('X')
plt.ylabel('Y')
plt.xlim(0, 1)
plt.ylim(0, 2)
plt.grid()
# add grid lines at 0,1,2
plt.xticks(np.array([0,1]))
plt.yticks(np.array([0,1,2]))
plt.show()
```


2. A random point $(X,Y)$ is distributed uniformly on the square with vertices at $(1,1)$, $(1,-1)$, $(-1,1)$ and $(-1,-1)$. That is, the joint PDF of $(X,Y)$ is:
$$
f(x,y) =
\begin{cases} 
      \frac{1}{4} & -1 < x < 1 \,,\quad -1 < y < 1 \\
      0 & \text{otherwise}
\end{cases}
$$
Determine the probabilities of the following events:
a) $P(X^2 + Y^2 < 1)$
b) $P(X-Y>0)$
c) $P(|X + Y| < 2)$

**Solution**

a) $P(X^2 + Y^2 < 1)$
Intuitively, this is the area of the unit circle divided by the area of the square. So the answer is $\frac{\pi}{4}$.

We can formally obtain this by integration:

$$
P(X^2 + Y^2 < 1) = \int_{-1}^{1} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \frac{1}{4} \, dy \, dx
= \frac{1}{4} \int_{-1}^{1} 2\sqrt{1-x^2} \, dx = \frac{1}{2} \int_{-1}^{1} \sqrt{1-x^2} \, dx
= \frac{1}{2} \cdot \frac{\pi}{2} = \frac{\pi}{4}
$$

b)   $P(X-Y>0)$ 

We can obtain this by integration:

$$
P(X-Y>0) = \int_{-1}^{1} \int_{-1}^{x} \frac{1}{4} \, dy \, dx
= \frac{1}{4} \int_{-1}^{1} (x + 1) \, dx = \frac{1}{4} \left( \frac{x^2}{2} + x \right) \Big|_{-1}^{1} = \frac{1}{4} \left( \left(\frac{1}{2} + 1\right) - \left(\frac{1}{2} - 1\right) \right) = \frac{1}{4} \cdot 2 = \frac{1}{2}
$$

c)   $P(|X + Y| < 2)$
We can obtain this by integration:
$$
P(|X + Y| < 2) = \int_{-1}^{1} \int_{-1}^{1} \frac{1}{4} \, dy \, dx = 1
$$
This is because the condition $|X + Y| < 2$ is always satisfied within the square defined by $-1 < x < 1$ and $-1 < y < 1$.


3. A pdf of a random vector $(X,Y)$ is given by:
$$
f(x,y) =
\begin{cases} 
      k(x+2y) & 0<x<2\,,\,0<y<1\\
      0 & \text{otherwise}
\end{cases}
$$

a) Find the value of $k$ that makes $f(x,y)$ a valid PDF.
To find the value of $k$, we need to ensure that the integral of the PDF over its entire support equals 1:

$$
\int_{0}^{2} \int_{0}^{1} k(x + 2y) \, dy \, dx = 1
$$
Calculating the inner integral:
$$
\int_{0}^{1} (x + 2y) \, dy = xy + y^2 \Big|_{0}^{1} = x + 1
$$
Now, we substitute this result into the outer integral:
$$
\int_{0}^{2} k(x + 1) \, dx = k \left( \frac{x^2}{2} + x \right) \Big|_{0}^{2} = k \left( 2 + 2 \right) = 4k
$$
Setting this equal to 1, we find:
$$
4k = 1 \implies k = \frac{1}{4}
$$
Thus, the value of $k$ that makes $f(x,y)$ a valid PDF is $k = \frac{1}{4}$.


b) Find the marginal PDFs of $X$ and $Y$.
To find the marginal PDFs of $X$ and $Y$, we need to integrate the joint PDF $f(x,y)$ over the appropriate variable.
* Marginal PDF of $X$:
$$
f_X(x) = \int_{0}^{1} f(x,y) \, dy = \int_{0}^{1} \frac{1}{4}(x + 2y) \, dy = \frac{1}{4} \left( xy + y^2 \Big|_{0}^{1} \right) = \frac{1}{4} (x + 1)
$$
Thus, the marginal PDF of $X$ is:
$$
f_X(x) =
\begin{cases} 
      \frac{x + 1}{4} & 0 < x < 2 \\
      0 & \text{otherwise}
\end{cases}
$$
* Marginal PDF of $Y$:
$$
f_Y(y) = \int_{0}^{2} f(x,y) \, dx = \int_{0}^{2} \frac{1}{4}(x + 2y) \, dx = \frac{1}{4} \left( \frac{x^2}{2} + 2yx \Big|_{0}^{2} \right) = \frac{1}{4} (2 + 4y) = \frac{1 + 2y}{2}
$$
Thus, the marginal PDF of $Y$ is:
$$
f_Y(y) =
\begin{cases} 
      \frac{1 + 2y}{2} & 0 < y < 1 \\
      0 & \text{otherwise}
\end{cases}
$$
c) Find the conditional distribution of $Y$ given $X=x$.
The conditional distribution of $Y$ given $X=x$ is given by:
$$
f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)} = \frac{\frac{1}{4}(x + 2y)}{\frac{x + 1}{4}} = \frac{x + 2y}{x + 1}
$$
for $0 < y < 1$ and $0 < x < 2$. Outside this range, the conditional PDF is 0.
Thus, the conditional distribution of $Y$ given $X=x$ is:
$$
f_{Y|X}(y|x) =
\begin{cases} 
      \frac{x + 2y}{x + 1} & 0 < y < 1 \\
      0 & \text{otherwise}
\end{cases}
$$





4.  A woman leaves for work between 8 AM and 8:30 AM and takes between 40 and 50
minutes to get there. Let the random variable X denote her time of departure, and
the random variable Y the travel time. Assuming that these variables are independentand uniformly distributed, find the probability that the woman arrives at work before 9 AM.

**Solution**
We can define the random variables as follows:

- $X$: Time of departure, uniformly distributed between 8:00 AM (0 minutes) and 8:30 AM (30 minutes).

- $Y$: Travel time, uniformly distributed between 40 minutes and 50 minutes.
We want to find the probability:
$$
P(X + Y < 60)
$$

This means that the
woman arrives at work before 9:00 AM (60 minutes after 8:00 AM).
We can start by finding the joint distribution of $(X,Y)$.
The PDF of $X$ is:
$$
f_X(x) =
\begin{cases} 
      \frac{1}{30} & 0 < x < 30 \\
      0 & \text{otherwise}
\end{cases}
$$
The PDF of $Y$ is:
$$
f_Y(y) =
\begin{cases} 
      \frac{1}{10} & 40 < y < 50 \\
      0 & \text{otherwise}
\end{cases}
$$
Since $X$ and $Y$ are independent, the joint PDF is:
$$
f(x,y) = f_X(x) \cdot f_Y(y) =
\begin{cases} 
      \frac{1}{300} & 0 < x < 30 \,,\,40 < y < 50 \\
      0 & \text{otherwise}
\end{cases}
$$

We need to find the probability that $X + Y < 60$. We can do this by integrating the joint PDF over the appropriate region:
$$
P(X + Y < 60) = \int_{50}^{50} \int_{0}^{60 - y} \frac{1}{300} \, dx \, dy
$$
Calculating the inner integral:
$$
\int_{0}^{60 - y} \frac{1}{300} \, dx = \frac{60 - y}{300}
$$
Now, we substitute this result into the outer integral:

$$
P(X + Y < 60) = \int_{40}^{50} \frac{60 - y}{300} \, dy = \frac{1}{300} \left( 60y - \frac{y^2}{2} \right) \Big|_{40}^{50} = \frac{1}{300} \left( (3000 - 1250) - (2400 - 800) \right) = \frac{1}{300} \cdot 150 = \frac{1}{2}
$$
Thus, the probability
that the
woman
arrives at work before 9 AM is $\frac{1}{2}$ or 50%.




5. Consider a continuous random variable $X$ that follows a Normal distribution with mean $\mu=4$ and variance $\sigma^2=4$. That is, $X \sim N(10, 4)$.
a) What is the probability that $X$ takes negative values?
To find the probability that $X$ takes negative values, we need to calculate:
$$
P(X < 0)
$$

We have 
$$
P(X<0)=P\left(\frac{X-\mu}{\sigma}<\frac{0-4}{2}\right)=P(Z<-2)
$$
where $Z$ is a standard normal random variable, i.e., $Z \sim N
(0, 1)$.
We can find this probability using standard normal distribution tables or a computational tool. Using Python, we can calculate this as follows:
```{python}
import scipy.stats as stats
# Mean and standard deviation
mu = 4
sigma = 2
# Calculate the probability P(X < 0)
probability = stats.norm.cdf(0, loc=mu, scale=sigma)
print(f"P(X < 0) = {probability}")
```

So is about 2\%. 

b) What is the probability that $X$ takes values between 5 and 10?
To find the probability that $X$ takes values between 5 and 10, we need
to calculate:
$$
P(5 < X < 10) = P\left(\frac{5-4}{2} < Z < \frac{10-4}{2}\right) = P(
0.5 < Z < 3)
$$
We can find this probability using standard normal distribution tables or a computational tool. Using Python, we
can calculate this as follows:
```{python}
# Calculate the probability P(5 < X < 10)
probability2 = stats.norm.cdf(10, loc=mu, scale=sigma) - stats.norm.cdf(5, loc=mu, scale=sigma)
print(f"P(5 < X < 10) = {probability2}")
```
so is about 30%.

**Note** In the exam you would need to compute cumulative normal probabilities using tables. 

6. Show that the Moment Generating Function (MGF) of a Geometric distribution with parameter $p$ is given by:
$$
M_X(t) = \frac{pe^t}{1-(1-p)e^t} \quad \text{for } t < -\ln(1-p)
$$
The MGF of a random variable $X$ is defined as:
$$
M_X(t) = E[e^{tX}] = \sum_{k=1}^{\infty} e^{tk} P(X=k)
$$
For a Geometric distribution with parameter $p$, the probability mass function (PMF) is
$$
P(X=x) = (1-p)^{x-1} p \quad \text
{for } k = 1, 2, 3, \ldots
$$
Substituting the PMF into the MGF definition, we have:
$$
M_X(t) = \sum_{k=1}^{\infty} e^{tk
} (1-p)^{k-1} p
$$
Factoring out the constant $p$, we get:
$$
M_X(t) = p \sum_{k=1}^{\infty} e^{
tk} (1-p)^{k-1}
$$
We can rewrite the sum as:
$$
M_X(t) = p e^t \sum_{k=0}^{\infty
} \left(e^t (1-p)\right)^k
$$
This is a geometric series with the first term $a = 1$ and common ratio 
$r = e^t (1-p)$. The sum of an infinite geometric series is given by:
$$
\sum_{k=0}^{\infty} ar^k = \frac{a}{1-r} \quad \text{for } |r| < 1
$$

Applying this formula, we get:
$$
M_X(t) = p e^t \cdot \frac{1}{1 -
e^t (1-p)} = \frac{pe^t}{1-(1-p)e^t}
$$
The condition for convergence is $|e^t (1-p)| < 1$, which
implies:
$$
e^t < \frac{1}{1-p} \implies t < -\ln(1-p)
$$
Thus, we have shown that the MGF of a Geometric distribution with parameter $p$
is given by:
$$
M_X(t) = \frac{pe^t}{1-(1-p)e^t
} \quad \text{for } t < -\ln(1-p)
$$
This completes the proof.



7. Find the MGF of a Normal distribution with mean $\mu$ and variance $\sigma^2$.
The MGF of a Normal distribution with mean $\mu$ and variance $\sigma^2$
is given by:
$$
M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} f(x) \, dx
$$


where $f(x)$ is the PDF of the Normal distribution:
$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$
Substituting the PDF into the MGF definition, we have:
$$
M_X(t) = \int_{-\infty}^{\infty} e^{tx
} \cdot \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \, dx
$$
Combining the exponentials, we get:
$$
M_X(t) = \frac{1}{\sigma \sqrt{2\pi}}
 \int_{-\infty}^{\infty} e^{tx - \frac{(x-\mu)^2}{2\sigma^2}} \, dx
$$
To simplify the exponent, we complete the square:
$$
tx - \frac{(x-\mu)^2}{2\sigma^2} =
-\frac{1}{2\sigma^2} \left( (x-\mu)^2 - 2\sigma^2 tx \right)
= -\frac{1}{2\sigma^2} \left( x^2
 - 2(\mu + \sigma^2 t)x + \mu^2 \right)
= -\frac{1}{2\sigma^2} \left( (x -
 (\mu + \sigma^2 t))^2 - (\mu + \sigma^2 t)^2 + \mu^2 \right)
= -\frac{(x - (\mu + \sigma^2 t))^2
}{2\sigma^2} + \frac{(\mu + \sigma^2 t)^2 - \mu^2}{2\sigma^2}
$$
Substituting this back into the MGF expression, we have:
$$
M_X(t) = \frac{1}{\sigma \sqrt{2\pi}}
 \int_{-\infty}^{\infty} e^{-\frac{(x - (\mu + \sigma^2 t))^2}{2\sigma^2}} e^{\frac{(\mu + \sigma^2 t)^2 - \mu^2}{2\sigma^2}} \, dx
$$
The term $e^{\frac{(\mu + \sigma^2 t)^2 -
 \mu^2}{2\sigma^2}}$ is a constant with respect to $x$, so we can factor it out of the integral:
$$
M_X(t) = e^{\frac{(\mu + \sigma^2 t)^
2 - \mu^2}{2\sigma^2}} \cdot \frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(x - (\mu + \sigma^2 t))^2}{2\sigma^2}} \, dx
$$
The integral is the integral of a Normal distribution with mean $\mu + \sigma^2 t
$ and variance $\sigma^2$, which equals 1. Therefore, we have:
$$
M_X(t) = e^{\frac{(\mu + \sigma^2 t)^
2 - \mu^2}{2\sigma^2}} = e^{\frac{\mu^2 + 2\mu\sigma^2 t + \sigma^4 t^2 - \mu^2}{2\sigma^2}} = e^{\mu t + \frac{\sigma^2 t^2}{2}}
$$
Thus, the MGF of a Normal distribution with mean $\mu$ and variance $\sigma^
2$ is given by:
$$
M_X(t) = e^{\mu t + \frac{\sigma^2 t^
2}{2}}
$$
This completes the derivation.






