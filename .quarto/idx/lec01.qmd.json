{"title":"Probability Theory for Data Scientists","markdown":{"headingText":"Probability Theory for Data Scientists","containsRefs":false,"markdown":"\n\n\n##  Set theory Concepts\n\n::: {.callout-note icon=\"false\"}\n\n## Definition: Sample Space, Event, and Empty Set\n\n::: {#def-1.1.1}\n\nConsider an uncertain scenario. This include a random experiment, a data-generating process  or simply the future. We define the following concepts:\n\n*   **Sample Space ($\\Omega$)**: The set of all possible outcomes or results from the scenario. Sample spaces can be either countable  or uncountable. If the elements of a sample space can be put into one-to-one correspondence with  the set of integers, the sample space is countable.  If the sample space contains only a finite number of elements, it is also countable. Otherwise is uncountable.\n   \n\n*   **Event**: A subset of the sample space. It represents a specific outcome or a collection of outcomes of interest.\n   \n\n*   **Empty Set ($\\emptyset$)**: A set containing no elements. It represents an impossible event.\n\n:::\n\n:::\n\n\n::: {#exm-1-1.1}\n\nIf we flip a coin twice then the sample space can be written as:\n$$\n\\Omega =\\{HH,HT,TH,TT\\}\n$$\n\nwhere $H$ represents _heads_ and $T$ _tails_. This sample space is finite. An event (say $A$) could be _at least one head appears_, that is\n\n$$A =\\{HT,TH,HH\\}\\subset \\Omega$$\n\n::: \n\n\n::: {#exm-2-1.1}\nIf we are analyzing customer purchase behavior for a single online transaction, the sample space could be the set of all possible combinations of items a customer might select from the store's catalog. This sample space is in principle finite and therefore countable. However, if the catalog is very large, the sample space can be considered uncontably large for practical purposes. More on this later.\n\nAn event could be \"customer buys at least one item from category X\", or \"customer buys product Y\".\n:::\n\n\n::: {#exm-3-1.1}\nWe measure the time (in seconds) it takes for a user to complete a task on a website. \nThe time limit is predefined at 5 minutes. Then the sample space is $\\Omega = \\{0, 1, 2, 3,\\ldots, 300\\}$ which is finite. If, however, we  measure the time with  arbitrary precision, then the sample space is the interval $(0,300)$ of real numbers. This sample space is uncountable. \n\nAn event could be  \"user completes the task in under 2 minutes\".\nIn the former case,  this corresponds to the set $A=\\{1,2\\ldots, 119 \\}$. Int he latter case is the real interval $A=(0, 120)$.\n\n\n:::\n\n\nEvents can be described in many different ways. We will use set theory and notation to describe events and operations on events. This can help later in the computation of probabilities.\n\n::: {.callout-note icon=\"false\"}\n\n## Basic Set Operations\n::: {#def-set-ops}\n\n\nGiven  events $A,B,C$ in the sample space $\\Omega$:\n\n*   **Union ($A \\cup B$)**: The event that $A$ occurs, or $B$ occurs, or both occur.\n\n    \n\n*   **Intersection ($A \\cap B$)**: The event that both $A$ and $B$ occur.\n   \n\n*   **Complement ($A^c$)**: The event that $A$ does not occur. It is the set of all outcomes in $\\Omega$ that are not in $A$.\n   \nThe following propertties hold for any events $A, B, C$:\n\n*   **Commutativity**:\n    *   Union: $A \\cup B = B \\cup A$\n    *   Intersection: $A \\cap B = B \\cap A$\n\n*   **Associativity**:\n    *   Union: $(A \\cup B) \\cup C = A \\cup (B \\cup C)$\n    *   Intersection: $(A \\cap B) \\cap C = A \\cap (B \\cap C)$\n\n*   **Distributive Laws**:\n    *   Intersection over Union: $A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)$\n    *   Union over Intersection: $A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)$\n\n*   **De Morgan's Laws**:\n    *   $(A \\cup B)^c = A^c \\cap B^c$\n    *   $(A \\cap B)^c = A^c \\cup B^c$\n\n\n:::\n:::\n\n::: {.callout-note icon=\"false\"}\n\n## Disjoint Sets and Partitions of Sample Space\n\n::: {#def-disjoint-partition}\n\n\n*   **Disjoint Sets (Mutually Exclusive Events)**: Two sets $A$ and $B$ are disjoint if they have no elements in common, i.e., $A \\cap B = \\emptyset$.\n    \n\n*   **Partition**: A collection of non-empty, disjoint subsets (events) of $\\Omega$ whose union is $\\Omega$. That is $A_1,A_2, \\ldots$ is a partition if\n\n$$\n\\bigcup_{i} A_i = \\Omega \\quad \\text{and} \\quad A_i \\cap A_j = \\emptyset \\text{ for } i \\ne j\n$$\n    \n:::\n:::\n\n::: {.callout-note icon=\"false\"}\n\n## Representation of events using set operations  \n\n::: {#exm-union}\n\nWhen we flip a coin twice, the event  $A$ \"at least one head appears\" can be written in variosu ways. These include\n\n* the union of three events $A = \\{HT\\} \\cup \\{TH\\} \\cup \\{HH\\}$. That is, $A$ occurs if we get heads on the first flip and tails on the second flip, or tails on the first flip and heads on the second flip, or heads on both flips. Note that these three events are disjoint as they do not share any outcomes.\n\n\n* the union $A = A_1 \\cup A_2$ where $A_1 = \\{HT, HH\\}$ is the event \"head on first flip\" and $A_2 = \\{TH, HH\\}$ is the event \"head on second flip\". Note that $A_1$ and $A_2$ are not disjoint as they both contain the outcome $HH$.\n\n* the complement $A = B^c$ where $B = \\{TT\\}$ is the event \"no heads appear\".\n\nThree different partitions of the sample space are given by:\n\n* The trivial partition where each event contains a single outcome:\n$$\n\\mathcal P_1=\\{\\{HT\\},\\{TH\\},\\{HH\\},\\{TT\\}\\}\n$$\n\n\n* The partition: \n$$\n\\mathcal P_{equal}=\\{\\{HH,TT\\}, \\{HT,TH\\}\\}\n$$\nthat is, when we throw the coin twice, either we get the same results in both throws OR  different ones.\n\n\n* The partition where we group the outcomes based on the number of heads:\n\n$$\n\\mathcal P_{heads} =\\{\\{TT\\}, \\{HT,TH\\}, \\{HH\\}\\}\n$$\n\nthat is, when we flip the coin twice, we can get no heads, one head or two heads.\n\n:::\n\n\n:::\n\n\n::: {.callout-note icon=\"false\"}\n\n## Sigma Algebra \n\n\n\n::: {#def-sigma-algebra}\n\n\nA collection $\\mathcal{F}$ of subsets of $\\Omega$ is a **sigma algebra** (or $\\sigma$-algebra) if it satisfies the following properties:\n\n1.  $\\Omega \\in \\mathcal{F}$ (The sample space is in the collection).\n2.  If $A \\in \\mathcal{F}$, then $A^c \\in \\mathcal{F}$ (The collection is closed under complementation).\n3.  If $A_1, A_2, \\dots$ are in $\\mathcal{F}$, then $A_1 \\cup A_2 \\cup \\dots \\cup A_n \\in \\mathcal{F}$ (The collection is closed under arbitray number of  unions).\n\n\n:::\n\n:::\n\n\nNote the definition of sigma-algebra does not explicitly require that the intersection of two sets in $\\mathcal F$ is also in $\\mathcal F$. However, this property follows from the other properties and De Morgan's laws. For example, if $A,B \\in \\mathcal F$, then\n\n$$\nA\\cup B \\in \\mathcal F \\implies (A\\cup B)^c = A^c \\cap B^c \\in \\mathcal F \\implies (A^c \\cap B^c)^c = A \\cup B \\in \\mathcal F\n$$\n\n::: {.callout-note icon=\"false\"}\n\n## Examples of sigma-algebras\n\n::: {#exm-sigma-algebra}\n\nThe trivial sigma algebra is clearly $\\mathcal F_0=\\{\\emptyset, \\Omega\\}$ which does not seem very useful.\n\nThe partition $\\mathcal P_{equal}=\\{\\{HH,TT\\}, \\{HT,TH\\}\\}$ above, is not\na sigma-algebra as it does not contain the empty set. If we  add the empty set, then is still not a sigma algebra as it is not closed under union. The union of the only two elements is $\\Omega$. If we include $\\Omega$ then we have the sigma algebra:\n\n$$\n\\mathcal F_{equal}=\\{\\emptyset, \\Omega, \\{HH,TT\\}, \\{HT,TH\\}\\}\n$$\n\nThe partition $\\mathcal P_{heads}$ above is also not a sigma algebra but if we add all possible unions then we obtain the sigma algebra: \n\n$$\n\\begin{aligned}\n\\mathcal F_{heads}& =\\{\\emptyset, \\Omega, \\{TT\\}, \\{HT,TH\\}, \\{HH\\}, \\{HT,TH,HH\\},\\\\\n & \\{HT,TH,TT\\}, \\{HH,TT\\}\\}\n\\end{aligned}\n$$\n\n\n\nThe set \n$$\n\\mathcal G =\\{\\emptyset,\\Omega,\\{HT\\},\\{TH\\},\\{HH\\},\\{TT\\}\\}\n$$\n\nis neither a  partition  nor a sigma algebra as it is not closed under union. For example, $\\{HT\\}\\cup \\{TH\\}=\\{HT,TH\\}\\notin \\mathcal G$. However, if we add all possible unions of the elements of $\\mathcal G$ we obtain the **power set** of $\\Omega$, that is the set of all subsets of $\\Omega$: \n\n$$\n\\begin{aligned}\n\\mathcal F_{max} &= \\{\\emptyset,  \\{HT\\},\\{TH\\},\\{HH\\},\\{TT\\},\\\\\n&  \\{HT,TH\\}, \\{HT,HH\\}, \\{HT,TT\\}, \\{TH,HH\\}, \\{TH,TT\\}, \\{HH,TT\\}\\\\\n&  \\{HT,TH,HH\\}, \\{HT,TH,TT\\}, \\{HT,HH,TT\\}, \\{TH,HH,TT\\},\\Omega \\}\n\\end{aligned}\n$$\n\n\nThis is the largest possible sigma-algebra for this sample space. It has $2^4=16$ elements since the sample space has 4 elements. In general, if the sample space has $n$ elements, then its power set has $2^n$ elements. \n\nAlso generally, if \nwe have a finite  partition of $\\Omega$ then the collection of all unions of sets in the partition (including the empty set) is a sigma-algebra.\n\nNote that different sigma algebras serve for different purposes. For example, the sigma algebra $\\mathcal F_{equal}$ is useful if we are only interested in whether the two coin flips are the same or different. The sigma algebra $\\mathcal F_{heads}$ is useful if we areinterested in the number of heads.  The power set $\\mathcal F_{max}$ is a sigma algebra that me be more useful if we are interested in all possible events.\n\n:::\n\n:::\n\n\n\n\n##  Probability\n\nWe will start by defining probability in an intuitive way. Later we will give a more formal mathematical definition .\n\n### Types of Probability\n\nThere are several ways to think about probability. These include\n\n*   **Classical Probability:** Assumes  all possible outcomes in a finite sample space are equally likely. That is, for any event $A$ with $n(A)$ outcomes in a sample space $\\Omega$ with $n(\\Omega)$ equally likely outcomes, the probability of $A$ is:\n        $$ P(A) = \\frac{n(A)}{n(\\Omega)} $$\n\n\n    ::: {#exm-classical-prob}        \n    Under this framework, the probability of rolling an even number on a  die is assigned to be $P(\\text{rolling an even number}) = \\frac{3}{6}$. More, generally this is equivalent to say the die is fair. Another example is when we assign the probability of rain tomorrow, locally at 10 AM, to be 1/2 as there are only two possible outcomes: rain or no rain.\n\n    :::\n\n*   **Empirical (or Frequentist) Probability:** Based on observed frequencies from repeated experiments. As the number $N$ of experiment repetitions  increases, the probability of an event $A$ approaches the true probability:\n        $$ P(A) \\approx \\frac{\\text{Number of times A occurred}}{N} $$\n\n    ::: {#exm-frequentist-prob}     \n    If we do not what the probability of heads when flipping a coin is. We can  we  flip the coin 1000 times and if it lands heads 537 times, we would say the empirical probability of heads is $0.537$. Furthermore we might say that the true probability of heads is $\\approx 0.537$ and the important aspect of thios framework is that, in theory, the more times we flip the coin the closer the empirical proportion will be to the true probability. Finally,  according to historical data for our location, it has rained 33.6% of the days out of the last 10 years. The empirical probability of rain tomorrow locally at 10 AM is 0.336. \n    :::\n\n*   **Subjective Probability:** Based on personal belief or judgment, often used when objective data is scarce.\n\n    :::{#exm-subjective-prob}\n\n    I had a look through the window and is a bit overcast, then I believe the probability of rain tomorrow locally at 10 AM is 0.7. On the other hand, if I am a weather expert from the point of atmospheric physics, I might believe the probability of rain tomorrow locally at 10 AM is 0.9. \n\n    :::\n\n\n### Formal definition of probability\n\nAfter we have chosen a sigma algebra $\\mathcal F$ that contains events we are interested in, we can define probabilities for all the  events  in a more formal way. \n\n::: {.callout-note icon=\"false\"}\n\n## Probability Measure (Kolmogorov's Axioms)\n::: {#def-probability-measure}\n\n\n\nA **probability measure** $P$ on a sample space $\\Omega$ with a $\\sigma$-algebra $\\mathcal{F}$ is a function $P: \\mathcal{F} \\to [0, 1]$ that assigns a probability to each event in $\\mathcal{F}$ and satisfies the following three axioms:\n\n1.  **Non-negativity**: For any event $A \\in \\mathcal{F}$, $P(A) \\ge 0$.\n    The probability of any event is non-negative.\n\n2.  **Normalization**: $P(\\Omega) = 1$.\n    The probability of the entire sample space (the certain event) is 1.\n\n3.  **Additivity (for disjoint events)**: If $A_1, A_2, \\dots, A_n$ are disjoint events in $\\mathcal{F}$ (i.e., $A_i \\cap A_j = \\emptyset$ for $i \\ne j$), then\n    $$ P\\left(\\bigcup_{i=1}^n A_i\\right) = \\sum_{i=1}^n P(A_i) $$\n    For a countably infinite sequence of disjoint events, this extends to:\n    $$ P\\left(\\bigcup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty P(A_i) $$\n    The probability of the union of disjoint events is the sum of their individual probabilities.\n:::\n\n:::\n\n:::{.callout-note icon=\"false\"}\n\n## Probability measure for the equality of two coin flips\n\n::: {#exm-probability-measure-equal}\n\nFor the sigma algebra $\\mathcal F_{equal}=\\{\\emptyset, \\Omega, \\{HH,TT\\}, \\{HT,TH\\}\\}$ we can define a probability measure simply by specifying:\n\n* $P(\\emptyset) = 0$\n* $P(\\{HH,TT\\}) = 0.4$\n\n\nNote we can compute the probability of the other two events in $\\mathcal F_{equal}$ using the axioms:\n\n* $P(\\Omega) = 1$ (by axiom 2)\n\n* $P(\\{HT,TH\\}) = P(\\Omega) - P(\\{HH,TT\\}) = 1 - 0.4 = 0.6$ (by axiom 3)\n\nThe assignment of probability of $\\{HH,TT\\}$ to be $0.4$ maybe frequentist or subjective but regardless of this, it generates is a valid probability measure as it satisfies all three axioms.\n\n:::\n\n:::\n\n\n:::{.callout-note icon=\"false\"}\n\n## Probability measure for the number of heads in two coin flips\n\n::: {#exm-probability-measure-heads}\n\nFor the sigma algebra $\\mathcal F_{heads}$ we can define a probability measure simply by specifying:\n\n\n* $P(\\{HT,TH\\}) = 0.5$\n* $P(\\{TT\\}) = 0.1$\n\n\n\n\nThe probabilities for the rest of the event in $\\mathcal F_{heads}$ can be computed using  axiom 3 as follows:\n\n* $P(\\{HH\\}) = 1-0.1-0.5 = 0.4$ \n* $P(\\{HT,TH,HH\\}) = P(\\{HT,TH\\}) + P(\\{HH\\}) = 0.5 + 0.4 = 0.9$\n* $P(\\{HT,TH,TT\\}) = P(\\{HT,TH\\}) + P(\\{TT\\})= 0.5 + 0.1 =0.6$\n* $P(\\{HH,TT\\}) = 0.1 +0.4 = 0.5$\n* $P(\\Omega) = 1$ (Trivial but good to double check in practice)\n* $P_{heads}(\\emptyset) = 1-1=0$ (Trivial, always true)\n\n\n\nAs before the probability assignment maybe frequentist or subjective but regardless of this, it generates is a valid probability measure as it satisfies all three axioms.\n:::\n\n:::\n\n\n\n:::{.callout-note icon=\"false\"}\n\n## Probability measure for power set \n\n::: {#exm-probability-measure-power-set}\n\nFor the largest sigma algebra $\\mathcal F_{max}$ we can define a probability measure simply by specifying probabilities for the four singletons or atoms:\n\n\n* $P(\\{HH\\}) = 0.3$\n* $P(\\{HT\\}) = 0.2$\n* $P(\\{TH\\}) = 0.4$\n\n\n\n\nThe probabilities for the rest of the events in $\\mathcal F_{max}$ can be computed using  the axioms as follows:\n\n* $P(\\{TT\\}) = 1-0.3-0.2-0.4 = 0.1$\n* $P(\\{HT,TH\\}) = 0.2 + 0.4 = 0.6$\n* $P(\\{HT,HH\\}) = 0.2 + 0.3 = 0.5$\n* $P(\\{HT,TT\\}) = 0.2 + 0.1 = 0.3$\n* $P(\\{TH,HH\\}) = 0.4 + 0.3 = 0.7$\n* $P(\\{TH,TT\\}) = 0.4 + 0.1 = 0.5$\n* $P(\\{HH,TT\\}) = 0.3 + 0.1 = 0.4$\n* $P(\\{HT,TH,HH\\}) = 0.2 + 0.4 + 0.3 = 0.9$\n* $P(\\{HT,TH,TT\\}) = 0.2 + 0.4 + 0.1 = 0.7$\n* $P(\\{HT,HH,TT\\}) = 0.2 + 0.3 + 0.1 = 0.6$\n* $P(\\{TH,HH,TT\\}) = 0.4 + 0.3 + 0.1 = 0.8$\n\n\n\n\nAs before the probability assignment maybe frequentist or subjective but regardless of this, it generates is a valid probability measure as it satisfies all three axioms.\n:::\n\n:::\n\n\n:::{.callout-note icon=\"false\"} \n\n## Simple Probability Operations\n\n::: {#prp-probability-operations}\n\nFrom the axioms, we can derive several useful properties:\n\n*   **Probability of the Complement**: For any event $A \\in \\mathcal{F}$,\n    $$ P(A^c) = 1 - P(A) $$\n  \n\n*   **Probability of the empty set**: $P(\\emptyset) = 0$.\n  \n  \n*    **Probability of the Union of Two Events (General)**: For any two events $A, B \\in \\mathcal{F}$:\n    $$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) $$\n    This is known as the addition rule. It accounts for the overlap between events.\n   \n   \n    \n:::\n:::\n\n\n\n:::{.callout-note icon=\"false\"}\n\n##  Probability of the union  \n:::{#exm-probability-operations}\n\n$$\n\\begin{aligned}\nP(\\{HT,TH,TT\\}\\cup\\{HH,TT\\})&=P(\\{HT,TH,TT\\})+P(\\{HH,TT\\})-P(\\{TT\\})\\\\\n& =  0.6+0.5-0.1\\\\\n& =1      \n\\end{aligned}\n$$\n\nclearly this is correct as the union of these two events is $\\Omega$.\n\n\n:::\n:::\n\n\n:::{.callout-note icon=\"false\"}\n\n## Boole and Bonferroni inequalities\n\n:::{#thm-boole-bonferroni}\n\n*  **Boole's inequality** For any  events $A_1, A_2, \\ldots, A_n$ in $\\mathcal F$:\n    $$ P\\left(\\bigcup_{i=1}^n A_i\\right) \\le \\sum_{i=1}^n P(A_i) $$\n    This inequality provides an upper bound for the probability of the union of events.\n    \n   **Bonferroni Inequality**: For any events $A_1, A_2, \\ldots, A_n$ in $\\mathcal F$:\n    $$ P\\left(\\bigcap_{i=1}^n A_i\\right) \\ge 1 - \\sum_{i=1}^n P(A_i^c) $$\n    This inequality provides a lower bound for the probability of the intersection of events.\n\n:::\n\n:::\n\n\nThese inequalities, specially Bonferroni's will be useful later. Booles inequality can be proved by induction and Bonferroni's inequality follows from Booles inequality and the properties of complements. These facts can be verified by the reader.\n\n\n## Conditional Probability \n\n:::{.callout-note icon=\"false\"}\n\n#### Conditional Probability\n\n::: {#def-cond-probability}\n\n\n\nThe **conditional probability** of event $A$ given that event $B$ has occurred, denoted $P(A|B)$, is defined as:\n$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\nprovided that $P(B) > 0$. This measures the probability of event $A$ occurring, knowing that event $B$ has already happened.\n\n:::\n:::\n\n\n:::{.callout-note icon=\"false\"}\n\n## Example of Conditional Probability\n\n::: {#exm-cond-probability}\n\nWhat is the probability of getting heads on the first coin flip GIVEN that at least one head appears in two flips? This can be expressed as $P(A|B)$ where $A=\\{HT,HH\\}$ is \"head on first flip\" and $B=\\{HT,TH,HH\\}$ is \"at least one head appears\". We have:\n\n$$\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} =\\frac{P(A)}{P(B)}= \\frac{P(\\{HT,HH\\})}{P(\\{HT,TH,HH\\})} \n$$\n\nsince $A\\subset B$ in  this case. We  notice a subtlety here. The event $A =\\{HT,HH\\}$ (head on the first flip) is not a member of the sigma-algebra $\\mathcal F_{heads}$. So cannot use the probability measure $P_{heads}$ to compute this conditional probability.  However, it is a member of the sigma algebra (the power set) $\\mathcal F_{max}$ so we might need to define probabilities using such sigma algebra as follows:\n\n$$\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(\\{HT,HH\\})}{P(\\{HT,TH,HH\\})} = \n \\frac{0.5}{0.9} \\approx 0.556\n$$\n\n\n\n<!-- Note that $A \\cap B = A$ as $A \\subset B$ \nTHIS IS AN EXAMPLE OF THE HALLUCINATION PROBLEM\n\n$$\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(\\{HT,HH\\})}{P(\\{HT,TH,HH\\})} = \\frac{0.5}{0.9} \\approx 0.556\n$$\n\nas soon as i finish writing P(A|B) the auto complete kicks in and writes the rest of the equation. which is obviously wrong as is taking P(A int B) to be 0.5 from P(HH,TT)  \n-->\n\nOn a more practical situation, if $A$ is \"a user makes a purchase\" and $B$ is \"a user clicks on an advertisement\", then $P(A|B)$ is the probability that a user makes a purchase GIVEN that they clicked on the advertisement. This is a key metric for evaluating ad campaign effectiveness.\n:::\n:::\n\n\nTwo very useful  consequences of the above are:   the law of total probability that combines the notion of partition with that of  conditional probability  and Bayes rule that allows us to reverse conditional probabilities.\n\n\n:::{.callout-note icon=\"false\"}\n\n#### Law of Total Probability\n\n::: {#prp-total-probability}\n\n\nLet $B_1, B_2, \\dots, B_n$ be a partition of the sample space $\\Omega$ (i.e., they are disjoint and their union is $\\Omega$). Then for any event $A \\in \\mathcal{F}$:\n$$ P(A) = \\sum_{i=1}^n P(A|B_i) P(B_i) $$\n\n:::\n:::\n\n\n::: {.callout-note icon=\"false\"}\n\n## Bayes' Rule\n\n::: {#prp-bayes-rule}\n\n\nFor events $A$ and $B$ where $P(B) > 0$:\n$$ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $$\nIf $B_1, \\dots, B_n$ form a partition of $\\Omega$, and $P(B_i) > 0$ for all $i$, then Bayes' Rule can be written using the Law of Total Probability for the denominator:\n$$ P(A|B) = \\frac{P(B|A) P(A)}{\\sum_{i=1}^n P(B|B_i) P(B_i)} $$\n\n\n\n\n:::\n:::\n\n\nThe proof of this result is staightforward and left to the reader. \n\n:::{.callout-note icon=\"false\"}\n\n## Example of Law of Total Probability and Bayes' Rule\n\n:::{#exm-total-probability-bayes}\n\n###  Medical Testing\n\nSuppose a rare disease affects 1 in 10,000 people. A test for this disease is 99% accurate:\n\n*   If a person has the disease, the test correctly identifies it 99% of the time (True Positive).\n*   If a person does not have the disease, the test correctly identifies it 99% of the time (True Negative).\n\nLet $D$ be the event that a person has the disease, and $T^+$ be the event that the test is positive. The probabilities we know are:\n\n*   $P(D) = \\frac{1}{10000} = 0.0001$ (Prevalence)\n*   $P(T^+|D) = 0.99$ (Sensitivity - True Positive Rate)\n*   $P(T^-|D^c) = 0.99$ (Specificity - True Negative Rate)\n\nBefore we proceed we note the probability specifications above are emprical. \n\nSuppose we want to find $P(D|T^+)$, the probability that a person actually has the disease given a positive test result.\n\nFirst, we need $P(T^+)$. A positive test can occur in two ways: \n\n*  ($D \\cap T^+$) or \n\n*  ($D^c \\cap T^+$)\n\ne.g. a partition of $A$. We also have:\n\n*   $P(T^+|D^c) = 1 - P(T^-|D^c) = 1 - 0.99 = 0.01$ (False Positive Rate)\n*   $P(D^c) = 1 - P(D) = 1 - 0.0001 = 0.9999$\n\nUsing the law of total probability:\n\n$$\n\\begin{aligned}\nP(T^+) &=P(T^+ \\cap D)+P(T^+\\cap D^c)\\\\\n& =P(T^+|D)P(D) + P(T^+|D^c)P(D^c)\\\\\n&= (0.99)(0.0001) + (0.01)(0.9999)\\\\\n&= 0.000099 + 0.009999 = 0.010098\n\\end{aligned}\n$$\n\nNow, using Bayes' Theorem:\n$$\n\\begin{aligned}\nP(D|T^+) &= \\frac{P(T^+|D) P(D)}{P(T^+)} \\\\\n&=   \\frac{(0.99)(0.0001)}{0.010098} \\approx 0.0098\n\\end{aligned}\n$$\n\nThis may look counter-intuitive. Even with a positive test, there's only about a 0.98% (less than 1%) chance the person actually has the disease! In particular it is a rare disease. This highlights the importance of understanding base rates and conditional probabilities in interpreting results.\n\n\n:::\n\n:::\n\n\nThe Law of Total probability allows us to calculate the probability of an event $A$ by considering the different ways it can occur through the events in a partition.\n\n\n:::{.callout-note icon=\"false\"}\n\n## Customer churn  \n\n:::{#exm-total-probability-data-science}\n\nSuppose we have three models, M1, M2, and M3, that are used to predict customer churn. Let $P(M1)=0.5$, $P(M2)=0.3$, $P(M3)=0.2$ be the probabilities that each model is the \"best\" for a given customer. Let $A$ be the event \"customer churns\". If we know the probability of churn given each best model (e.g., $P(A|M1)=0.1$, $P(A|M2)=0.2$, $P(A|M3)=0.15$), the Law of Total Probability allows us to find the overall probability of churn: \n\n$$\n\\begin{aligned}\nP(A) &= P(A|M1)P(M1) + P(A|M2)P(M2) + P(A|M3)P(M3) \\\\\n&= (0.1)(0.5) + (0.2)(0.3) + (0.15)(0.2)\\\\\n& = 0.05 + 0.06 + 0.03 = 0.14\n\\end{aligned}\n$$\n\n:::\n\n:::\n\n\n## Independence\n\n### Independence of events\n\n\n\nFirst intuitively, two events, $A$ and $B$, are considered **independent** if the occurrence of one event does not affect the probability of the other event occurring. Formally, \n\n::: {.callout-note icon=\"false\"}\n\n## Independent Events\n\n:::{#def-independent-events}\n\nTvents $A$ and $B$ are independent if:\n$$ P(A \\cap B) = P(A) \\times P(B) $$\nor equivalently, if either of the following conditions hold:\n\n* $P(A|B) = P(A)$\n\n* $P(B|A) = P(B)$\n\n\n:::\n\n:::\n\n\n\nThis means that knowing that event $B$ has occurred gives us no new information about the probability of event $A$ occurring, and vice versa.\n\n\n\n:::{.callout-note icon=\"false\"}\n\n## Independent event when flipping a coin twice\n\n:::{exm-independent-events}\n\nConsider the following events when flipping a coin twice:\n\n*  $A=\\{HT, HH\\}$  the first flip is heads\n*  $B=\\{TT, HH\\}$ the two flips are the same\n\nThen using the probabilities in @exm-probability-measure-power-set  we have:\n\n$$\nP(A\\cap B) = P(\\{HH\\}) = 0.3 \\neq P(\\{HT,HH\\})P(\\{TT,HH\\}) = 0.5\\times 0.4  = 0.2\n$$\n\nTherefore these two events are not independent. Of course, the assignment of probabilities here play a role. In this way, if we had assigned $P(\\{HH\\})=0.2$ then the events would have been independent.\n\n:::\n\n:::\n\n\nAn obvious consequence of @def-cond-probability of conditional probability is the so-called multiplication rule.\n\n:::{.callout-note icon=\"false\"}\n\n## Multiplication Rule\n\n::: {#prp-multiplication-rule}\n\nFor any two events $A$ and $B$ \n$$ P(A \\cap B) = P(A) \\times P(B|A) = P(B) \\times P(A|B) $$\n\n:::\n\n:::\n\n\n:::{.callout-note icon=\"false\"}\n\n## Drawing Cards without Replacement\n\n:::{#exm-drawing-cards-dependence}\n\nImagine drawing two cards from a standard deck without replacement.\nLet $A$ be the event that the first card is a Heart. $P(A) = \\frac{13}{52}$.\nLet $B$ be the event that the second card is a Heart.\nSince the first card is not replaced, these events are dependent. The probability of the second card being a Heart *depends* on the first card drawn.\n$P(B|A)$ (the probability the second card is a Heart, given the first was a Heart) is $\\frac{12}{51}$ (as there are 12 Hearts left and 51 total cards).\nSo, the probability of drawing two Hearts in a row is $P(A \\cap B) = P(A) \\times P(B|A) = \\frac{13}{52} \\times \\frac{12}{51}$.\n\n:::\n:::\n\n:::{.callout-note}\n\nThe outcome of flipping a coin maybe independent of the outcome of any previous coin flips. If you flip a coin and get heads, the probability of getting heads on the next flip should remain as before. Of course, this a simplifying assumption that may not hold in practice. In this course we will make these kind of assumption specially when it involves sequences of events. Not assuming independence for sequences of events may things more complicated for what we want to achieve in this course.\n\n\n:::\n\n\n## Random Variables\n\nSo far we have talked about events, which are subsets of the sample space. In many applications, especially in data science, we are interested in quantifying outcomes numerically. This is where random variables come into play.\n\n\n:::{.callout-note icon=\"false\"}\n##  Random Variable\n::: {#def-random-variable}\n\n\nA **random variable** $X$ is a function that maps outcomes from the sample space $\\Omega$ to real numbers. That is, $X: \\Omega \\to \\mathbb{R}$. It quantifies the outcomes of a random phenomenon numerically.\n\n:::\n:::\n\n:::{.callout-note icon=\"false\"}\n##  Random variable examples\n::: {#exm-random-variable}\n\nIf $\\Omega$ is the set of all possible customer orders, a random variable $X$ could be \"the total dollar amount spent in an order\". For each order (an outcome in $\\Omega$), $X$ assigns a specific monetary value. As another example: for a user's session on a website, $X$ could be \"the number of pages visited\" or the \"overall time spent in the website\".\n:::\n:::\n\nNote a random variable evaluates from the outcomes in the sample space rather than the events defined in the sigma algebra. However, events can be defined in terms of random variables. For example, the event \"the total amount spent in an order is greater than 50 dollars\" can be expressed as $\\{X > 50\\}$.\n\n\n\n:::{.callout-note icon=\"false\"}\n##  Random variable: Number of equal coin flips\n::: {#exm-random-variable-heads}\n\nWhen we flip a coin twice, the sample space is $\\Omega = \\{TT, HT, TH, HH\\}$. We can define a very simple random variable $X$ as the \"number of times the flips are the same\". The mapping would be:\n\n* $X(\\{TT\\}) = 1$ (both flips are the same)\n* $X(\\{HT\\}) = 0$ (flips are different)\n* $X(\\{TH\\}) = 0$ (flips are different)\n* $X(\\{HH\\}) = 1$ (both flips are the same)\n\nThe possible values of $X$ are $\\{0, 1\\}$.  \n\n:::\n\n:::\n\n\n\n\n:::{.callout-note icon=\"false\"}\n##  Random variable:  Number of heads in two coin flips \n::: {#exm-random-variable-heads}\n\nWhen we flip a coin twice, the sample space is $\\Omega = \\{TT, HT, TH, HH\\}$. We can define a random variable $X$ as the \"number of heads\" in the two flips. The mapping would be:\n\n* $X(\\{TT\\}) = 0$ (no heads)\n* $X(\\{HT\\}) = 1$ (one head)\n* $X(\\{TH\\}) = 1$ (one head)\n* $X(\\{HH\\}) = 2$ (two heads)\n\nThe possible values of $X$ are $\\{0, 1, 2\\}$. This random variable quantifies the outcome of the coin flips in terms of the number of heads observed. Also note the order in which the heads appear does not matter for this random variable.\n\n:::\n\n:::\n\n\n\nThe above two random variables are discrete random variables as they take on a finite or countable number of values, that is $X(\\Omega)$ is finite or countable. There are also continuous random variables that can take on any value in a continuous range. \n\n\n:::{.callout-note icon=\"false\"}\n\n\n## Continuous vs Discrete Random Variables\n\n::: {#exm-time-website-random-variable}\n\nGoing back to @exm-3-1.1 we have already defined a random variable $X$ as the time to complete a task in a website with a  limit of 5 minutes. \nIf we round to the nearest second, then the possible values of $X$ are $\\{0,1, 2, 3, \\ldots, 300\\}$ and $X$ is a discrete random variable. However, if we do not round then $X$ can take any value in the interval $(0,300)$ and $X$ is a continuous random variable.\n\n:::\n:::\n\nThe definition of continuous random variables requires a bit more than simply having an uncountably infinite image set $X(\\Omega)$ . The definition is  a bit  thechnical as it involves the notion of probability density function.\n\n:::{.callout-note icon=\"false\"}\n\n## Discrete and continuous random variables\n\n\n:::{#def-discrete-continuous-random-variable} \n\nWe say a random variable $X$ is\n\n* **discrete** if it takes on a finite or countably infinite number of distinct values. That is if the image set $X(\\Omega)$ is either finite or countably infinite.\n\nThe function: \n$$\nf_X(x) = P(X=x):=P(\\{\\omega\\,:\\,X(\\omega)=x\\})\\quad  \\mbox{for } x\\in X(\\Omega)\n$$ \n\nis called the **probability mass function (PMF)** of the discrete random variable $X$. \nThe PMF satisfies:\n\n*   $f_X(x) \\ge 0$ for all $x \\in X(\\Omega)$.\n*   $\\sum_{x \\in X(\\Omega)} f_X(x) = 1$\n\n\n*  **continuous** if there exists a function $f_X(x)$ such that for any two numbers $a$ and $b$ with $a < b$:\n$$ P(a \\le X \\le b):=P(\\{\\omega\\,:\\, a \\leq X(\\omega)\\leq b\\}) = \\int_a^b f_X(x) dx $$\nwhere \n\n* $f_X(x) \\ge 0$ for all $x$ and \n\n* $\\int_{-\\infty}^{\\infty} f_X(x) dx = 1$.\n\nThe function $f_X(x)$ is called the **probability density function (PDF)** of the random variable $X$.\n:::\n\n:::\n\nThe idea is that there are no \"gaps\", which would correspond to real numbers which have a finite probability of occurring. Instead, continuous random variables never take an exact prescribed value, that is $P(X=x)=0$ for all $x$ but there is a positive probability that its value will lie in particular intervals which can be arbitrarily small.  \n\n\n:::{.callout-note icon=\"false\"}\n\n\n\n##  Cumulative Distribution Function (CDF)\n\n:::{#def-cdf}\n\nThe **cumulative distribution function (CDF)** of a random variable $X$, denoted by $F_X(x)$, is the function \n$F: \\mathcal R \\to [0,1]$\ndefined by\n$$\nF_X(x) = P(X \\le x):=P(\\{\\omega\\,:\\,X(\\omega)\\leq x\\})\n$$\n\nfor any real number $x$. The CDF gives the probability that the random variable $X$ takes on a value less than or equal to $x$.\n\n:::\n:::\n\n\nWe note that\n\n$$\n\\begin{aligned}\nP(a \\leq X <b ) &= F_X(b) - F_X(a)\\\\\n\\rule{0in}{4ex}    &= \\int_a^b f_X(x) dx  \n\\end{aligned}\n$$\n\nso that\n\n$$\nf_X(x) = \\frac{d}{dx} F_X(x)\\qquad \\forall x \\in \\mathbb R\n$$\n\n\n:::{.callout-note icon=\"false\"}\n\n## CDF of a discrete random variable\n\n::: {#exm-cdf-discrete}\n\nConsider a discrete random variable $X$ with possible values in the set $\\{0, 1, 2, 3\\}$. \nAssume we probability mass function (pmf) is given by:\n$$\nf_X(x) = P(X=x) =\n\\begin{cases}\n0.1 & \\text{if } x = 0 \\\\\n0.3 & \\text{if } x = 1 \\\\\n0.4 & \\text{if } x = 2 \\\\\n0.2 & \\text{if } x = 3 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nthen the CDF  is given by\n\n$$\nF_X(x) = P(X \\le x) = \n\\begin{cases}\n0 & \\text{if } x < 0 \\\\\n0.1 & \\text{if } 0 \\le x < 1 \\\\\n0.4 & \\text{if } 1 \\le x < 2 \\\\\n0.8 & \\text{if } 2 \\le x < 3 \\\\\n1.0 & \\text{if } x \\ge 3\n\\end{cases}\n$$\n\n:::\n:::\n\n\n\n::: {.callout-note icon=\"false\"}\n## Properties of Cumulative Distribution Functions \n\n::: {#properties-cdf}\n\n\nFor any random variable $X$, its CDF $F_X(x)$ has the following properties:\n\n1.  **Monotonicity**: $F_X(x)$ is non-decreasing. For any $x_1 < x_2$, $F_X(x_1) \\le F_X(x_2)$.\n2.  **Limits**: $\\lim_{x \\to -\\infty} F_X(x) = 0$ and $\\lim_{x \\to \\infty} F_X(x) = 1$.\n3.  **Right-continuity**: $F_X(x)$ is right-continuous, meaning $\\lim_{h \\to 0^+} F_X(x+h) = F_X(x)$ for all $x$.\n\n:::\n:::\n\nThe properties above hold for both discrete and continuous random variables. For discrete random variables, the CDF is a step function (continuous from the right), while for continuous random variables, the CDF is a continuous function.\n\n:::{.callout-note icon=\"false\"}\n\n## CDF of a continuous random variable\n\n::: {#exm-cdf-continuous}\n\nConsider a random variable $X$ representing the time (in hours) a server remains operational before crashing. $X$ can take any non-negative real value. Assume the probability density function (pdf) is given by:\n$$\nf_X(x)=\n\\begin{cases}\n\\frac{1}{100} e^{-x/100} & \\text{if } x \\ge 0 \\\\\n\\rule{0in}{3ex}0 & \\text{if } x < 0\n\\end{cases}\n$$\n\nThis probability distribution is called an **exponential distribution with a mean of 100 hours**. We will define and talk about mean later. The CDF is computed as follows:\n\n$$\nF_X(x) = P(X \\le x) = \\int_{-\\infty}^x f_X(t) dt =\n\\begin{cases}\n0 & \\text{if } x < 0 \\\\\n1 - e^{-x/100} & \\text{if } x \\ge 0\n\\end{cases}\n$$\n\nThe CDF $F_Y(y) = P(Y \\le y)$ would give the probability that the server operates for at most $y$ hours. For instance, $F_Y(10)$ would be the probability the server fails within the first 10 hours.\n\n:::\n:::\n\n\n\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["mathjax_header.html"],"toc":true,"toc-depth":3,"embed-resources":true,"output-file":"lec01.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","bibliography":["book.bib","packages.bib"],"theme":{"light":"cosmo","dark":"darkly"},"cover-image":"contractions.jpg","code-copy":true,"code-block-bg":true,"code-summary":"R Code"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":["preamble.tex"],"output-file":"lec01.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"bibliography":["book.bib","packages.bib"],"documentclass":"scrbook","citation_package":"natbib","code-block-bg":true,"urlcolor":"blue","colorlinks":true},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}