{
  "hash": "deb91d6ae35691fc01afb86e884476fa",
  "result": {
    "engine": "knitr",
    "markdown": "# Optimisation and differentiation in R {#sec-optimisation-in-R}\n\n\n## Default optimisation in R\n\nNowadays, there many R packages for optimisation. For a general overview\nsee <https://cran.r-project.org/web/views/Optimization.html>). In this\nsection we will focus on the default optimisation module. The R function\n`optim` performs general purpose unconstrained minimisation. The main\narguments for `optim` are:\n\n-   `par`: vector of initial values for the parameters\n\n-   `fn`: A function to be minimised, with first argument the vector of\n    parameters over which minimisation is to take place\n\n-   `gr`: A function that returns the gradient of the objective\n    function. Only used for BFGS and finite differences are used if the\n    gradient is not specified\n\n-   `method`: The quasi-Newton BFGS algorithm is implemented by\n    specifying `method=\"BFGS\"` and the polytope method of Nelder and\n    Mead (default) with \\`method=\"Nelder-Mead\". For BFGS, the\n    step-length used for the line search is slightly more sophisticated\n    than the one we used (modified backtracking) and will generally give\n    a different sequence of iterations.\n\n-   `hessian`: Logical. `hessian=TRUE` returns an approximate hessian at\n    the found minimum using finite differences. The function that\n    performs this approximation is `optimHess` and can be called\n    separately. Type `?optimHess` for details. We note that is usually\n    better to use finite differences instead of the approximate inverse\n    hessian $\\bm{B}_k$ at the last iteration. BFGS can converge\n    even when $\\bm{B}_k$ is a poor approximation to the inverse\n    Hessian and therefore $\\bm{B}_k$ may be a poor\n    representation of the shape of the objective function in directions\n    which the BFGS iteration has not explored recently.\n\n-   `control`: A list of control parameters. By default, does not report\n    the iterations. For example `control=list(trace=1,REPORT=1)` makes\n    the oputput to report the progress of the optimisation at every\n    iteration while `control=list(trace=1,REPORT=10)` will report every\n    10 iterations. Explore the help file for `optim` for more details\n    about `control` parameters.\n\n-   `...` Further arguments to be passed to `fn` and `gr`. For us these\n    will commonly be the vector sample values and a matrix of covariate\n    values.\n\nThe output of `optim` is an R list with main components:\n\n-   `par`: The best set of parameters found\n\n-   `value`: The value of the objective function `fn` corresponding to\n    `par`\n\n-   `counts`: A two-element integer vector giving the number of calls to\n    `fn` and `gr` respectively. This excludes those calls needed to\n    compute the Hessian, if requested, and any calls to `fn` to compute\n    a finite-difference approximation to the gradient.\n\n-   `convergence`: An integer code. `0` indicates successful completion\n    of the algorithm. See `?optim` for more details.\n\n-   `hessian`: If `hessian=TRUE`, a symmetric matrix giving the finite\n    difference approximation to the Hessian at the solution found\n\n\n## Automatic differentiation in R {#sec-automatic-differentiation}\n\nUsing Newton's method requires an R function to evaluate the objective\nfunction $l(\\bm{\\theta})$, its gradient vector and its Hessian\nmatrix, for any supplied value of $\\bm{\\theta}$. We can obtain\nthese by hand and then code up but it might be easier to automate simple\ndifferentiation using a couple of R functions. Function\n[`D`](https://rdrr.io/r/stats/deriv.html) differentiates symbolically\n(i.e. it produces a symbolic expression representing the derivative of\nits first argument). For example, try the following out in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD(expression(theta1*sin(theta2)),\"theta2\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntheta1 * cos(theta2)\n```\n\n\n:::\n\n```{.r .cell-code}\nD(D(expression(theta1*sin(theta2)),\"theta2\"),\"theta1\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncos(theta2)\n```\n\n\n:::\n:::\n\n\nAlso take a look at `?D`. Differentiating twice a vector function by\nhand can be tedious and can lead to clerical errors in the coding. The R\nfunction [`deriv`](https://rdrr.io/r/stats/deriv.html) performs the task\nof [automatic\ndifferentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n. It produces a piece of code which, when run, will evaluate `deriv`'s\nfirst argument, alongside the first and second derivatives of that\nargument (although it will not necessarily produce an expression\ndirectly recognisable as the required derivative). Take a look at\n`?deriv` and then try out\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo <- deriv(expr         = expression(theta1*sin(theta2)), \n             namevec      = c(\"theta1\",\"theta2\"),\n             function.arg = c(\"theta1\",\"theta2\"), \n             hessian      = TRUE)\n```\n:::\n\n\nWe first confirm that the output of `deriv` is a function, in this case\ncalled [`foo`](https://en.wikipedia.org/wiki/Foobar)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (theta1, theta2) \n{\n    .expr1 <- sin(theta2)\n    .expr2 <- theta1 * .expr1\n    .expr3 <- cos(theta2)\n    .value <- .expr2\n    .grad <- array(0, c(length(.value), 2L), list(NULL, c(\"theta1\", \n        \"theta2\")))\n    .hessian <- array(0, c(length(.value), 2L, 2L), list(NULL, \n        c(\"theta1\", \"theta2\"), c(\"theta1\", \"theta2\")))\n    .grad[, \"theta1\"] <- .expr1\n    .hessian[, \"theta1\", \"theta1\"] <- 0\n    .hessian[, \"theta1\", \"theta2\"] <- .hessian[, \"theta2\", \"theta1\"] <- .expr3\n    .grad[, \"theta2\"] <- theta1 * .expr3\n    .hessian[, \"theta2\", \"theta2\"] <- -.expr2\n    attr(.value, \"gradient\") <- .grad\n    attr(.value, \"hessian\") <- .hessian\n    .value\n}\n```\n\n\n:::\n:::\n\n\nFor example if we evaluate at $(\\theta_1,\\theta_2)=(1,2)$ we obtain\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfoo(theta1 = 1,\n    theta2 = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9092974\nattr(,\"gradient\")\n        theta1     theta2\n[1,] 0.9092974 -0.4161468\nattr(,\"hessian\")\n, , theta1\n\n     theta1     theta2\n[1,]      0 -0.4161468\n\n, , theta2\n\n         theta1     theta2\n[1,] -0.4161468 -0.9092974\n```\n\n\n:::\n:::\n\n\nThe value of the function at $(1,2)$, i.e.\n0.9092974 is what is returned by\n`foo(1,2)`. The gradient and Hessian were returned as *attributes* (an\nattribute being any piece of information `stuck on' to an`R\\` object,\nand carried around with it). It is easy to extract these, e.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbar <- foo(1,2)\nf   <- as.numeric(bar) # `as.numeric`  strips off the attributes \nf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9092974\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- attr(bar,\"gradient\")[1,] # `attr` extracts the attributes of an R object\ng\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    theta1     theta2 \n 0.9092974 -0.4161468 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nH <- attr(bar,\"hessian\")[1,,]\nH\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           theta1     theta2\ntheta1  0.0000000 -0.4161468\ntheta2 -0.4161468 -0.9092974\n```\n\n\n:::\n:::\n\n\nNote we extract the required information by using `[1,]` and `[1,,]`.\nThis is because `attr(bar,\"gradient\")` and `attr(bar,\"hessian\")` are\narrays of the following dimensions\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(attr(bar,\"gradient\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(attr(bar,\"hessian\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2 2\n```\n\n\n:::\n:::\n\n\nIn both cases the first dimension is one so is superfluous . Using\n`[1,]` and `[1,,]` extracts only the relevant part which is a vector of\ndimension 2 for the gradient and a matrix of dimension $2 \\times 2$ for\nthe Hessian.\n\n<!--## A quick cautionary note\n\nRemember that in R, vectors are treated differently than matrices. Vectors are just vectors with no particular reference to a column vector being a matrix with one column or a row vector being a matrix with one row. For example the function \\texttt{dim}, which returns the dimensions of a matrix, does not apply to vectors\n\n::: {.cell}\n\n```{.r .cell-code}\nv<-c(1,2,3)\nM<-matrix(c(1,2,3,4),nrow=2)\ndim(v)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(M)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2 2\n```\n\n\n:::\n:::\n\nHowever, when we apply matrix operations to vectors, R will coerce them into matrices, see for example\n\n::: {.cell}\n\n```{.r .cell-code}\nt(v)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(t(v))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 3\n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "07-optimisation-in-R_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}