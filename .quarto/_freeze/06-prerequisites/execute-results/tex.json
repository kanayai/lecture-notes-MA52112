{
  "hash": "3a07dd1688aec164ceae8f199c78a02e",
  "result": {
    "engine": "knitr",
    "markdown": "# Prerequisites {#sec-prerequisites} \n\n## Numerical\n\n### Vectors and matrices in R\n\n\n## Linear Algebra \n\n\n\n:::{.callout-tip icon=false}\n\n## Vector and matrix notation\n\n::: {#def-vector-notation} \n\n\n\n* Vectors are assumed to be **column vectors by default**, that is, if $\\boldsymbol{a}\\in \\rel^p$ then we write\n\n$$\\bm{a}=\\left(\n\\begin{array}{c}\na_1 \\\\\n\\vdots\\\\\na_p\n\\end{array}\n\\right)$$\n\n\n* $\\bm{0}_m$ and $\\bm{1}_m$ will denote the vector of zeros and ones, respectively, in $\\rel^m$ while $\\bm{0}_{n \\times m}$ denotes the zero matrix of dimension $n \\times m$ and $\\bm{I}_{n}$  the identity matrix of dimension $n \\times n$.\n\n* The **transpose of a vector** $\\bm{a} \\in \\rel^p$ is denoted by $\\bm{a}^T$ and is therefore a row vector, i.e.\n$$\\bm{a}^T=(a_1,a_2,\\ldots,a_p)\\,.$$\n\n* The **transpose of a matrix** $\\bm{M}$ of dimension $n \\times m$ is denoted by $\\bm{M}^T$ and is a matrix of dimension $m \\times n$.\n \n* For a given vector $\\bm{a}\\in \\rel ^n$, $\\mbox{diag}(\\bm{a})$ will denote the diagonal matrix of dimensio $n \\times n$ with entries of $\\bm{a}$ as entries in the main diagonal, that is\n$$\n\\mbox{diag}(\\bm{a})=\n\\begin{pmatrix}\na_1    & 0      & 0      & \\cdots  & 0 \\\\\n0      & a_2    & 0      & \\cdots  & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots  & \\vdots \\\\\n0      & 0      & \\cdots & a_{n-1} & 0\\\\\n0      & 0      & \\cdots & 0       & a_n\n\\end{pmatrix}\n$$\n\n:::\n:::\n\n:::{.callout-tip icon=false}\n\n## Inner product and norm\n\n::: {#def-inner-prod-norm} \n\n\n**Inner product:** Let \n\n$$\\bm{a}=\\left(\n\\begin{array}{c}\na_1 \\\\\n\\vdots\\\\\na_p\n\\end{array}\n\\right)\\,,\\qquad \\bm{b}=\\left(\n\\begin{array}{c}\nb_1 \\\\\n\\vdots\\\\\nb_p\n\\end{array}\n\\right)$$\n\nbe two vectors in $\\rel^p$. The inner product between $\\bm{a}$ and $\\bm{b}$ is defined as\n$$\\bm{a}^T \\bm{b}:=\\sum_{i=1}^p a_i b_i$$\n\n**Norm of a vector:** The **norm** (or length) of a vector $\\bm{a} \\in \\rel^p$ is defined by\n$$\\|\\bm{a}\\|=\\sqrt{\\bm{a}^T\\bm{a}}$$\n\n:::\n:::\n\n\n:::{.callout-tip icon=false}\n\n## Quadratic forms\n\n::: {#def-quadratic-form} \n\n\n\nIf $\\bm{M}$ is a **symmetric** matrix in $\\rel^{p\\times p}$ then a **quadratic form in** $\\bm{M}$ is defined as \n\n$$\\bm{a}^T\\bm{M}\\bm{a}:=a_1^2 \\bm{M}_{11}+\\cdots+2a_{i}a_{j}\\bm{M}_{ij} +\\cdots +a_p^2\\bm{M}_{pp}$$\n\n\n:::\n:::\n\n:::{.callout-tip icon=false}\n\n## Positive (semi)definite matrix\n\n::: {#def-pos-definite-matrix} \n\nA   symmetric matrix $\\bm{M}$ is **positive (semi) definite** if \n\n$$\n\\bm{a}^T\\bm{M}\\bm{a}> (\\geq )0\\qquad \\forall \\,\\bm{a}\\neq \\bm{0}\n$$ {#eq-positive-definite-definition}\n\nEquivalently, $\\bm{M}$ is positive (semi)definite if all eigenvalues of $\\bm{M}$ are (nonnegative) positive.\n\n:::\n:::\n\n\n:::{.callout-tip icon=false}\n\n## Nonsingularity of positive definite matrices\n\n::: {#prp-posdef-invertible} \n\nA   symmetric positive definite matrix $\\bm{M}$ is nonsingular.\n\n:::\n:::\n\n\n\n\n\n\n:::{.callout-tip icon=false}\n\n## Matrix square-root\n\n::: {#def-matrix-square-root}\n\n\nGiven a positive definite matrix $\\bm{A}$ of dimension $p\\times p$, the symmetric square root of $\\bm{A}$ (denoted by $\\bm{A}^{1/2}$) is defined as\n$$\\bm{A}^{1/2}:=\\bm{U}\\bm{\\Lambda}^{1/2}\\bm{U}^T$$\nwhere $\\bm{U}$ is the matrix of eigenvectors of $\\bm{A}$ and $\\bm{\\Lambda}$ is the diagonal matrix of eigenvalues of $\\bm{A}$. Here $\\bm{\\Lambda}^{1/2}$ is simply the diagonal matrix with the square root of the diagonal entries of $\\bm{\\Lambda}$\n\n:::\n:::\n\n\n\n\n## Vector calculus \n\n\nWe start by distinguishing between the derivative of a real valued function and that of a vector valued one.\n\n\n\n:::{.callout-tip icon=false}\n\n## Gradient: derivative of a real valued function\n\n::: {#def-gradient}\n\n\nLet   $f:\\rel^p \\to \\rel$ be a twice continuously differentiable real-valued function over  $\\rel^p$. This means that the function has first and second derivatives which are continuous over $\\rel^p$. The vector \n\n$$ \n\\displaystyle \\frac{\\partial f}{\\partial \\bm{\\theta}}\n:=\n\\left(\n\\begin{array}{c}\n\\displaystyle\n\\frac{\\partial f}{\\partial \\theta_1}\\\\\n\\rule{0in}{4ex}\\displaystyle \\frac{\\partial f}{\\partial \\theta_2}\\\\\n\\vdots\\\\\n\\rule{0in}{4ex}\\displaystyle \\frac{\\partial f}{\\partial \\theta_p}\\\\\n\\end{array}\n\\right)\n$$\n  \nis called the **gradient  of** $f$ . \n\n\n\n\n*  The gradient of $f$ will be  denoted  by $\\nabla_{\\! \\bm{\\theta}} f(\\bm{\\theta})$. \n\n* The **gradient is a vector-valued function** of $\\bm{\\theta}$. The gradient function evaluated at a particular point $\\bm{\\theta}_k$ will be denoted by\n\n$$\n\\nabla_{\\!\\bm{\\theta}} f(\\bm{\\theta}_k):=\n\\left.\\rule{0in}{2.5ex}\n\\nabla_{\\!\\bm{\\theta}}  f(\\bm{\\theta})\n\\right|_{\\bm{\\theta}=\\bm{\\theta}_k}\n=\n\\left.\\rule{0in}{2.5ex}\n\\displaystyle \\frac{\\partial f}{\\partial \\bm{\\theta}}\n\\right|_{\\bm{\\theta}=\\bm{\\theta}_k}\n$$\n\n* The transpose of the gradient will be denoted by:\n$$\n \\frac{\\partial f}{\\partial \\bm{\\theta}^T} := \\nabla_{\\!\\bm{\\theta}}  f(\\bm{\\theta}) ^T=\\left(\\frac{\\partial f}{\\partial \\bm{\\theta}}\\right)^T=\\left(\\frac{\\partial f}{\\partial \\theta_1}\\,,\\frac{\\partial f}{\\partial \\theta_2}\\,,\\cdots\\,,\\frac{\\partial f}{\\partial \\theta_p}\\right)\n$$\nThe notation $\\frac{\\partial f}{\\partial \\bm{\\theta}}$ and $\\frac{\\partial f}{\\partial \\bm{\\theta}^T}$ respect the fact that $\\bm{\\theta}$ is a column vector and that $\\bm{\\theta}^T$ is a row vector. This notation  is  called *denominator layout notation*.\n\n\n\n:::\n:::\n\n\n\n\n:::{.callout-tip icon=false}\n\n### Derivatives of linear and quadratic functions\n\n::: {#prp-derivatives-matrices}\n\n* The gradient of a constant real valued function $f(\\bm{\\theta})=k$ ($\\bm{\\theta}\\in \\rel^p$) is the zero vector\n$$\n\\nabla_{\\!\\bm{\\theta}}f(\\bm{\\theta}) = \\nabla_{\\!\\bm{\\theta}}\\, k = \\bm{0}_p\n$$ {#eq-gradient-real-constant}\n\n* The gradient of a linear function $\\bm{c}^T\\bm{\\theta}$ (for some vector of constants $\\bm{c}\\in\\rel^p$) is given by\n$$\n\\nabla_{\\!\\bm{\\theta}}  [\\bm{\\theta}^T\\bm{c}]= \\nabla_{\\!\\bm{\\theta}}   [\\bm{c}^T\\bm{\\theta}]=\\bm{c}\n$$ {#eq-gradient-cTx}\n\n\n* The gradient of the quadratic form $\\bm{\\theta}^T\\bm{B}\\bm{\\theta}$\n  for some  matrix  of constants $\\bm{B}\\in \\rel^{p\\times p}$ is given by:\n$$\n\\nabla_{\\!\\bm{\\theta}} \n\\left[\n\\bm{\\theta}^T\\bm{B}\\bm{\\theta}\n\\right]\n=(\\bm{B}+\\bm{B}^T)\\,\\bm{\\theta}\n$$ {#eq-gradient-quad-form-nonsym} \nIf $\\bm{B}$ is symmetrical then   \n  $$\n\\nabla_{\\!\\bm{\\theta}} \n\\left[\n\\bm{\\theta}^T\\bm{B}\\bm{\\theta}\n\\right]\n=\n2\\bm{B}\\,\\bm{\\theta}\n$$ {#eq-gradient-quad-form}\n\n:::\n:::\n\n\n:::{.callout-tip icon=false}\n\n## Jacobian: derivative of a vector valued function\n\n::: {#def-gradient-vector-value}\n\nLet $\\bm{g}:\\rel^p \\to \\rel^m$ be a differentiable vector-valued function over  $\\rel^p$,\nthat is \n$$\n\\bm{g}(\\bm{\\theta}) =\n\\left(\n\\begin{array}{c}\ng_1(\\bm{\\theta})\\\\\ng_2(\\bm{\\theta})\\\\\n\\vdots\\\\\ng_m(\\bm{\\theta})\\\\\n\\end{array}\n\\right)\n$$\n\nfor given differentiable real-valued functions $g_1,\\ldots,g_m$ such that $g_i:\\rel^p\\to \\rel$ for $i=1,\\ldots,m$. The $m \\times p$ derivative matrix\n\n\n\n$$\n\\frac{\\partial\\,\\bm{g}(\\bm{\\theta})}{\\partial \\bm{\\theta}^T}\n:=\n\\left(\n\\begin{array}{c}\n\\displaystyle \\frac{\\partial}{\\partial \\bm{\\theta}^T}g_1(\\bm{\\theta})\\\\\n\\displaystyle \\frac{\\partial}{\\partial \\bm{\\theta}^T}g_2(\\bm{\\theta})\\\\\n\\vdots\\\\\n\\displaystyle \\frac{\\partial}{\\partial \\bm{\\theta}^T}g_m(\\bm{\\theta})\\\\\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{c}\n\\displaystyle [\\nabla_{\\!\\bm{\\theta}}\\, g_1(\\bm{\\theta})]^T\\\\\n\\displaystyle [\\nabla_{\\!\\bm{\\theta}}\\, g_2(\\bm{\\theta})]^T\\\\\n\\vdots\\\\\n\\displaystyle [\\nabla_{\\!\\bm{\\theta}}\\, g_m(\\bm{\\theta})]^T\\\\\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{cccc}\n\\displaystyle \\frac{\\partial g_1(\\bm{\\theta})}{\\partial \\theta_1} & \\displaystyle \\frac{\\partial g_1(\\bm{\\theta})}{\\partial \\theta_2} & \\cdots & \\displaystyle \\frac{\\partial g_1(\\bm{\\theta})}{\\partial \\theta_p}\\\\\n\\displaystyle \\frac{\\partial g_2(\\bm{\\theta})}{\\partial \\theta_1} & \\displaystyle \\frac{\\partial g_2(\\bm{\\theta})}{\\partial \\theta_2} & \\cdots & \\displaystyle \\frac{\\partial g_2(\\bm{\\theta})}{\\partial \\theta_p}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots\\\\\n\\displaystyle\\frac{\\partial g_m(\\bm{\\theta})}{\\partial \\theta_1} &  \\displaystyle\\frac{\\partial g_m(\\bm{\\theta})}{\\partial \\theta_2} & \\cdots &  \\displaystyle\\frac{\\partial g_m(\\bm{\\theta})}{\\partial \\theta_p}\\\\\n\\end{array}\n\\right)\n$$ {#eq-gradient-vector-matrix}\n\n is called the **Jacobian** of $\\bm{g}$\n\n\n\n  \n\n* The **Jacobian** will be denoted by $\\bm{J}_{\\bm{g}}(\\bm{\\theta})$\n\n* The **Jacobian is a matrix-valued function** of $\\bm{\\theta}$ where the resulting matrix is of dimension $m \\times p$ \n\n* The Jacobian evaluated at a particular point $\\bm{\\theta}_k$ will be denoted by\n$$\\bm{J}_{\\bm{g}}(\\bm{\\theta}_k):=\\left.\\rule{0in}{2ex} \n\\frac{\\partial\\,\\bm{g}(\\bm{\\theta})}{\\partial \\bm{\\theta}^T}\n\\right|_{\\bm{\\theta}=\\bm{\\theta}_k}$$\n\n:::\n:::\n\n:::{.callout-tip icon=false}\n\n## Derivatives of vector-valued linear functions\n\n::: {#prp-derivative-gral-lin-form}\n\n\n* The Jacobian of a constant vector-valued function $\\bm{g}(\\bm{\\theta})=\\bm{k} \\in \\rel^m$ (for all $\\bm{\\theta}\\in \\rel^p$) is the zero matrix\n$$\n\\bm{J}_{\\bm{g}}(\\bm{\\theta})=  \\bm{0}_{m \\times p}\n$$\n* The Jacobian  of the vector-valued linear function $\\bm{g}(\\bm{\\theta})=\\bm{A}\\bm{\\theta}$ (for some matrix of constants $\\bm{A}\\in\\rel^{m\\times p}$) is given by\n$$\n\\bm{J}_{\\bm{g}}(\\bm{\\theta})=\\bm{A}\n$$ {#eq-gradient-Ax}\n\n\n:::\n:::\n\n\n:::{.callout-tip icon=false}\n\n## Chain rule: gradient of a composition\n\n::: {#prp-chain-rule-gral}\n\nLet \n\n* $\\bm{g}:\\rel^p \\to \\rel^m$ be a differentiable **vector-valued** function over  $\\rel^p$ and  \n\n* $h:\\rel^m \\to \\rel$ be a differentiable **real-valued** function over  $\\rel^m$.\n\nThe gradient of the  composition  $h(\\bm{g}(\\bm{\\theta}))$  is given by:\n\n\n$$\n\\underbrace{\\nabla_{\\!\\bm{\\theta}}\\, h(\\bm{g}(\\bm{\\theta}))}\n_{\\text{column $p$ vector}}=\n\\sum_{i=1}^m \\underbrace{\\left.\\frac{\\partial h}{\\partial u_i}\\right|_{\\bm{u}=\\bm{g}(\\bm{\\theta})}}_{\\text{scalar}}\\times\n\\underbrace{\\nabla_{\\!\\bm{\\theta}} g_i(\\bm{\\theta})}_{\\text{column $p$ vector}} = \\underbrace{\\bm{J}_{\\bm{g}}^T(\\bm{\\theta})}_{p\\times m \\,\\,\\text{matrix}}\\,\n\\underbrace{\\left.\\displaystyle \\nabla_{\\!\\bm{u}}\\, h(\\bm{u})\\right|_{\\bm{u}=\\bm{g}(\\bm{\\theta})}}_{\\text{column $m$ vector}}\n$$ {#eq-chain-rule-gradient}\n\nwhen $m=1$ we simply have:\n\n$$\n\\underbrace{\\nabla_{\\bm{\\theta}}\\, h(\\bm{g}(\\bm{\\theta}))}\n_{\\text{column $p$ vector}}\n= \n\\underbrace{h'(g(\\bm{\\theta}))}_{\\text{scalar}}\\, \n\\underbrace{\\nabla_{\\! \\bm{\\theta}}\\, g(\\bm{\\theta})}_{\\text{column $p$ vector}}\n$$ {#eq-chain-rule-1dim}\n\n\n\n:::\n:::\n\nThe following result is an application of the chain rule.\n\n:::{.callout-tip icon=false}\n\n## Chain rule for inner-product functions\n\n::: {#cor-chain-rule-ex}\n\nLet \n\n* $\\bm{g}:\\rel^p \\to \\rel^m$ be a differentiable **vector-valued** function over  $\\rel^p$ and  \n\n* Let $\\bm{k} \\in \\rel^m$ be a constant vector then\n\n\n$$\n\\underbrace{\\nabla_{\\bm{\\theta}} (\\bm{k}^T\\bm{g}(\\bm{\\theta}))}\n_{\\text{column $p$ vector}}\n= \\underbrace{\\bm{J}_{\\bm{g}}^T(\\bm{\\theta})}_{\\text{$p \\times m$ matrix }}\\,\n\\underbrace{\\bm{k}}_{\\text{column $m$ vector}}\n$$ {#eq-chain-rule-ex}\n\n\n\n:::\n:::\n\n<!--\n$$\n\\frac{\\partial f}{\\partial \\bm{\\theta}^T}=\n\\sum_{i=1}^m \\underbrace{\\left.\\frac{\\partial h}{\\partial u_i}\\right|_{\\bm{u}=\\bm{g}(\\bm{\\theta})}}_{\\text{scalar}}\\times\n\\underbrace{\\frac{\\partial g_i}{\\partial \\bm{\\theta}^T}}_{\\text{row vector}} = \\underbrace{\\left. \\frac{\\partial h}{\\partial \\bm{u}^T}\\right|_{\\bm{u}=\\bm{g}(\\bm{\\theta})}}_{\\text{row vector}}\\,\\underbrace{\\frac{\\partial \\bm{g}}{\\partial \\bm{\\theta}^T}}_{\\text{matrix}}\n$$ {#eq-chain-rule-alt}\n-->\n\n\n\nThe second derivative (Hessian) matrix of a real-valued function $f$ can be obtained as  the Jacobian of the corresponding gradient, that is,  when  $\\bm{g}(\\bm{\\theta})=\\nabla_{\\!\\bm{\\theta}} f(\\bm{\\theta})$ and $m=p$.\n\n\n:::{.callout-tip icon=false}\n\n## Hessian: second derivative of a real valued function\n\n::: {#def-Hessian}\n\n\n\n\nGiven a function $f:\\rel^p \\to \\rel$, the matrix \n\n$$\\frac{\\partial}{\\partial \\bm{\\theta}^T }\\,\\nabla_{\\! \\bm{\\theta}} f(\\bm{\\theta})=\n\\left(\n\\begin{array}{c}\n\\displaystyle \\frac{\\partial}{\\partial \\bm{\\theta}^T}\\,\\frac{\\partial f}{\\partial \\theta_1}\\\\\n\\displaystyle \\frac{\\partial}{\\partial \\bm{\\theta}^T}\\,\\frac{\\partial f}{\\partial \\theta_2}\\\\\n\\vdots\\\\\n\\displaystyle \\frac{\\partial}{\\partial \\bm{\\theta}^T}\\,\\frac{\\partial f}{\\partial \\theta_p}\\\\\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{cccc}\n \\displaystyle\n\\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_1} &\n\\displaystyle\n  \\frac{\\partial^2 f}{\\partial \\theta_2 \\partial \\theta_1}  & \\cdots &\\displaystyle \n\\frac{\\partial^2 f}{\\partial \\theta_p \\partial \\theta_1}  \\\\\n\\rule{0in}{4ex}\\displaystyle \n\\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_2} &\n\\displaystyle  \n\\frac{\\partial^2 f}{\\partial \\theta_2 \\partial \\theta_2}  & \\cdots &\\displaystyle\n\\frac{\\partial^2 f}{\\partial \\theta_p \\partial \\theta_2}  \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\rule{0in}{4ex}\\displaystyle\n\\frac{\\partial^2 f}{\\partial \\theta_1 \\partial \\theta_p} &\n\\displaystyle  \n\\frac{\\partial^2 f}{\\partial \\theta_2 \\partial \\theta_p}  & \\cdots &\\displaystyle\n\\frac{\\partial^2 f}{\\partial \\theta_p \\partial \\theta_p}  \\\\\n\\end{array}\n\\right)$$\n\n\nis called the **Hessian matrix of** $f$. \n\n* The **Hessian** will be  denoted by $\\nabla^2_{\\! \\bm{\\theta}} f(\\bm{\\theta})$. \n\n* By the continuity of $\\bm{f}$ we have that\n\n$$\\left(\\frac{\\partial}{\\partial \\bm{\\theta}^T }\\,\\nabla_{\\! \\bm{\\theta}} f(\\bm{\\theta})\\right)^T = \\frac{\\partial}{\\partial \\bm{\\theta}^T }\\,\\nabla_{\\! \\bm{\\theta}} f(\\bm{\\theta})$$\n\nso that the Hessian is a symmetric matrix.\n\n* We will sometimes  use the notation\n\n$$\\frac{\\partial^2 f}{\\partial \\bm{\\theta}^T \\partial \\bm{\\theta}}=\\frac{\\partial^2 f}{\\partial \\bm{\\theta} \\partial \\bm{\\theta}^T}$$\nfor the Hessian $\\nabla^2_{\\! \\bm{\\theta}} f(\\bm{\\theta})$.\n\n\n  \n\n\n\n* The **Hessian is a matrix-valued function** of $\\bm{\\theta}$. The Hessian evaluated at a particular point $\\bm{\\theta}_k$ will be denoted by\n$$\\nabla^2_{\\! \\bm{\\theta}} f(\\bm{\\theta}_k):=\\left.\\rule{0in}{2ex}\\nabla^2_{\\! \\bm{\\theta}} f(\\bm{\\theta})\\right|_{\\bm{\\theta}=\\bm{\\theta}_k}$$\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n:::{.callout-tip icon=false}\n\n## Chain rule for inner-product functions 2\n\n::: {#cor-chain-rule-ex-2}\n\n* Let $\\bm{X}$ be a $n \\times p$ matrix with $n$ rows  $\\bm{x}_1,\\ldots,\\bm{x}_n\\in \\rel^p$ and $p$ columns $\\bm{w}_1,\\ldots,\\bm{w}_p \\in \\rel ^n$ so that we can write:\n\n$$\n\\bm{X}\n=\n\\begin{pmatrix}\n\\bm{x}_1^T \\\\\n\\bm{x}_2^T \\\\\n\\vdots\\\\\n\\bm{x}_n^T \\\\\n\\end{pmatrix}\n\\qquad \n\\bm{X}^T\n=\n\\begin{pmatrix}\n\\bm{w}_1^T \\\\\n\\bm{w}_2^T \\\\\n\\vdots\\\\\n\\bm{w}_p^T \\\\\n\\end{pmatrix}\n$$\n\n\n* Let $h:\\rel \\to \\rel$ be a smooth real-valued function then we define:\n$$\n\\bm{h}(\\bm{\\theta})\n:=\n\\begin{pmatrix}\nh(\\bm{x}_1^T\\bm{\\theta})\\\\\nh(\\bm{x}_2^T\\bm{\\theta})\\\\\n\\vdots \\\\\nh(\\bm{x}_n^T\\bm{\\theta})\\\\\n\\end{pmatrix}\\,,\n\\quad\n\\bm{h}'(\\bm{\\theta})\n:=\n\\begin{pmatrix}\nh'(\\bm{x}_1^T\\bm{\\theta})\\\\\nh'(\\bm{x}_2^T\\bm{\\theta})\\\\\n\\vdots\\\\\nh'(\\bm{x}_n^T\\bm{\\theta})\\\\\n\\end{pmatrix}\\,,\n\\qquad\n\\bm{h}''(\\bm{\\theta})\n:=\n\\begin{pmatrix}\nh''(\\bm{x}_1^T\\bm{\\theta})\\\\\nh''(\\bm{x}_2^T\\bm{\\theta})\\\\\n\\vdots\\\\\nh''(\\bm{x}_n^T\\bm{\\theta})\\\\\n\\end{pmatrix}\n$$\n\n* Let $\\bm{k}=(k_1,\\ldots,k_n)^T\\in \\rel^n$ be a constant vector\n\nThen \n\n\n\n$$\n\\nabla_{\\bm{\\theta}} (\\bm{k}^T\\bm{h}(\\bm{\\theta})) \n =\n \\bm{X}^T\\mbox{diag}(\\bm{h}'(\\bm{\\theta})) \\bm{k}\n =\n \\bm{X}^T(\\bm{h}'(\\bm{\\theta}) \\odot \\bm{k})\n$$ {#eq-chain-rule-innerp-A}\n \n$$\n\\nabla_{\\bm{\\theta}}^2 (\\bm{k}^T\\bm{h}(\\bm{\\theta})) \n\\rule{0in}{4ex}=\n\\bm{X}^T\\text{diag}(\\bm{h}''(\\bm{\\theta})\\odot \\bm{k})\\bm{X}\n$$ {#eq-chain-rule-innerp-B}\n \n \nwhere $\\odot$ denotes entry-wise multiplication of vectors and\n\n$$\nh'(\\bm{x}_i^T\\bm{\\theta})\n:=\n\\left.\\frac{d}{d\\eta}h(\\eta)\\right|_{\\eta=\\bm{x}_i^T\\bm{\\theta}}\n\\qquad\nh''(\\bm{x}_i^T\\bm{\\theta})\n:=\n\\left.\\frac{d^2}{d\\eta^2}h(\\eta)\\right|_{\\eta=\\bm{x}_i^T\\bm{\\theta}}\n$$\n\n**NOTE 1**  The derivatives $h'$ and $h''$ above are with respect  to the argument $\\eta$ of $h$ and not with respect to $\\bm{\\theta}$. \n\n**NOTE 2** **Error in ([-@eq-chain-rule-innerp-A]) in  previous version has now been corrected.**\n\n:::\n:::\n\n\n\n::: {.Proof }\n\nProof. We first obtain the Jacobian of $\\bm{g}$ using @def-gradient-vector-value. Then we use the chain rule ([-@eq-chain-rule-1dim])   together with  ([-@eq-gradient-cTx])  to obtain:\n\n\n\\begin{align}\n\\bm{J}_{\\bm{h}}(\\bm{\\theta})\n&= \n\\begin{pmatrix}\n\\displaystyle \n\\left[\\nabla_{\\!\\bm{\\theta}}\\, h(\\bm{\\theta}^T\\bm{x}_1)\\right]^T \\\\\n\\displaystyle \\left[\\nabla_{\\!\\bm{\\theta}}\\, h(\\bm{\\theta}^T\\bm{x}_2)\\right]^T \\\\\n\\vdots \\\\\n\\displaystyle \\left[\\nabla_{\\!\\bm{\\theta}}\\, h(\\bm{\\theta}^T\\bm{x}_n)\\right]^T \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\displaystyle \n\\left[\nh'(\\bm{\\theta}^T\\bm{x}_1)\\,\\nabla_{\\!\\bm{\\theta}}\\, (\\bm{\\theta}^T\\bm{x}_1)\\right]^T \\\\\n\\displaystyle \n\\left[\nh'(\\bm{\\theta}^T\\bm{x}_2)\\,\\nabla_{\\!\\bm{\\theta}}\\, (\\bm{\\theta}^T\\bm{x}_2)\\right]^T \\\\\n\\vdots \\\\\n\\displaystyle \n\\left[\nh'(\\bm{\\theta}^T\\bm{x}_n)\\,\\nabla_{\\!\\bm{\\theta}}\\, (\\bm{\\theta}^T\\bm{x}_n)\\right]^T \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\displaystyle \nh'(\\bm{\\theta}^T\\bm{x}_1)\\, \\bm{x_1}^T\\\\\n\\displaystyle \nh'(\\bm{\\theta}^T\\bm{x}_2)\\, \\bm{x_2}^T\\\\\n\\vdots\\\\\n\\displaystyle \nh'(\\bm{\\theta}^T\\bm{x}_n)\\, \\bm{x_n}^T\n\\end{pmatrix}\\\\\n\\rule{0in}{10ex}&=\n\\begin{pmatrix}\nh'(\\bm{\\theta}^T\\bm{x}_1) & 0 & 0 & \\cdots & 0 \\\\\n0 & h'(\\bm{\\theta}^T\\bm{x}_1) & 0 & \\cdots & 0 \\\\\n\\vdots &\\vdots &\\vdots &\\vdots &\\vdots \\\\\n0 & 0 & \\cdots & 0 & h'(\\bm{\\theta}^T\\bm{x}_1)\n\\end{pmatrix}\n\\begin{pmatrix}\n\\bm{x_1}^T\\\\\n\\bm{x_2}^T\\\\\n\\vdots\\\\\n\\bm{x_n}^T\n\\end{pmatrix}\\\\\n\\rule{0in}{4ex}&= \\mbox{diag}(\\bm{h}'(\\bm{\\theta}))\\bm{X} \n\\end{align} \n\nso that \n\n$$\n\\bm{J}_{\\bm{h}}(\\bm{\\theta})\n=\n\\mbox{diag}(\\bm{h}'(\\bm{\\theta}))\\bm{X}\n$${#eq-jacobian-mu-gral}\n\nNow, using @cor-chain-rule-ex  and ([-@eq-jacobian-mu-gral])   we obtain\n\n\n\n\\begin{align}\n\\nabla_{\\! \\bm{\\theta}} (\\bm{k}^T\\bm{h}(\\bm{\\theta})) \n&= \\bm{J}_{\\bm{h}}(\\bm{\\theta})^T \\bm{k}\\\\\n&= \n[\\mbox{diag}(\\bm{h}'(\\bm{\\theta}))\\bm{X}]^T\\bm{k}\\\\\n&= \\bm{X}^T[\\mbox{diag}(\\bm{h}'(\\bm{\\theta}))]^T\\bm{k}\\\\\n&= \\bm{X}^T\\mbox{diag}(\\bm{h}'(\\bm{\\theta}))\\,\\bm{k}\\\\\n&= \\bm{X}^T(\\bm{h}'(\\bm{\\theta})\\odot \\bm{k})\\quad  \n\\,.\n\\end{align}\n\nwhich proves ([-@eq-chain-rule-innerp-A]). We note we can write\n$$\n\\bm{X}^T(\\bm{h}'(\\bm{\\theta})\\odot \\bm{k})=\n\\begin{pmatrix}\n\\bm{w}_1^T [\\bm{h}'(\\bm{\\theta})\\odot \\bm{k}]\\\\\n\\bm{w}_2^T [\\bm{h}'(\\bm{\\theta})\\odot \\bm{k}] \\\\\n\\vdots \\\\\n\\bm{w}_p^T [\\bm{h}'(\\bm{\\theta})\\odot \\bm{k}]\\\\\n\\end{pmatrix}\n$$ {#eq-XTmu}\n\n\nthen using @def-Hessian, ([-@eq-chain-rule-innerp-A]) and ([-@eq-XTmu]) followed by @cor-chain-rule-ex, we obtain:\n\n\n\n\\begin{align}\n\\nabla^2_{\\! \\bm{\\theta}}\\, (\\bm{k}^T\\bm{h}(\\bm{\\theta}))\n&=\n\\frac{\\partial }{\\partial \\bm{\\theta^T}}\\,\\nabla_{\\! \\bm{\\theta}}\\, (\\bm{k}^T\\bm{h}(\\bm{\\theta}))\n\\\\\n&= \\frac{\\partial }{\\partial \\bm{\\theta^T}} \\bm{X}^T(\\bm{h}'(\\bm{\\theta})\\odot \\bm{k}) \\\\\n&= \\rule{0in}{13ex}\n\\begin{pmatrix}\n\\displaystyle \n\\frac{\\partial}{\\partial \\bm{\\theta}^T}\\bm{w}_1^T[\\bm{h}'(\\bm{\\theta})\\odot \\bm{k}]\\\\\n\\displaystyle \n\\frac{\\partial}{\\partial \\bm{\\theta}^T}\\bm{w}_2^T[\\bm{h}'(\\bm{\\theta})\\odot \\bm{k}]\\\\\n\\vdots\\\\\n\\displaystyle \n\\frac{\\partial}{\\partial \\bm{\\theta}^T}\\bm{w}_p^T[\\bm{h}'(\\bm{\\theta})\\odot \\bm{k}]\\\\\n\\end{pmatrix}  \\\\\n&= \n\\rule{0in}{12ex}\n\\begin{pmatrix}\n\\displaystyle [\\bm{J}^T_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\\bm{w}_1]^T\\\\\n\\displaystyle [\\bm{J}^T_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\\bm{w}_2]^T\\\\\n\\vdots\\\\\n\\displaystyle [\\bm{J}^T_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\\bm{w}_p]^T\\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\displaystyle \\bm{w}_1^T\\,\\bm{J}_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\\\\\n\\displaystyle \\bm{w}_2^T\\,\\bm{J}_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\\\\\n\\vdots\\\\\n\\displaystyle \\bm{w}_p^T\\,\\bm{J}_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\\\\\n\\end{pmatrix}\\\\\n&= \\rule{0in}{3ex}\n\\bm{X}^T \\bm{J}_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\n\\end{align} \n\n\n\n\n\nso that \n\n$$\n\\nabla^2_{\\! \\bm{\\theta}}\\, (\\bm{k}^T\\bm{h}(\\bm{\\theta}))\n=\n\\bm{X}^T \\bm{J}_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\n$${#eq-hessian-innerp-partial}\n\n\nNow, using @def-gradient-vector-value  followed by ([-@eq-chain-rule-1dim])  and ([-@eq-gradient-cTx]) we have\n\n\n$$\n\\bm{J}_{\\bm{h}'\\odot \\bm{k}}(\\bm{\\theta})\n= \n\\begin{pmatrix}\n\\displaystyle [\\nabla_{\\!\\bm{\\theta}}\\, (k_1\\,h'(\\bm{\\theta}^T\\bm{x}_1))]^T \\\\\n\\displaystyle [\\nabla_{\\!\\bm{\\theta}}\\, (k_2\\,h'(\\bm{\\theta}^T\\bm{x}_2))]^T \\\\\n\\vdots \\\\\n\\displaystyle [\\nabla_{\\!\\bm{\\theta}}\\, (k_n\\,h'(\\bm{\\theta}^T\\bm{x}_n))]^T \\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\displaystyle k_1\\,h''(\\bm{\\theta}^T\\bm{x}_1)\\, \\bm{x_1}^T\\\\\n\\displaystyle k_2\\,h''(\\bm{\\theta}^T\\bm{x}_2)\\, \\bm{x_2}^T\\\\\n\\vdots\\\\\n\\displaystyle k_n\\,h''(\\bm{\\theta}^T\\bm{x}_n)\\, \\bm{x_n}^T\n\\end{pmatrix}\n= \\mbox{diag}(\\bm{h}''(\\bm{\\theta}) \\odot \\bm{k})\\bm{X} \n$$  {#eq-jacobian-mu-gral-hess}\n\nFinally, using ([-@eq-hessian-innerp-partial]) and ([-@eq-jacobian-mu-gral-hess]) we obtain \n\n\n$$\n\\nabla^2_{\\! \\bm{\\theta}}\\, (\\bm{k}^T\\bm{h}(\\bm{\\theta}))\n=\n\\bm{X}^T\\mbox{diag}(\\bm{h}''(\\bm{\\theta})\\odot \\bm{k})\\bm{X}\n$$\n\n\nwhich proves ([-@eq-chain-rule-innerp-B]). $\\quad \\square$\n\n\n:::\n\n\n\n\n:::{.callout-tip icon=false}\n\n## Chain rule: Hessian of a composition\n\n::: {#prp-chain-rule}\n\n\n\n\nLet $f:\\rel^p\\to \\rel$ be a function and $\\bm{g}:\\rel^p\\to\\rel ^p$ a one-to-one function. The  Hessian of the composition is  given by:\n\n\n$$\n\\begin{split}\n\\rule{0in}{4ex}\n\\displaystyle\n\\underbrace{\n\\nabla^2_{\\!\\bm{\\lambda}}\n(f\\circ \\bm{g})(\\bm{\\lambda})\n}_{\\text{$p \\times p$ matrix}}\n&=\n\\underbrace{\n\\bm{J}_{\\bm{g}}(\\bm{\\lambda})^T\n}_{\\text{$p \\times p$ matrix}}\n\\,\n\\underbrace{\n\\left[\n\\left.\n\\nabla^2_{\\!\\bm{\\theta}}\\, f(\\bm{\\theta})\n\\right|_{\\bm{\\theta}=\\bm{g}(\\bm{\\lambda})}\n\\right]\n}_{\\text{$p \\times p$ matrix}}\n\\underbrace{\n\\bm{J}_{\\bm{g}}(\\bm{\\lambda})\n}_{\\text{$p \\times p$ matrix}}\n+\n\\sum_{i=1}^p\n\\underbrace{\n\\left[ \\left.\\frac{\\partial f (\\bm{\\theta})}{\\partial \\theta_i}\\right|_{\\bm{\\theta}=\\bm{g}(\\bm{\\lambda})}\\right]}\n_{\\text{scalar}}\n\\,\n\\underbrace{\n\\nabla^2_{\\!\\bm{\\lambda}} \\,g_i(\\bm{\\lambda})}\n_{\\text{$p \\times p$ matrix}}\n\\end{split}\n$$  {#eq-chain-rule-hessian} \n\n**NOTE:** The argument of $f$ is denoted by $\\bm{\\theta}\\in \\rel^p$ while the argument of $\\bm{g}$ and $f \\circ \\bm{g}$ is denoted by $\\bm{\\lambda}\\in \\rel^p$. \n:::\n:::\n\n\n\n\n:::{.callout-tip icon=false}\n\n## Directional derivative\n\n::: {#def-directional-derivative}\n\n\n\n The directional derivative of a function   $f:\\rel^p \\to \\rel$ at a point $\\bm{\\theta}_k$ and along the unit-length vector $\\bm{v}$  is defined as \n$$D_{\\bm{v}}\\,f(\\bm{\\theta}_k):=\\lim_{h\\to 0}\\frac{f(\\bm{\\theta}_k+h\\,\\bm{v})-f(\\bm{\\theta}_k)}{h}$$\n\n:::\n:::\n\nThe link between the directional derivative and the gradient is given by the following result.\n\n\n:::{.callout-tip icon=false}\n\n## Directional derivative in terms of gradient\n\n::: {#prp-dir-deriv-gradient}\n\nLet $f:\\rel^p \\to \\rel$ be a smooth function then\n\n$$D_{\\bm{v}}\\,f(\\bm{\\theta}_k)=\\nabla_{\\!\\bm{\\theta}} f(\\bm{\\theta}_k) ^T\\bm{v}=\\|\\nabla_{\\! \\bm{\\theta}} f(\\bm{\\theta}_k) \\|\\,\\cos (\\alpha)$$\n\nwhere $\\alpha$ is the angle between $\\bm{v}$ and $\\nabla_{\\! \\bm{\\theta}} f(\\bm{\\theta}_k)$.\nAlso, the gradient, $\\nabla_{\\! \\bm{\\theta}} f(\\bm{\\theta})$, is a vector perpendicular\nto the level sets, $f(\\bm{\\theta}) = c$, where $c$ is a constant.\n\n\n:::\n:::\n  \n  \n\n:::{.callout-tip icon=false}\n\n## Taylor's Theorem\n\n::: {#thm-Taylor}\n\n\nLet $f:\\rel^p \\to \\rel$ be a smooth function and  $\\bm{\\Delta}\\in \\rel^p$ then there exists $t_1\\in (0,1)$ such that \n\n$$\nf(\\bm{\\theta}+\\bm{\\Delta})=f(\\bm{\\theta})+\\left[\\nabla f(\\bm{\\theta}+t_1 \\bm{\\Delta})\\right]^T \\bm{\\Delta}\n$$ {#eq-Taylor-first-order}\n\nAlso, there exists $t_2\\in (0,1)$ such that\n\n$$\nf(\\bm{\\theta}+\\bm{\\Delta}) = f(\\bm{\\theta}) + \\nabla f(\\bm{\\theta})^T \\bm{\\Delta} + \\frac{1}{2} \\bm{\\Delta}^T \\left[\\nabla^2f(\\bm{\\theta}+t_2\\bm{\\Delta})\\right]\\bm{\\Delta}\n$$ {#eq-Taylor-second-order}\n\n:::\n:::\n\n\nNote there is a linear form, namely $\\nabla f(\\bm{\\theta})^T \\bm{\\Delta}$ and a  quadratic form, namely $\\bm{\\Delta}^T\\left[ \\nabla ^2 f(\\bm{\\theta}+t_2\\bm{\\Delta})\\right]\\bm{\\Delta}$.  \n\n\n\n\n\n\n\n\n## Convex functions  {#sec-convexity}\n\n\n:::{.callout-tip icon=false}\n\n## Convexity\n\n::: {#def-convexity}\n\n\n*  A set $\\bm{A} \\subset \\rel^p$ is a **convex set** if, for any two points $\\bm{\\theta}_1\\in \\bm{A}$ and $\\bm{\\theta}_2\\in \\bm{A}$, we have $\\beta\\,\\bm{\\theta}_1+ (1-\\beta)\\,\\bm{\\theta}_2 \\in \\bm{A}$ for all $\\beta\\in [0,1]$ that is, if the straight line segment connecting the points lies entirely inside $\\bm{A}$. \n\n* A function $f:\\rel^p\\to \\rel$ is a **convex function** if its domain $\\bm{A}$ is a convex set and if for any two points $\\bm{\\theta}_1$ and $\\bm{\\theta}_2$ in $\\bm{A}$, the following property is satisfied:\n\n$$\nf(\\beta\\,\\bm{\\theta}_1+(1-\\beta)\\,\\bm{\\theta}_2)\\leq \\beta\\,f(\\bm{\\theta}_1)+(1-\\beta)\\,f(\\bm{\\theta}_2)\\qquad \\mbox{for all } \\beta \\in [0,1]\n$$ {#eq-convexity-definition}\n\n* We say that $\\phi$ is **strictly convex** if the inequality ([-@eq-convexity-definition]) is strict whenever $\\bm{\\theta}_1\\neq \\bm{\\theta}_2$ and $\\beta\\in (0,1)$. \n\n* A function $f$ is said to be **concave** if $-f$ is convex.\n\n:::\n:::\n\n\n\n\n\n\n:::{.callout-tip icon=false}\n\n## Convexity and positive-definiteness\n\n::: {#prp-Hessian-convexity}\n\n\n\n\n Let $f$ be twice continuously differentiable function over $\\rel^p$. Then \n\n* $f$ is a  convex function  if and only if $\\nabla ^2_{\\! \\bm{\\theta}} f(\\bm{\\theta})$ is positive semidefinite for all $\\bm{\\theta}$\n  \n* If  $\\nabla^2_{\\! \\bm{\\theta}} f(\\bm{\\theta})$ is positive definite then $f$ is strictly convex\n\n:::\n:::\n\n\n### Examples convex functions {#sec-examples-convex}\n\n* Let $\\bm{B}$ be a positive definite matrix. The quadratic function \n$$\n\\phi(\\bm{\\theta})=\\bm{\\theta}^T\\bm{B}\\bm{\\theta}\n$$\nis a strict convex function\n\n* The **LogSumExp** function given by\n\n\\begin{equation}\n\\mbox{LogSumExp}(\\bm{\\theta})=\\log(\\exp(\\theta_1)+\\cdots+\\exp(\\theta_p))\n\\end{equation}\n\nis a convex function. \n\n\n\n:::{.callout-tip icon=false}\n\n## Convexity and affine functions\n\n::: {#prp-composition-convexity}\n\n\nIf $f$ is a  convex function  then $f(\\bm{B}\\bm{\\theta}+\\bm{c})$  (for some nonsingular matrix $\\bm{B}\\in \\rel^{p \\times p}$ and some vector $\\bm{c}\\in \\rel^p$) is a convex function. \n \n:::\n:::\n\n:::{.callout-tip icon=false}\n\n## Sum of convex functions\n\n::: {#prp-sum-convexity}\n\n\n\n\nIf $f_1,\\ldots,f_r$ are convex functions and $w_1,\\ldots,w_r$ are nonnegative numbers then\n$$\\sum_{i=1}^r\\,w_i f_i(\\bm{\\theta})$$\n  is a convex function.\n\n:::\n:::\n\n\n\n## Random Vectors \n\n Let $\\bm{Z}$ be a **random matrix** of dimension $n \\times m$, that is\n$$\\bm{Z}=\n\\left(\n\\begin{array}{cccc}\nZ_{11} & Z_{12} & \\cdots & Z_{1m}\\\\\nZ_{21} & Z_{22} & \\cdots & Z_{2m}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nZ_{n1} & Z_{n2} & \\cdots & Z_{nm}\\\\\n\\end{array}\n\\right)$$\nwhere all $Z_{ij}$ are real-valued random variables.\n\n\n:::{.callout-tip icon=false}\n\n## Expected value of random matrices\n\n::: {#def-expected-value-matrix}\n\n\n\n\nThe **expected value** of $\\bm{Z}$ is defined as:\n$$\\pE[\\bm{Z}]:=\n\\left(\n\\begin{array}{cccc}\n\\pE[Z_{11}] & \\pE[Z_{12}] & \\cdots & \\pE[Z_{1m}]\\\\\n\\pE[Z_{21}] & \\pE[Z_{22}] & \\cdots & \\pE[Z_{2m}]\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\pE[Z_{n1}] & \\pE[Z_{n2}] & \\cdots & \\pE[Z_{nm}]\\\\\n\\end{array}\n\\right)$$\n\n\nIn particular,  if  $\\bm{W}=(W_1,\\ldots,W_n)^T$ is a random vector (that is when $m=1$), the expected value of $\\bm{W}$ is defined as:\n\n$$\\pE[\\bm{W}]=\\left(\n\\begin{array}{c}\n\\pE[W_1] \\\\\n\\vdots\\\\\n\\pE[W_n] \n\\end{array}\n\\right)$$\n\n\n:::\n:::\n\n\n\n:::{.callout-tip icon=false}\n\n## Expected value of functions of random vectors\n\n::: {#def-expected-value-matrix-function}\n\n\n\n\nLet $\\bm{W}=(W_1,\\ldots,W_n)^T$ be a continuous random vector with probability density function $f_{\\bm{W}}(\\bm{w})$. Let $\\bm{g}:\\rel^n\\to \\rel^{m\\times r}$ be defined as\n\n$$\\bm{g}(\\bm{W})=\n\\left(\n\\begin{array}{cccc}\ng_{11}(\\bm{W}) & g_{12}(\\bm{W}) & \\cdots & g_{1r}(\\bm{W})\\\\\ng_{21}(\\bm{W}) & g_{22}(\\bm{W}) & \\cdots & g_{2r}(\\bm{W})\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\ng_{m1}(\\bm{W}) & g_{m2}(\\bm{W}) & \\cdots & g_{mr}(\\bm{W})\\\\\n\\end{array}\n\\right)\n$$\nbe a continuous function, then the expected value of $\\bm{g}(\\bm{W})$ is defined as\n\n$$\\pE[\\bm{g}(\\bm{W})]=\n\\left(\n\\begin{array}{cccc}\n\\pE[g_{11}(\\bm{W})] & \\pE[g_{12}(\\bm{W})] & \\cdots & \\pE[g_{1r}(\\bm{W})]\\\\\n\\pE[g_{21}(\\bm{W})] & \\pE[g_{22}(\\bm{W})] & \\cdots & \\pE[g_{2r}(\\bm{W})]\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\pE[g_{m1}(\\bm{W})] & \\pE[g_{m2}(\\bm{W})] & \\cdots & \\pE[g_{mr}(\\bm{W})]\\\\\n\\end{array}\n\\right)\n$$\n\nwhere each expectation is defined as \n\n\n\n$$\\pE[g_{ij}(\\bm{W})]=\\int_{\\rel^n}g_{ij}(\\bm{w})f_{\\bm{W}}(\\bm{w})\\,d\\bm{w}$$\n\n:::\n:::\n\n\n There is a similar definition using a multiple sum (instead of a multiple integral) when $\\bm{W}$ is a discrete random vector.\n\n\n\n:::{.callout-tip icon=false}\n\n## Covariance matrix\n\n::: {#def-covariance-matrix}\n\n\n\n\nThe **covariance matrix** of the random vector $\\bm{W}=(W_1,\\ldots,W_n)^T$ is defined as:\n\n$$\\pV [\\bm{W}]:=\\pE[(\\bm{W}-\\pE[\\bm{W}])(\\bm{W}-\\pE[\\bm{W}])^T]=\\pE[\\bm{W}\\bm{W}^T]-\\pE[\\bm{W}]\\pE[\\bm{W}]^T$$\n\n:::\n:::\n\n\nNote that $\\pV [\\bm{W}]$ is a matrix of dimension $n \\times n$. Also note that $g(\\bm{W})=\\bm{W}\\bm{W}^T$ is a random matrix of dimension $n \\times n$ and is an example of a function of a random vector where $\\bm{g}:\\rel^n \\to \\rel^{m\\times r}$  where $m=r=n$.\n\n\n\n:::{.callout-tip icon=false}\n\n## Expected value and variance of linear functions of random vectors\n\n\n::: {#prp-expected-value-linear-function}\n\n\n\nLet $\\bm{A}\\in \\rel^{m\\times n}$ be a matrix of constants then\n\n\\begin{align*}\n\\pE [\\bm{AW}]&=\\bm{A}\\pE [\\bm{W}]\\\\\n\\pV [\\bm{AW}]&=\\bm{A}\\pV[\\bm{W}]\\bm{A}^T\n\\end{align*}\nIn particular, if $\\bm{a}\\in \\rel^n$ then \n$$0\\leq \\pV [\\bm{a}^T\\bm{W}]=\\bm{a}^T\\pV[\\bm{W}] \\bm{a}$$\nTherefore, a covariance matrix is always semi-positive definite.\n\n:::\n:::\n\n\nNote that $\\pV [\\bm{AW}]$ is a matrix of dimension $m \\times m$. Also note the conformability \nof the matrices $\\bm{A}\\in \\rel^{m \\times n}$, $\\pV[\\bm{W}]\\in \\rel^{n \\times n}$ and $\\bm{A}^T\\in \\rel^{n \\times m}$.\n\n### Transformation of multivariate random vectors\n\n\n:::{.callout-tip icon=false}\n\n## Transformation formula for continuous random vectors\n\n::: {#prp-transformation-random-vectors}\n\n\n\n\nLet $\\bm{Y}$ be a continuous random vector in $\\rel^m$ with probability density function $f_{\\scriptsize \\bm{Y}}(\\bm{y})$. Consider a transformation given in the follwoing form\n$$\\bm{Y}=\\bm{g}(\\bm{Z})$$ where $\\bm{g}:\\rel^m\\to \\rel^m$ is a one-to-one function. Then the pdf of the random vector $\\bm{Z}$ is given by:\n\n$$\nf_{\\scriptsize \\bm{Z}}(\\bm{z})=f_{\\scriptsize \\bm{Y}}(\\bm{g}(\\bm{z}))\\times |\\mbox{det}(  \\bm{J}_{\\bm{g}}(\\bm{z}))|\n$$ {#eq-transformation-random-vector}\n\nwhere $|\\mbox{det}(\\bm{J}_{\\bm{g}}(\\bm{z}))|$ is the absolute value of the determinant of the Jacobian matrix of the transformation $\\bm{g}$.\n\n:::\n:::\n\n\n### Multivariate Normal distribution\n\nLet $\\bm{Y}=(Y_1,\\ldots,Y_p)^T$ be a p-dimensional continuous random vector\n\nConsider a random vector $\\bm{Z}=(Z_1,Z_2,\\ldots,Z_p)^T$ of $p$ independent and identically distributed  standard normal random variables, that is $Z_i \\sim N(0,1)$ for $i=1,2,\\ldots,p$. \nClearly, $E[\\bm{Z}]=\\bm{0}$ and the variance matrix of $\\bm{Z}$ is given by $\\pV(\\bm{Z})=\\bm{I}_p$  the identity matrix of dimension $p\\times p$.\nIn fact, the joint density of $\\bm{Z}$ can be easily derived as follows\n$$f_{\\bm{Z}}(\\bm{z})=\\prod_{i=1}^pf_{Z_i}(z_i)\n=\\frac{1}{\\sqrt{(2\\pi)^p}}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^p z_i^2\\right)=\\frac{1}{\\sqrt{(2\\pi)^p}}\\exp\\left(-\\frac{1}{2}\\bm{z}^T\\bm{z}\\right)$$\n\n\n:::{.callout-tip icon=false}\n\n## Multivariate Normal Distribution\n\n::: {#def-multivariate-normal-distribution}\n\n\n\n\nLet $\\bm{Z}=(Z_1,Z_2,\\ldots,Z_p)^T$ be a random vector  of $p$ independent and identically distributed  standard normal random variables. Let $\\bm{B}$ be a matrix of constants of dimension $p\\times p$\nand $\\bm{\\mu}$ be a $p$ dimensional vector of constants. The random vector $\\bm{Y}$ defined by\n$$\\bm{Y}=\\bm{B}\\bm{Z}+\\bm{\\mu}$$\nis said to follow a Multivariate Normal Distribution with vector of means $\\mu$ and variance matrix $\\Sigma=\\bm{B}\\bm{B}^T$ and we use the notation $\\bm{Y}\\sim N_p(\\bm{\\mu},\\bm{\\Sigma})$. \n  \n:::\n:::\n\n\nThe expected value and variance matrix are obtained as follows  \n\n\n\\begin{align*}\nE[\\bm{Y}]&=E[\\bm{B}\\bm{Z}+\\bm{\\mu}]=\\bm{B}\\pE[\\bm{Z}+\\bm{\\mu}]\\\\\n\\pV(\\bm{Y})&=\\pV(\\bm{B}\\bm{Z}+\\bm{\\mu})=\\bm{B}\\pV(\\bm{Z})\\bm{B}^T=\\bm{B}\\bm{B}^T\n\\end{align*}\n\nusing  @prp-expected-value-linear-function. Using @eq-transformation-random-vector is easy to verify that the joint density of $\\bm{y}$ is given by\n\n$$\n\\frac{1}{\\sqrt{(2\\pi)^p}|\\bm{\\Sigma}|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\bm{y}-\\bm{\\mu})^T\\,\\bm{\\Sigma}^{-1}\\,(\\bm{y}-\\bm{\\mu})\\right)\n$$ {#eq-multivariate-normal-density}\n\nAn important property of the multivariate normal distribution is that if $Y_1$ and $Y_2$ have a multivariate normal distribution and zero covariance, then they must be independent. This implication only holds for the multivariate normal. The reverse is always true, e.g. independence implies zero covariance for any distribution.\n\n\n\n:::{.callout-tip icon=false}\n\n## Transformation of multivariate normal vectors\n\n::: {#prp-transformation-normal-random-vectors}\n\n\n\n\nLet $\\bm{Y}$ be a random vector in $\\rel^p$ such that $\\bm{Y}\\sim N_p(\\bm{\\mu},\\bm{\\Sigma})$ and let $\\bm{A}$ be a $p\\times p$ matrix of constants and $\\bm{b}$ a vector of length $p$ of constants. Then we have that the random vector $\\bm{W}=\\bm{A}\\bm{Y}+\\bm{b}$ is distributed as\n$$\\bm{W}\\sim N_p(\\bm{A}\\bm{\\mu}+\\bm{b},\\bm{A}\\bm{\\Sigma}\\bm{A}^T)$$\n\n:::\n:::\n\n\nThe following result  on conditional distributions is very useful in different parts of the course\n\n\n\n:::{.callout-tip icon=false}\n\n## Multivariate normal conditional distributions\n\n::: {#prp-conditional-gaussians}\n\n\n\n\nSuppose that $\\bm{Y}\\sim N_{p_1}(\\bm{\\mu}_{\\bms{Y}},\\bm{\\Sigma}_{\\bms{Y}})$ and $\\bm{W}\\sim N_{p_2}(\\bm{\\mu}_{\\bms{W}},\\bm{\\Sigma}_{\\bms{W}})$ are random vectors with a multivariate normal joint distribution. That is $(\\bm{Y}^T,\\bm{W}^T)^T\\sim N_p(\\bm{\\mu},\\bm{\\Sigma})$ where\n\n$$\\bm{\\mu}=\n\\left(\n\\begin{array}{c}\n \\bm{\\mu}_{\\bms{Y}}\\\\\n \\bm{\\mu}_{\\bms{W}} \n \\end{array}\n \\right)  \n  \\qquad\\bm{\\Sigma}=\n  \\left(\n\\begin{array}{cc}\n \\bm{\\Sigma}_{\\bms{Y}} & \\bm{\\Sigma}_{\\bms{Y,W}} \\\\\n \\bm{\\Sigma}_{\\bms{Y,W}} & \\bm{\\Sigma}_{\\bms{W}} \n \\end{array}\n \\right)\n$$\nthen\n$$\\bm{W}|\\bm{Y}=\\bm{y}\\sim N_m(\\bm{\\mu}_{\\bms{W}}+\\bm{\\Sigma}_{\\bms{W,Y}}\\bm{\\Sigma}_{\\bms{Y}}^{-1}(\\bm{y}-\\bm{\\mu}_{\\bms{Y}}),\\bm{\\Sigma}_{\\bms{W}}-\\bm{\\Sigma}_{\\bms{W,Y}}\\bm{\\Sigma}_{\\bms{Y}}^{-1}\\bm{\\Sigma}_{\\bms{Y,W}})$$\n\n:::\n:::\n\n\nwhere clearly $\\bm{\\Sigma}_{\\bms{Y,W}}=\\bm{\\Sigma}_{\\bms{W,Y}}$.\n\n\n:::{.callout-tip icon=false}\n\n## Quadratic forms in normal variables\n\n::: {#prp-quadratic-forms-normal}\n\n\n\n\nLet $\\bm{Y}\\sim N_r(\\bm{\\mu},\\bm{\\Sigma})$ then\n$$Q=(\\bm{Y}-\\bm{\\mu})^T\\bm{\\Sigma}^{-1}(\\bm{Y}-\\bm{\\mu})\\sim \\chi^2_r$$\n  \nLet $\\bm{A}$ be a symmetric  matrix of dimension $r\\times r$ then\n\n$$\n\\pE(\\bm{Y}^T\\bm{A}\\bm{Y})=tr(\\bm{A\\Sigma})+\\bm{\\mu}^T \\bm{A}\\bm{\\mu}\n$$ {#eq-mean-quadratic-form-mvn}\n\nwhere $tr$ denotes the trace of a matrix.\n  \n:::\n:::\n\n\n## Inequalities\n\n\n:::{.callout-tip icon=false}\n\n## Jensen's inequality\n\n::: {#prp-Jensen-inequality}\n\n\n\n\nLet $X$ be a real random variable. If $g$ is a concave function then \n\n$$\ng(E[X])\\geq E[g(X)]\n$$ {#eq-jensen-inequality-concave}\n\nand if $g$ is a convex function\n\n$$\ng(E[X])\\leq E[g(X)]\n$$ {#eq-jensen-inequality-convex}\n:::\n:::\n\n\n\n## The Monte Carlo algorithm for approximating sampling distributions {#sec-Monte-Carlo}\n\nIn cases where  there are no analytic expressions available for  sampling distributions we can use the so-called Monte Carlo method to compute these approximately.  In this sense, the Monte Carlo is a useful tool needed in the estimation process. We start this section with a reminder of the strong law of large numbers.\n\n\n\n\n:::{.callout-tip icon=false}\n\n## The strong law of large numbers\n\n::: {#thm-Theo-Law-large-number}\n\n\n\n\nLet $\\bm{S}_1,\\ldots,\\bm{S}_r$ be  independent and identically distributed random vectors in $\\rel^p$ and let $\\overline{\\bm{S}}_r$ denote the arithmetic average of $\\bm{S}_1,\\ldots,\\bm{S}_r$. If all the entries of $\\bm{\\mu}=\\pE[\\bm{S}_1]$ are finite then \n$$P\\left[\\lim_{r \\to \\infty}\\overline{\\bm{S}}_r=\\bm{\\mu}\\right]=1$$\nIt is then said that $\\overline{\\bm{S}}_r$ **converges almost surely to** $\\bm{\\mu}$ when $r \\to \\infty$.\n\n:::\n:::\n\n\nThe interpretation is that when the Monte-Carlo sample size $r$ is large then the sample mean is very close (in the sense described in Theorem @thm-Theo-Law-large-number)  to the  mean $\\bm{\\mu}$, the only condition required is that the means are finite. \n\n\nThe Monte Carlo method can be applied whenever we can express the sampling distribution quantity of interest as the mean $\\bm{\\mu}$ in Theorem @thm-Theo-Law-large-number.\n\n\nAssume   $Y_1,\\ldots,Y_n$ is a sample and we are interested in computing quantities of the form\n\n\n\n$$\\bm{\\mu}=\n\\left(\n\\begin{array}{c}\nE[g_1(Y_1,\\dots,Y_n)]\\\\\nE[g_2(Y_1,\\dots,Y_n)]\\\\\n\\vdots \\\\\nE[g_p(Y_1,\\dots,Y_n)]\n\\end{array}\n\\right)\n$$\n\n\nfor some functions $g_i:\\rel^n\\to\\rel$ for $i=1,\\ldots,p$.\nTransforming by $S_i=g_i(Y_1,\\dots,Y_n)$  for $i=1,\\ldots,p$ and defining\n $\\bm{S}^T=(S_1,\\ldots,S_p)$\nso that $\\bm{\\mu}=E[\\bm{S}]$ means that we can use the Monte Carlo method if we are able to generate  a large number $r$ of independent and identically distributed samples from $\\bm{S}$ so that $\\overline{\\bm{S}}$ will be a good approximation to $\\bm{\\mu}=E[\\bm{S}]$. \n\nTo generate a single sample from  $\\bm{S}$ we can generate a sample from $Y_1,\\ldots,Y_n$  and then apply the functions $g_1,\\ldots,g_p$.\nThe main requirement for a correct application of the Monte Carlo method is that the entries of $\\bm{\\mu}$ are  finite. The other more practical requirement is that we have an easy way of simulating the samples $Y_1,\\ldots,Y_n$.  \n\n\nGenerally, the Monte Carlo estimate  is constructed as follows.\n\n\n\n\n\n1. Simulate $r$  samples of size $n$ from $f(x_1,\\ldots,x_n)$ that is, \n\n\n\\begin{eqnarray*}\nY^{(1)}_1,\\ldots,Y^{(1)}_n & \\mbox{  from } & f(x_1,\\ldots,x_n)\\\\\nY^{(2)}_1,\\ldots,Y^{(2)}_n & \\mbox{  from } & f(x_1,\\ldots,x_n)\\\\\n\\vdots  & & \\vdots\\\\\nY^{(r)}_1,\\ldots,Y^{(r)}_n & \\mbox{  from } & f(x_1,\\ldots,x_n)\n\\end{eqnarray*}\n\nThese are called the Monte Carlo replicates of the original random sample.\n\n2. To each one of the replicate samples, apply the  functions $g_i(Y_1,\\ldots,Y_n)$ to obtain \n\n\\begin{eqnarray*}\n\\bm{S}^T_1 &=& (g_1(Y^{(1)}_1,\\ldots,Y^{(1)}_n),g_2(Y^{(1)}_1,\\ldots,Y^{(1)}_n),\\ldots,g_p(Y^{(1)}_1,\\ldots,Y^{(1)}_n))\\\\\n\\bm{S}^T_2 &=& (g_1(Y^{(2)}_1,\\ldots,Y^{(2)}_n),g_2(Y^{(2)}_1,\\ldots,Y^{(2)}_n),\\ldots,g_p(Y^{(2)}_1,\\ldots,Y^{(2)}_n))\\\\\n\\vdots  &  & \\vdots\\\\\n\\bm{S}^T_r &=& (g_1(Y^{(r)}_1,\\ldots,Y^{(r)}_n),g_2(Y^{(r)}_1,\\ldots,Y^{(r)}_n),\\ldots,g_p(Y^{(r)}_1,\\ldots,Y^{(r)}_n))\n\\end{eqnarray*}\n\nThe generated sample $\\bm{S}_1,\\ldots,\\bm{S}_r$ is a random sample of size $r$ from the density of  $\\bm{S}$ which has mean $\\bm{\\mu}$. \n\n3. Compute the Monte Carlo estimate of the quantity of interest $\\bm{\\mu}=E[\\bm{S}]$ as follows\n$$\\hat{\\mu}_{MC}=\\frac{1}{r}\\sum _{j=1}^r \\bm{S}_i=\\overline{\\bm{S}}$$\nthat is\n$$\\widehat{\\bm{\\mu}}_{MC}=\\left(\\sum_{i=1}^r g_1(Y^{(i)}_1,\\ldots,Y^{(i)}_n),\n\\sum_{i=1}^r g_2(Y^{(i)}_1,\\ldots,Y^{(i)}_n),\\ldots,\n\\sum_{i=1}^r g_p(Y^{(i)}_1,\\ldots,Y^{(i)}_n)\n\\right)$$\n\n\n\n\nClearly we have that \n$$E\\left[\\widehat{\\bm{\\mu}}_{MC}\\right]=E\\left[\\overline{\\bm{S}}\\right]=E[\\bm{S}_1]=\\bm{\\mu}$$\nso that the Monte Carlo estimate is  unbiased (correct on average). Furthermore,  \nby the Strong Law of Large Numbers in Theorem @thm-Theo-Law-large-number we have that \n\n$$P\\left[\\lim_{r \\to \\infty}\\overline{\\bm{S}}=\\bm{\\mu}\\right]=1$$\n \n\n\n\n\n\nWe can use the Monte-Carlo method to approximate the  density $f_{n}$ of the sampling distribution of $\\bm{S}$. TO see this lets consider   a small open set $B\\rel^p$  so that the mean value theorem formula applies, that is $$f_{(n)}(\\bm{s})=\\frac{P[\\bm{S}\\in B]}{|B|}$$\nwhere $\\bm{s}$ is a specific value in the interior of $B$ and $|B|$ is the size (area, volume, etc) of $B$. We can write\n$P[\\bm{S}\\in B]=E[1_B(\\bm{S})]$\nso that we can use the Monte Carlo Method to approximate $E[1_B(\\bm{S})]$ and therefore to aproximate $f_{(n)}(\\bm{s})$. In this case the Monte Carlo estimate of $f_{(n)}(\\bm{s})$ is given by\n$$\\hat{f}_{(n)}(\\bm{s})=\\frac{1}{r}\\sum_{i=1}^r \\frac{1_B(\\bm{S}_i)}{|B|}$$\n\nThe above expression defines simply the (multidimensional) histogram of the sample $\\bm{s}_1,\\ldots,\\bm{s}_r$. The following example hopefully clarifies the application of the Monte-Carlo method.\n\n\n### Example\n\nLet $Y_1,\\ldots,Y_n$ be a random sample from a Uniform(0,1) and we are interested in the density $f_{(n)}$ of the maximum of the sample. For the sake of illustration we will ignore the fact that such density is the density of a Beta distribution and use the Monte Carlo method to obtain an approximation to it.\n\nIn R we can check the goodness of the aproximation in two  situations where the number of replicates differ.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn<-100 # original sample size\npar(mfrow=c(1,2))\nr<-1000 # r= number of Monte Carlo replicates\n# Stage 1\n# generates  replicates in a matrix where each row is a Monte Carlo replicate\nY<-runif(n*r) \nY<-matrix(Y,nrow=r,ncol=n,byrow=T)\n# Stage 2: computes the maximum for each replicate \nS<-apply(Y,1,max)\n# stage 3: computes the Monte Carlo estimate (in this case: the histogram)\nbreaks=seq(0,1,by=0.001)\n# we choose small bins of equal length 0.001\nhist(S,breaks=breaks,freq=F,xlim=c(0.94,1),main=\"r=1000\") \n# compares to exact density\ncurve(dbeta(x,n,1),from=0,to=1,add=T,n=1000,col=\"red\") \n### now all over again but with r=100,000\nr<-100000 # r= number of Monte Carlo replicates\n# Stage 1\nY<-runif(n*r) \nY<-matrix(Y,nrow=r,ncol=n,byrow=T)\n# Stage 2\nS<-apply(Y,1,max)\n# stage 3\nhist(S,breaks=breaks,freq=F,xlim=c(0.94,1),main=\"r=100000\")\ncurve(dbeta(x,n,1),from=0,to=1,add=T,n=1000,col=\"red\")\n```\n\n::: {.cell-output-display}\n![Monte Carlo approximations to the density of the maximum of Uniform samples of size $n=100$](06-prerequisites_files/figure-pdf/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAs shown in Figure  the Monte Carlo approximation to the density  of the maximum  gets better when $r$ grows.\n\n\n\n",
    "supporting": [
      "06-prerequisites_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}